{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierachical Reconciliation Methods\n",
    "> Classes in this module have the `reconcile` method capable of reconcile base forecasts using `numpy` arrays.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from collections import OrderedDict\n",
    "from copy import deepcopy\n",
    "from typing import Dict, List, Union\n",
    "\n",
    "import numpy as np\n",
    "from numba import njit\n",
    "from statsmodels.stats.moment_helpers import cov2corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from fastcore.test import ExceptionExpected, test_close, test_eq\n",
    "from nbdev.showdoc import add_docs, show_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _reconcile(S: np.ndarray, P: np.ndarray, W: np.ndarray, \n",
    "               y_hat: np.ndarray, SP: np.ndarray = None):\n",
    "    if SP is None:\n",
    "        SP = S @ P\n",
    "    return np.matmul(SP, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def bottom_up(S: np.ndarray,\n",
    "              y_hat: np.ndarray,\n",
    "              idx_bottom: List[int]):\n",
    "    n_hiers, n_bottom = S.shape\n",
    "    P = np.zeros_like(S, dtype=np.float32)\n",
    "    P[idx_bottom] = S[idx_bottom]\n",
    "    P = P.T\n",
    "    W = np.eye(n_hiers, dtype=np.float32)\n",
    "    return _reconcile(S, P, W, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "class BottomUp:\n",
    "    \"\"\"Bottom Up Reconciliation Class.\n",
    "    [Source code](https://github.com/dluuo/hierarchicalforecast/blob/main/hierarchicalforecast/methods.py).\n",
    "\n",
    "    The most basic hierarchical reconciliation is performed using an Bottom-Up strategy. It was proposed for \n",
    "    the first time by Orcutt in 1968.\n",
    "    The corresponding hierarchical \"projection\" matrix is defined as:\n",
    "\n",
    "    $$\\mathbf{P}_{\\text{BU}} = [\\mathbf{0}_{\\mathrm{[b],[a]}}\\;|\\;\\mathbf{I}_{\\mathrm{[b][b]}}]$$\n",
    "\n",
    "    \n",
    "    **Parameters:**<br>\n",
    "    None\n",
    "    \n",
    "    **References:**<br>\n",
    "    - [Orcutt, G.H., Watts, H.W., & Edwards, J.B.(1968). Data aggregation and information loss. The American \n",
    "    Economic Review, 58 , 773{787)](http://www.jstor.org/stable/1815532).\n",
    "    \"\"\"\n",
    "    \n",
    "    def reconcile(\n",
    "            self,\n",
    "            S: np.ndarray, # Summing matrix of size (`base`, `bottom`)\n",
    "            y_hat: np.ndarray, # Forecast values of size (`base`, `horizon`)\n",
    "            idx_bottom: np.ndarray # Indices corresponding to the bottom level of `S`, size (`bottom`)\n",
    "        ):\n",
    "        \"\"\"Bottom Up Reconciliation Method.\n",
    "        [Source code](https://github.com/dluuo/hierarchicalforecast/blob/main/hierarchicalforecast/methods.py).\n",
    "\n",
    "        **Parameters:**<br>\n",
    "        `S`: Summing matrix of size (`base`, `bottom`).<br>\n",
    "        `y_hat`: Forecast values of size (`base`, `horizon`).<br>\n",
    "        `idx_bottom`: Indices corresponding to the bottom level of `S`, size (`bottom`).<br>\n",
    "\n",
    "        \"\"\"\n",
    "        return bottom_up(S=S, y_hat=y_hat, idx_bottom=idx_bottom)\n",
    "    \n",
    "    __call__ = reconcile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "add_docs(BottomUp, \"Bottom up reconciliation method.\",\n",
    "         reconcile=\"Reconcile using `BottomUp` approach.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "show_doc(BottomUp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(BottomUp.reconcile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- The most basic hierarchical reconciliation is performed using an Bottom-Up strategy. It was proposed for the first time by Orcutt in 1968.\n",
    "The corresponding hierarchical \"projection\" matrix is defined as:\n",
    "\n",
    "$$\\mathbf{P}_{\\text{BU}} = [\\mathbf{0}_{\\mathrm{[b],[a]}}\\;|\\;\\mathbf{I}_{\\mathrm{[b][b]}}]$$\n",
    "\n",
    "- [Orcutt, G.H., Watts, H.W., & Edwards, J.B.(1968). Data aggregation and information loss. The American Economic Review, 58 , 773{787)](http://www.jstor.org/stable/1815532) -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "S = np.array([\n",
    "    [1., 1., 1., 1.],\n",
    "    [1., 1., 0., 0.],\n",
    "    [0., 0., 1., 1.],\n",
    "    [0., 1., 0., 0.],\n",
    "    [1., 0., 0., 0.],\n",
    "    [0., 0., 1., 0.],\n",
    "    [0., 0., 0., 1.],\n",
    "])\n",
    "h = 10\n",
    "_y = np.array([10., 5., 4., 2., 1.])\n",
    "y_bottom = np.vstack([i * _y for i in range(1, 5)])\n",
    "y_hat_bottom_insample = np.roll(y_bottom, 1)\n",
    "y_hat_bottom_insample[:, 0] = np.nan\n",
    "y_hat_bottom = np.vstack([i * np.ones(h) for i in range(1, 5)])\n",
    "idx_bottom = [4, 3, 5, 6]\n",
    "levels = {'level1': np.array([0]),\n",
    "          'level2': np.array([1, 2]),\n",
    "          'level3': idx_bottom}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "cls_bottom_up = BottomUp()\n",
    "test_eq(\n",
    "    cls_bottom_up(S=S, y_hat=S @ y_hat_bottom, idx_bottom=idx_bottom),\n",
    "    S @ y_hat_bottom\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def is_strictly_hierarchical(S: np.ndarray, \n",
    "                             levels: Dict[str, np.ndarray]):\n",
    "    # main idea:\n",
    "    # if S represents a strictly hierarchical structure\n",
    "    # the number of paths before the bottom level\n",
    "    # should be equal to the number of nodes\n",
    "    # of the previuos level\n",
    "    levels_ = dict(sorted(levels.items(), key=lambda x: len(x[1])))\n",
    "    # removing bottom level\n",
    "    levels_.popitem()\n",
    "    # making S categorical\n",
    "    hiers = [np.argmax(S[idx], axis=0) + 1 for _, idx in levels_.items()]\n",
    "    hiers = np.vstack(hiers)\n",
    "    paths = np.unique(hiers, axis=1).shape[1] \n",
    "    nodes = levels_.popitem()[1].size\n",
    "    return paths == nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "assert is_strictly_hierarchical(S, levels)\n",
    "S_non_hier = np.array([\n",
    "    [1., 1., 1., 1.], # total\n",
    "    [1., 1., 0., 0.], # city 1\n",
    "    [0., 0., 1., 1.], # city 2\n",
    "    [1., 0., 1., 0.], # transgender\n",
    "    [0., 1., 0., 1.], # no transgender\n",
    "    [1., 0., 0., 0.], #city 1 - transgender\n",
    "    [0., 1., 0., 0.], #city 1 - no transgender\n",
    "    [0., 0., 1., 0.], #city 2 - transgender\n",
    "    [0., 0., 0., 1.], #city 2 - no transgender\n",
    "])\n",
    "levels_non_hier = {\n",
    "    'Country': np.array([0]),\n",
    "    'Country/City': np.array([2, 1]),\n",
    "    'Country/Transgender': np.array([3, 4]),\n",
    "    'Country-City-Transgender': np.array([5, 6, 7, 8]),\n",
    "}\n",
    "assert not is_strictly_hierarchical(S_non_hier, levels_non_hier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _get_child_nodes(S: np.ndarray, levels: Dict[str, np.ndarray]):\n",
    "    childs = {}\n",
    "    level_names = list(levels.keys())\n",
    "    nodes = OrderedDict()\n",
    "    for i_level, level in enumerate(level_names[:-1]):\n",
    "        parent = levels[level]\n",
    "        child = np.zeros_like(S)\n",
    "        idx_child = levels[level_names[i_level+1]] \n",
    "        child[idx_child] = S[idx_child]\n",
    "        nodes_level = {}\n",
    "        for idx_parent_node in parent:\n",
    "            parent_node = S[idx_parent_node]\n",
    "            idx_node = child * parent_node.astype(bool)\n",
    "            idx_node, = np.where(idx_node.sum(axis=1) > 0)\n",
    "            nodes_level[idx_parent_node] = [idx for idx in idx_child if idx in idx_node]\n",
    "        nodes[level] = nodes_level\n",
    "    return nodes        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _reconcile_fcst_proportions(S: np.ndarray, y_hat: np.ndarray,\n",
    "                                levels: Dict[str, np.ndarray],\n",
    "                                nodes: Dict[str, Dict[int, np.ndarray]],\n",
    "                                idx_top: int):\n",
    "    reconciled = np.zeros_like(y_hat)\n",
    "    reconciled[idx_top] = y_hat[idx_top]\n",
    "    level_names = list(levels.keys())\n",
    "    for i_level, level in enumerate(level_names[:-1]):\n",
    "        nodes_level = nodes[level]\n",
    "        for idx_parent, idx_childs in nodes_level.items():\n",
    "            fcst_parent = reconciled[idx_parent]\n",
    "            childs_sum = y_hat[idx_childs].sum()\n",
    "            for idx_child in idx_childs:\n",
    "                reconciled[idx_child] = y_hat[idx_child] * fcst_parent / childs_sum\n",
    "    return reconciled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def top_down(S: np.ndarray, \n",
    "             y_hat: np.ndarray,\n",
    "             y_insample: np.ndarray,\n",
    "             levels: Dict[str, np.ndarray],\n",
    "             method: str):\n",
    "    if not is_strictly_hierarchical(S, levels):\n",
    "        raise ValueError('Top down reconciliation requires strictly hierarchical structures.')\n",
    "    \n",
    "    n_hiers, n_bottom = S.shape\n",
    "    idx_top = int(S.sum(axis=1).argmax())\n",
    "    levels_ = dict(sorted(levels.items(), key=lambda x: len(x[1])))\n",
    "    idx_bottom = levels_[list(levels_)[-1]]\n",
    "    \n",
    "    if method == 'forecast_proportions':\n",
    "        nodes = _get_child_nodes(S=S, levels=levels_)\n",
    "        reconciled = [_reconcile_fcst_proportions(S=S, y_hat=y_hat_[:, None], \n",
    "                                                  levels=levels_, \n",
    "                                                  nodes=nodes,\n",
    "                                                  idx_top=idx_top) \\\n",
    "                      for y_hat_ in y_hat.T]\n",
    "        reconciled = np.hstack(reconciled)\n",
    "        return reconciled\n",
    "    else:\n",
    "        y_top = y_insample[idx_top]\n",
    "        y_btm = y_insample[idx_bottom]\n",
    "        if method == 'average_proportions':\n",
    "            prop = np.mean(y_btm / y_top, axis=1)\n",
    "        elif method == 'proportion_averages':\n",
    "            prop = np.mean(y_btm, axis=1) / np.mean(y_top)\n",
    "        else:\n",
    "            raise Exception(f'Unknown method {method}')\n",
    "    P = np.zeros_like(S, np.float64).T #float 64 if prop is too small, happens with wiki2\n",
    "    P[:, idx_top] = prop\n",
    "    W = np.eye(n_hiers, dtype=np.float32)\n",
    "    return _reconcile(S, P, W, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TopDown:\n",
    "    \"\"\"Top Down Reconciliation Class.\n",
    "    [Source code](https://github.com/dluuo/hierarchicalforecast/blob/main/hierarchicalforecast/methods.py).\n",
    "\n",
    "    The Top Down hierarchical reconciliation method, distributes the total aggregate predictions and decomposes \n",
    "    it down the hierarchy using proportions $\\mathbf{p}_{\\mathrm{[b]}}$ that can be actual historical values \n",
    "    or estimated.\n",
    "    \n",
    "    $$\\mathbf{P}=[\\mathbf{p}_{\\mathrm{[b]}}\\;|\\;\\mathbf{0}_{\\mathrm{[b][a,b\\;-1]}}]$$\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `method`: One of `forecast_proportions`, `average_proportions` and `proportion_averages`.<br>\n",
    "    \n",
    "    **References:**<br>\n",
    "    - [Disaggregation methods to expedite product line forecasting. Journal of Forecasting, 9 , 233–254. \n",
    "    doi:10.1002/for.3980090304](https://onlinelibrary.wiley.com/doi/abs/10.1002/for.3980090304).<br>\n",
    "    - [An investigation of aggregate variable time series forecast strategies with specific subaggregate \n",
    "    time series statistical correlation. Computers and Operations Research, 26 , 1133–1149. \n",
    "    doi:10.1016/S0305-0548(99)00017-9](https://doi.org/10.1016/S0305-0548(99)00017-9).\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self, \n",
    "            method: str # One of `forecast_proportions`, `average_proportions` and `proportion_averages`\n",
    "        ):\n",
    "        self.method = method\n",
    "    \n",
    "    def reconcile(\n",
    "            self, \n",
    "            S: np.ndarray, # Summing matrix of size (`base`, `bottom`)\n",
    "            y_hat: np.ndarray, # Forecast values of size (`base`, `horizon`)\n",
    "            y_insample: np.ndarray, # Insample values of size (`base`, `insample_size`)\n",
    "            levels: Dict[str, np.ndarray] # Each key is a level and each value its `S` indices\n",
    "        ):\n",
    "        \"\"\"Top Down Reconciliation Method.\n",
    "        [Source code](https://github.com/dluuo/hierarchicalforecast/blob/main/hierarchicalforecast/methods.py).\n",
    "\n",
    "        **Parameters:**<br>\n",
    "        `S`: Summing matrix of size (`base`, `bottom`).<br>\n",
    "        `y_hat`: Forecast values of size (`base`, `horizon`).<br>\n",
    "        `y_insample`: Insample values of size (`base`, `insample_size`).<br>\n",
    "        `levels`: Each key is a level and each value its `S` indices.<br>\n",
    "\n",
    "        \"\"\"\n",
    "        return top_down(S=S, y_hat=y_hat, \n",
    "                        y_insample=y_insample, \n",
    "                        levels=levels,\n",
    "                        method=self.method)\n",
    "    \n",
    "    __call__ = reconcile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "add_docs(TopDown, \"Top down reconciliation method.\",\n",
    "         reconcile=\"Reconcile using `TopDown` approach.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(TopDown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(TopDown.reconcile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- The Top Down hierarchical reconciliation method, distributes the total aggregate predictions and decomposes it down the hierarchy using proportions $\\mathbf{p}_{\\mathrm{[b]}}$ that can be actual historical values or estimated.\n",
    "\n",
    "$$\\mathbf{P}=[\\mathbf{p}_{\\mathrm{[b]}}\\;|\\;\\mathbf{0}_{\\mathrm{[b][a,b\\;-1]}}]$$\n",
    "\n",
    "- [Disaggregation methods to expedite product line forecasting. Journal of Forecasting, 9 , 233–254. doi:10.1002/for.3980090304](https://onlinelibrary.wiley.com/doi/abs/10.1002/for.3980090304)\n",
    "- [An investigation of aggregate variable time series forecast strategies with specific subaggregate time series statistical correlation. Computers and Operations Research, 26 , 1133–1149. doi:10.1016/S0305-0548(99)00017-9](https://doi.org/10.1016/S0305-0548(99)00017-9) -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# we are able to recover forecasts \n",
    "# from top_down in this example\n",
    "# because the time series\n",
    "# share the same proportion\n",
    "# across time\n",
    "# but it is not a general case\n",
    "for method in ['forecast_proportions', 'average_proportions', 'proportion_averages']:\n",
    "    cls_top_down = TopDown(method=method)\n",
    "    test_close(\n",
    "        cls_top_down(\n",
    "            S=S, \n",
    "            y_hat=S @ y_hat_bottom, \n",
    "            y_insample=S @ y_bottom, \n",
    "            levels=levels\n",
    "        ),\n",
    "        S @ y_hat_bottom\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def middle_out(S: np.ndarray, \n",
    "               y_hat: np.ndarray,\n",
    "               y_insample: np.ndarray,\n",
    "               levels: Dict[str, np.ndarray],\n",
    "               level: str,\n",
    "               top_down_method: str):\n",
    "    if not is_strictly_hierarchical(S, levels):\n",
    "        raise ValueError('Middle out reconciliation requires strictly hierarchical structures.')\n",
    "    if level not in levels.keys():\n",
    "        raise ValueError('You have to provide a `level` in `levels`.')\n",
    "    levels_ = dict(sorted(levels.items(), key=lambda x: len(x[1])))\n",
    "    reconciled = np.full_like(y_hat, fill_value=np.nan)\n",
    "    cut_nodes = levels_[level]\n",
    "    # bottom up reconciliation\n",
    "    idxs_bu = []\n",
    "    for node, idx_node in levels_.items():\n",
    "        idxs_bu.append(idx_node)\n",
    "        if node == level:\n",
    "            break\n",
    "    idxs_bu = np.hstack(idxs_bu)\n",
    "    #bottom up forecasts\n",
    "    bu = bottom_up(S=np.unique(S[idxs_bu], axis=1), \n",
    "                   y_hat=y_hat[idxs_bu], \n",
    "                   idx_bottom=np.arange(len(idxs_bu))[-len(cut_nodes):])\n",
    "    reconciled[idxs_bu] = bu\n",
    "    \n",
    "    #top down\n",
    "    child_nodes = _get_child_nodes(S, levels_)\n",
    "    # parents contains each node in the middle out level\n",
    "    # as key. The values of each node are the levels that\n",
    "    # are conected to that node.\n",
    "    parents = {node: {level: np.array([node])} for node in cut_nodes}\n",
    "    level_names = list(levels_.keys())\n",
    "    for lv, lv_child in zip(level_names[:-1], level_names[1:]):\n",
    "        # if lv is not part of the middle out to bottom\n",
    "        # structure we continue\n",
    "        if lv not in list(parents.values())[0].keys():\n",
    "            continue\n",
    "        for idx_middle_out in parents.keys():\n",
    "            idxs_parents = parents[idx_middle_out].values()\n",
    "            complete_idxs_child = []\n",
    "            for idx_parent, idxs_child in child_nodes[lv].items():\n",
    "                if any(idx_parent in val for val in idxs_parents):\n",
    "                    complete_idxs_child.append(idxs_child)\n",
    "            parents[idx_middle_out][lv_child] = np.hstack(complete_idxs_child)\n",
    " \n",
    "    for node, levels_node in parents.items():\n",
    "        idxs_node = np.hstack(list(levels_node.values()))\n",
    "        S_node = S[idxs_node]\n",
    "        S_node = S_node[:,~np.all(S_node == 0, axis=0)]\n",
    "        counter = 0\n",
    "        levels_node_ = deepcopy(levels_node)\n",
    "        for lv_name, idxs_level in levels_node_.items():\n",
    "            idxs_len = len(idxs_level)\n",
    "            levels_node_[lv_name] = np.arange(counter, idxs_len + counter)\n",
    "            counter += idxs_len\n",
    "        td = top_down(S_node, \n",
    "                      y_hat[idxs_node], \n",
    "                      y_insample[idxs_node], \n",
    "                      levels_node_, \n",
    "                      method=top_down_method)\n",
    "        reconciled[idxs_node] = td\n",
    "    return reconciled\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MiddleOut:\n",
    "    \"\"\"Middle Out Reconciliation Class.\n",
    "    [Source code](https://github.com/dluuo/hierarchicalforecast/blob/main/hierarchicalforecast/methods.py).\n",
    "\n",
    "    This method is only available for strictly hierarchical structures. It anchors the base predictions in \n",
    "    a middle level. The levels above the base predictions use the bottom-up approach, while the levels below \n",
    "    use a top-down.\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `level`: Middle level.<br>\n",
    "    `top_down_method`: One of `forecast_proportions`, `average_proportions` and `proportion_averages`.<br>\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self, \n",
    "            level: str, # Middle level \n",
    "            top_down_method: str # One of `forecast_proportions`, `average_proportions` and `proportion_averages`\n",
    "        ):\n",
    "        self.level = level\n",
    "        self.top_down_method = top_down_method \n",
    "    \n",
    "    def reconcile(\n",
    "            self, \n",
    "            S: np.ndarray, # Summing matrix of size (`base`, `bottom`)\n",
    "            y_hat: np.ndarray, # Forecast values of size (`base`, `horizon`)\n",
    "            y_insample: np.ndarray, # Insample values of size (`base`, `insample_size`)\n",
    "            levels: Dict[str, np.ndarray] # Each key is a level and each value its `S` indices\n",
    "        ):\n",
    "        \"\"\"Middle Out Reconciliation Method.\n",
    "        [Source code](https://github.com/dluuo/hierarchicalforecast/blob/main/hierarchicalforecast/methods.py).\n",
    "\n",
    "        **Parameters:**<br>\n",
    "        `S`: Summing matrix of size (`base`, `bottom`).<br>\n",
    "        `y_hat`: Forecast values of size (`base`, `horizon`).<br>\n",
    "        `y_insample`: Insample values of size (`base`, `insample_size`).<br>\n",
    "        `levels`: Each key is a level and each value its `S` indices.<br>\n",
    "\n",
    "        \"\"\"\n",
    "        return middle_out(S=S, y_hat=y_hat, \n",
    "                          y_insample=y_insample, \n",
    "                          levels=levels,\n",
    "                          level=self.level,\n",
    "                          top_down_method=self.top_down_method)\n",
    "    \n",
    "    __call__ = reconcile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "add_docs(MiddleOut, \"Middle out reconciliation method.\",\n",
    "         reconcile=\"Reconcile using `MiddleOut` approach.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(MiddleOut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(MiddleOut.reconcile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method is only available for strictly hierarchical structures. It anchors the base predictions in a middle level. The levels above the base predictions use the bottom-up approach, while the levels below use a top-down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# we are able to recover forecasts \n",
    "# from middle out in this example\n",
    "# because the time series\n",
    "# share the same proportion\n",
    "# across time\n",
    "# but it is not a general case\n",
    "for method in ['forecast_proportions', 'average_proportions', 'proportion_averages']:\n",
    "    cls_middle_out = MiddleOut(level='level2', top_down_method=method)\n",
    "    test_close(\n",
    "        cls_middle_out(\n",
    "            S=S, \n",
    "            y_hat=S @ y_hat_bottom, \n",
    "            y_insample=S @ y_bottom, \n",
    "            levels=levels\n",
    "        ),\n",
    "        S @ y_hat_bottom\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def crossprod(x):\n",
    "    return x.T @ x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def min_trace(S: np.ndarray, \n",
    "              y_hat: np.ndarray,\n",
    "              y_insample: np.ndarray,\n",
    "              y_hat_insample: np.ndarray,\n",
    "              method: str):\n",
    "    # shape residuals_insample (n_hiers, obs)\n",
    "    res_methods = ['wls_var', 'mint_cov', 'mint_shrink']\n",
    "    if method in res_methods and y_insample is None and y_hat_insample is None:\n",
    "        raise ValueError(f\"For methods {', '.join(res_methods)} you need to pass residuals\")\n",
    "    n_hiers, n_bottom = S.shape\n",
    "    if method == 'ols':\n",
    "        W = np.eye(n_hiers)\n",
    "    elif method == 'wls_struct':\n",
    "        W = np.diag(S @ np.ones((n_bottom,)))\n",
    "    elif method in res_methods:\n",
    "        #we need residuals with shape (obs, n_hiers)\n",
    "        residuals = (y_insample - y_hat_insample).T\n",
    "        n, _ = residuals.shape\n",
    "        masked_res = np.ma.array(residuals, mask=np.isnan(residuals))\n",
    "        covm = np.ma.cov(masked_res, rowvar=False, allow_masked=True).data\n",
    "        if method == 'wls_var':\n",
    "            W = np.diag(np.diag(covm))\n",
    "        elif method == 'mint_cov':\n",
    "            W = covm\n",
    "        elif method == 'mint_shrink':\n",
    "            tar = np.diag(np.diag(covm))\n",
    "            corm = cov2corr(covm)\n",
    "            xs = np.divide(residuals, np.sqrt(np.diag(covm)))\n",
    "            xs = xs[~np.isnan(xs).any(axis=1), :]\n",
    "            v = (1 / (n * (n - 1))) * (crossprod(xs ** 2) - (1 / n) * (crossprod(xs) ** 2))\n",
    "            np.fill_diagonal(v, 0)\n",
    "            corapn = cov2corr(tar)\n",
    "            d = (corm - corapn) ** 2\n",
    "            lmd = v.sum() / d.sum()\n",
    "            lmd = max(min(lmd, 1), 0)\n",
    "            W = lmd * tar + (1 - lmd) * covm\n",
    "    else:\n",
    "        raise ValueError(f'Unkown reconciliation method {method}')\n",
    "    \n",
    "    eigenvalues, _ = np.linalg.eig(W)\n",
    "    if any(eigenvalues < 1e-8):\n",
    "        raise Exception(f'min_trace ({method}) needs covariance matrix to be positive definite.')\n",
    "    \n",
    "    R = S.T @ np.linalg.pinv(W)\n",
    "    P = np.linalg.pinv(R @ S) @ R\n",
    "    \n",
    "    return _reconcile(S, P, W, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MinTrace:\n",
    "    \"\"\"MinTrace Reconciliation Class.\n",
    "    [Source code](https://github.com/dluuo/hierarchicalforecast/blob/main/hierarchicalforecast/methods.py).\n",
    "\n",
    "    This reconciliation algorithm proposed by Wickramasuriya et al. depends on a generalized least squares estimator \n",
    "    and an estimator of the covariance matrix of the coherency errors $\\mathbf{W}_{h}$. The Min Trace algorithm \n",
    "    minimizes the squared errors for the coherent forecasts under an unbiasedness assumption; the solution has a \n",
    "    closed form.<br>\n",
    "    \n",
    "    $$\\mathbf{P}_{\\text{MinT}}=\\left(\\mathbf{S}^{\\intercal}\\mathbf{W}_{h}\\mathbf{S}\\right)^{-1} \\mathbf{S}^{\\intercal}\\mathbf{W}^{-1}_{h}$$\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `method`: One of `ols`, `wls_struct`, `wls_var`, `mint_shrink`, `mint_co`.<br>\n",
    "    \n",
    "    **References:**<br>\n",
    "    - [Wickramasuriya, S. L., Athanasopoulos, G., & Hyndman, R. J. (2019). Optimal forecast reconciliation for\n",
    "    hierarchical and grouped time series through trace minimization. Journal of the American Statistical Association, \n",
    "    114 , 804–819. doi:10.1080/01621459.2018.1448825.](https://robjhyndman.com/publications/mint/).\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self, \n",
    "            method: str # One of `ols`, `wls_struct`, `wls_var`, `mint_shrink`, `mint_co`\n",
    "        ):\n",
    "        self.method = method\n",
    "        \n",
    "    def reconcile(\n",
    "            self, \n",
    "            S: np.ndarray, # Summing matrix of size (`base`, `bottom`)\n",
    "            y_hat: np.ndarray, # Forecast values of size (`base`, `horizon`)\n",
    "            y_insample: np.ndarray, # Insample values of size (`base`, `insample_size`)\n",
    "            y_hat_insample: np.ndarray # Insample forecasts of size (`base`, `insample_size`)\n",
    "        ):\n",
    "        \"\"\"MinTrace Reconciliation Method.\n",
    "        [Source code](https://github.com/dluuo/hierarchicalforecast/blob/main/hierarchicalforecast/methods.py).\n",
    "\n",
    "        **Parameters:**<br>\n",
    "        `S`: Summing matrix of size (`base`, `bottom`).<br>\n",
    "        `y_hat`: Forecast values of size (`base`, `horizon`).<br>\n",
    "        `y_insample`: Insample values of size (`base`, `insample_size`).<br>\n",
    "        `y_hat_insample`: Insample forecasts of size (`base`, `insample_size`).<br>\n",
    "\n",
    "        \"\"\"\n",
    "        return min_trace(S=S, y_hat=y_hat, \n",
    "                         y_insample=y_insample,\n",
    "                         y_hat_insample=y_hat_insample,\n",
    "                         method=self.method)\n",
    "    \n",
    "    __call__ = reconcile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "add_docs(MinTrace, \"Min trace reconciliation method.\",\n",
    "         reconcile=\"Reconcile using `MinTrace` approach.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(MinTrace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(MinTrace.reconcile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- This reconciliation algorithm proposed by Wickramasuriya et al. depends on a generalized least squares estimator and an estimator of the covariance matrix of the coherency errors $\\mathbf{W}_{h}$. The Min Trace algorithm minimizes the squared errors for the coherent forecasts under an unbiasedness assumption; the solution has a closed form.\n",
    "\n",
    "$$\\mathbf{P}_{\\text{MinT}} = \\left(\\mathbf{S}^{\\intercal}\\mathbf{W}_{h}\\mathbf{S}\\right)^{-1} \\mathbf{S}^{\\intercal} \\mathbf{W}^{-1}_{h}$$\n",
    "\n",
    "- [Wickramasuriya, S. L., Athanasopoulos, G., & Hyndman, R. J. (2019). Optimal forecast reconciliation for hierarchical and grouped time series through trace minimization. Journal of the American Statistical Association, 114 , 804–819. doi:10.1080/01621459.2018.1448825.](https://robjhyndman.com/publications/mint/) -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "for method in ['ols', 'wls_struct', 'wls_var', 'mint_shrink']:\n",
    "    cls_min_trace = MinTrace(method=method)\n",
    "    test_close(\n",
    "        cls_min_trace(\n",
    "            S=S, \n",
    "            y_hat=S @ y_hat_bottom, \n",
    "            y_insample=S @ y_bottom,\n",
    "            y_hat_insample=S @ y_hat_bottom_insample\n",
    "        ),\n",
    "        S @ y_hat_bottom\n",
    "    )\n",
    "with ExceptionExpected(regex='min_trace (mint_cov)*'):\n",
    "    cls_min_trace = MinTrace(method='mint_cov')\n",
    "    cls_min_trace(\n",
    "        S=S, \n",
    "        y_hat=S @ y_hat_bottom, \n",
    "        y_insample=S @ y_bottom,\n",
    "        y_hat_insample=S @ y_hat_bottom_insample\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def optimal_combination(S: np.ndarray, \n",
    "                        y_hat: np.ndarray,\n",
    "                        method: str,\n",
    "                        y_insample: np.ndarray = None,\n",
    "                        y_hat_insample: np.ndarray = None):\n",
    "    \n",
    "    return min_trace(S=S, y_hat=y_hat, \n",
    "                         y_insample=y_insample,\n",
    "                         y_hat_insample=y_hat_insample,\n",
    "                         method=method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class OptimalCombination:\n",
    "    \"\"\"Optimal Combination Reconciliation Class.\n",
    "    [Source code](https://github.com/dluuo/hierarchicalforecast/blob/main/hierarchicalforecast/methods.py).\n",
    "\n",
    "    This reconciliation algorithm was proposed by Hyndman et al. 2011, the method uses generalized least squares \n",
    "    estimator using the coherency errors covariance matrix. Consider the covariance of the base forecast \n",
    "    $\\textrm{Var}(\\epsilon_{h}) = \\Sigma_{h}$, the $\\mathbf{P}$ matrix of this method is defined by:\n",
    "\n",
    "    $$ \\mathbf{P} = \\left(\\mathbf{S}^{\\intercal}\\Sigma_{h}^{\\dagger}\\mathbf{S}\\right)^{-1}\\mathbf{S}^{\\intercal}\\Sigma^{\\dagger}_{h}$$\n",
    "\n",
    "    where $\\Sigma_{h}^{\\dagger}$ denotes the variance pseudo-inverse. The method was later proven equivalent to \n",
    "    `MinTrace` variants.\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `method`: Allowed Optimal Combination Methods: 'ols', 'wls_struct'.<br>\n",
    "    \n",
    "    **References:**<br>\n",
    "    - [Rob J. Hyndman, Roman A. Ahmed, George Athanasopoulos, Han Lin Shang. \"Optimal Combination Forecasts for \n",
    "    Hierarchical Time Series\" (2010).](https://robjhyndman.com/papers/Hierarchical6.pdf).<br>\n",
    "    - [Shanika L. Wickramasuriya, George Athanasopoulos and Rob J. Hyndman. \"Optimal Combination Forecasts for \n",
    "    Hierarchical Time Series\" (2010).](https://robjhyndman.com/papers/MinT.pdf).\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self, \n",
    "            method: str # Allowed Optimal Combination Methods: 'ols', 'wls_struct'\n",
    "        ):\n",
    "        comb_methods = ['ols', 'wls_struct']\n",
    "        if method not in comb_methods:\n",
    "            raise ValueError(f\"Optimal Combination class does not support method: \\\"{method}\\\"\")\n",
    "        \n",
    "        self.method = method\n",
    "    \n",
    "    def reconcile(\n",
    "            self,\n",
    "            S: np.ndarray, # Summing matrix of size (`base`, `bottom`)\n",
    "            y_hat: np.ndarray, # Forecast values of size (`base`, `horizon`)\n",
    "            y_insample: np.ndarray = None, # Insample values of size (`base`, `insample_size`)\n",
    "            y_hat_insample: np.ndarray = None # Insample forecasts of size (`base`, `insample_size`)\n",
    "        ):\n",
    "        \"\"\"Optimal Combination Reconciliation Method.\n",
    "        [Source code](https://github.com/dluuo/hierarchicalforecast/blob/main/hierarchicalforecast/methods.py).\n",
    "\n",
    "        **Parameters:**<br>\n",
    "        `S`: Summing matrix of size (`base`, `bottom`).<br>\n",
    "        `y_hat`: Forecast values of size (`base`, `horizon`).<br>\n",
    "        `y_insample`: Insample values of size (`base`, `insample_size`).<br>\n",
    "        `y_hat_insample`: Insample forecasts of size (`base`, `insample_size`).<br>\n",
    "\n",
    "        \"\"\"\n",
    "        return optimal_combination(S=S, \n",
    "                                   y_hat=y_hat, \n",
    "                                   y_insample=y_insample, \n",
    "                                   y_hat_insample=y_hat_insample, \n",
    "                                   method=self.method)\n",
    "    \n",
    "    __call__ = reconcile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "add_docs(OptimalCombination, \"Optimal combination reconciliation method.\",\n",
    "         reconcile=\"Reconcile using optimal combination approach.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(OptimalCombination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(OptimalCombination.reconcile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- This reconciliation algorithm was proposed by Hyndman et al. 2011, the method uses generalized least squares estimator using the coherency errors covariance matrix. Consider the covariance of the base forecast $\\textrm{Var}(\\epsilon_{h}) = \\Sigma_{h}$, the $\\mathbf{P}$ matrix of this method is defined by:\n",
    "\n",
    "$$ \\mathbf{P} = \\left(\\mathbf{S}^{\\intercal}\\Sigma_{h}^{\\dagger}\\mathbf{S}\\right)^{-1}\\mathbf{S}^{\\intercal}\\Sigma^{\\dagger}_{h}$$\n",
    "\n",
    "where $\\Sigma_{h}^{\\dagger}$ denotes the variance pseudo-inverse. The method was later proven equivalent to `MinTrace` variants.\n",
    "- [Rob J. Hyndman, Roman A. Ahmed, George Athanasopoulos, Han Lin Shang. \"Optimal Combination Forecasts for Hierarchical Time Series\" (2010).](https://robjhyndman.com/papers/Hierarchical6.pdf)\n",
    "- [Shanika L. Wickramasuriya, George Athanasopoulos and Rob J. Hyndman. \"Optimal Combination Forecasts for Hierarchical Time Series\" (2010).](https://robjhyndman.com/papers/MinT.pdf) -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "for method in ['ols', 'wls_struct']:\n",
    "    cls_optimal_combination = OptimalCombination(method=method)\n",
    "    test_close(\n",
    "        cls_optimal_combination(\n",
    "            S=S, \n",
    "            y_hat=S @ y_hat_bottom \n",
    "        ),\n",
    "        S @ y_hat_bottom\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "@njit\n",
    "def lasso(X: np.ndarray, y: np.ndarray, \n",
    "          lambda_reg: float, max_iters: int = 1_000,\n",
    "          tol: float = 1e-4):\n",
    "    # lasso cyclic coordinate descent\n",
    "    n, feats = X.shape\n",
    "    norms = (X ** 2).sum(axis=0)\n",
    "    beta = np.zeros(feats, dtype=np.float32)\n",
    "    beta_changes = np.zeros(feats, dtype=np.float32)\n",
    "    residuals = y.copy()\n",
    "    \n",
    "    for it in range(max_iters):\n",
    "        for i, betai in enumerate(beta):\n",
    "            # is feature is close to zero, we \n",
    "            # continue to the next.\n",
    "            # in this case is optimal betai= 0\n",
    "            if abs(norms[i]) < 1e-8:\n",
    "                continue\n",
    "            xi = X[:, i]\n",
    "            #we calculate the normalized derivative\n",
    "            rho = betai + xi.flatten().dot(residuals) / norms[i] #(norms[i] + 1e-3)\n",
    "            #soft threshold\n",
    "            beta[i] = np.sign(rho) * max(np.abs(rho) - lambda_reg * n / norms[i], 0.)#(norms[i] + 1e-3), 0.)\n",
    "            beta_changes[i] = np.abs(betai - beta[i])\n",
    "            if beta[i] != betai:\n",
    "                residuals += (betai - beta[i]) * xi\n",
    "        if max(beta_changes) < tol:\n",
    "            break\n",
    "    #print(it)\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def erm(S: np.ndarray,\n",
    "        y_hat: np.ndarray,\n",
    "        y_insample: np.ndarray,\n",
    "        y_hat_insample: np.ndarray,\n",
    "        idx_bottom: np.ndarray,\n",
    "        method: str,\n",
    "        lambda_reg: float = 1e-3):\n",
    "    n_hiers, n_bottom = S.shape\n",
    "    # y_hat_insample shape (n_hiers, obs)\n",
    "    # remove obs with nan values\n",
    "    nan_idx = np.isnan(y_hat_insample).any(axis=0)\n",
    "    y_insample = y_insample[:, ~nan_idx]\n",
    "    y_hat_insample = y_hat_insample[:, ~nan_idx]\n",
    "    #only using h validation steps to avoid \n",
    "    #computational burden\n",
    "    #print(y_hat.shape)\n",
    "    h = min(y_hat.shape[1], y_hat_insample.shape[1])\n",
    "    y_hat_insample = y_hat_insample[:, -h:] # shape (h, n_hiers)\n",
    "    y_insample = y_insample[:, -h:]\n",
    "    if method == 'closed':\n",
    "        B = np.linalg.inv(S.T @ S) @ S.T @ y_insample\n",
    "        B = B.T\n",
    "        P = np.linalg.pinv(y_hat_insample.T) @ B\n",
    "        P = P.T\n",
    "    elif method in ['reg', 'reg_bu']:\n",
    "        X = np.kron(np.array(S, order='F'), np.array(y_hat_insample.T, order='F'))\n",
    "        Pbu = np.zeros_like(S)\n",
    "        if method == 'reg_bu':\n",
    "            Pbu[idx_bottom] = S[idx_bottom]\n",
    "        Pbu = Pbu.T\n",
    "        Y = y_insample.T.flatten(order='F') - X @ Pbu.T.flatten(order='F')\n",
    "        if lambda_reg is None:\n",
    "            lambda_reg = np.max(np.abs(X.T.dot(Y)))\n",
    "        P = lasso(X, Y, lambda_reg)\n",
    "        P = P + Pbu.T.flatten(order='F')\n",
    "        P = P.reshape(-1, n_bottom, order='F').T\n",
    "    else:\n",
    "        raise ValueError(f'Unkown reconciliation method {method}')\n",
    "        \n",
    "    W = np.eye(n_hiers, dtype=np.float32)\n",
    "    \n",
    "    return _reconcile(S, P, W, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ERM:\n",
    "    \"\"\"Optimal Combination Reconciliation Class.\n",
    "    [Source code](https://github.com/dluuo/hierarchicalforecast/blob/main/hierarchicalforecast/methods.py).\n",
    "\n",
    "    The Empirical Risk Minimization reconciliation strategy relaxes the unbiasedness assumptions from\n",
    "    previous reconciliation methods like MinT and optimizes square errors between the reconciled predictions\n",
    "    and the validation data to obtain an optimal reconciliation matrix P.\n",
    "\n",
    "    The exact solution for $\\mathbf{P}$ (`method='closed'`) follows the expression:\n",
    "\n",
    "    $$\\mathbf{P}^{*} = \\left(\\mathbf{S}^{\\intercal}\\mathbf{S}\\right)^{-1}\\mathbf{Y}^{\\intercal}\\hat{\\mathbf{Y}}\\left(\\hat{\\mathbf{Y}}\\hat{\\mathbf{Y}}\\right)^{-1}$$\n",
    "\n",
    "    The alternative Lasso regularized $\\mathbf{P}$ solution (`method='reg_bu'`) is useful when the observations \n",
    "    of validation data is limited or the exact solution has low numerical stability.\n",
    "\n",
    "    $$\\mathbf{P}^{*} = \\text{argmin}_{\\mathbf{P}} ||\\mathbf{Y}-\\mathbf{S} \\mathbf{P} \\hat{Y} ||^{2}_{2} + \\lambda ||\\mathbf{P}-\\mathbf{P}_{\\text{BU}}||_{1}$$\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `method`: one of `closed`, `reg` and `reg_bu`.<br>\n",
    "    `lambda_reg`: l1 regularizer for `reg` and `reg_bu`.<br>\n",
    "    \n",
    "    **References:**<br>\n",
    "    - [Ben Taieb, S., & Koo, B. (2019). Regularized regression for hierarchical forecasting without \n",
    "    unbiasedness conditions. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge \n",
    "    Discovery & Data Mining KDD '19 (p. 1337{1347). New York, NY, USA: Association for Computing Machinery.]\n",
    "    (https://doi.org/10.1145/3292500.3330976).<br>\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self, \n",
    "            method: str, # one of `closed`, `reg` and `reg_bu`\n",
    "            lambda_reg: float = 1e-2 # l1 regularizer for `reg` and `reg_bu`\n",
    "        ):\n",
    "        self.method = method\n",
    "        self.lambda_reg = lambda_reg\n",
    "        \n",
    "    def reconcile(\n",
    "            self, \n",
    "            S: np.ndarray, # Summing matrix of size (`base`, `bottom`)\n",
    "            y_hat: np.ndarray, # Forecast values of size (`base`, `horizon`)\n",
    "            y_insample: np.ndarray, # Insample values of size (`base`, `insample_size`)\n",
    "            y_hat_insample: np.ndarray, # Insample forecasts of size (`base`, `insample_size`)\n",
    "            idx_bottom: np.ndarray # Indices corresponding to the bottom level of `S`, size (`bottom`)\n",
    "        ):\n",
    "        \"\"\"ERM Reconciliation Method.\n",
    "        [Source code](https://github.com/dluuo/hierarchicalforecast/blob/main/hierarchicalforecast/methods.py).\n",
    "\n",
    "        **Parameters:**<br>\n",
    "        `S`: Summing matrix of size (`base`, `bottom`).<br>\n",
    "        `y_hat`: Forecast values of size (`base`, `horizon`).<br>\n",
    "        `y_insample`: Insample values of size (`base`, `insample_size`).<br>\n",
    "        `y_hat_insample`: Insample forecasts of size (`base`, `insample_size`).<br>\n",
    "        `idx_bottom`: Indices corresponding to the bottom level of `S`, size (`bottom`).<br>\n",
    "        \"\"\"\n",
    "        return erm(S=S, y_hat=y_hat, \n",
    "                   y_insample=y_insample,\n",
    "                   y_hat_insample=y_hat_insample,\n",
    "                   idx_bottom=idx_bottom,\n",
    "                   method=self.method, lambda_reg=self.lambda_reg)\n",
    "    \n",
    "    __call__ = reconcile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "add_docs(ERM, \"ERM reconciliation method.\",\n",
    "         reconcile=\"Reconcile using `ERM` approach.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(ERM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(ERM.reconcile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- The Empirical Risk Minimization reconciliation strategy relaxes the unbiasedness assumptions from\n",
    "previous reconciliation methods like MinT and optimizes square errors between the reconciled predictions\n",
    "and the validation data to obtain an optimal reconciliation matrix P.\n",
    "\n",
    "The exact solution for $\\mathbf{P}$ (`method='closed'`) follows the expression:\n",
    "\n",
    "$$\\mathbf{P}^{*} = \\left(\\mathbf{S}^{\\intercal}\\mathbf{S}\\right)^{-1}\\mathbf{Y}^{\\intercal}\\hat{\\mathbf{Y}}\\left(\\hat{\\mathbf{Y}}\\hat{\\mathbf{Y}}\\right)^{-1}$$\n",
    "\n",
    "The alternative Lasso regularized $\\mathbf{P}$ solution (`method='reg_bu'`) is useful when the observations of validation data is \n",
    "limited or the exact solution has low numerical stability.\n",
    "\n",
    "$$\\mathbf{P}^{*} = \\text{argmin}_{\\mathbf{P}} ||\\mathbf{Y}-\\mathbf{S} \\mathbf{P} \\hat{Y} ||^{2}_{2} + \\lambda ||\\mathbf{P}-\\mathbf{P}_{\\text{BU}}||_{1}$$\n",
    "\n",
    "\n",
    "\n",
    "- [Ben Taieb, S., & Koo, B. (2019). Regularized regression for hierarchical forecasting without unbiasedness conditions. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining KDD '19 (p. 1337{1347). New York, NY, USA: Association for Computing Machinery.](https://doi.org/10.1145/3292500.3330976) -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "for method in ['reg_bu']:\n",
    "    cls_erm = ERM(method=method, lambda_reg=None)\n",
    "    test_close(\n",
    "        cls_erm(\n",
    "            S=S, \n",
    "            y_hat=S @ y_hat_bottom,\n",
    "            y_insample=S @ y_bottom,\n",
    "            y_hat_insample=S @ y_hat_bottom_insample,\n",
    "            idx_bottom=idx_bottom\n",
    "        ),\n",
    "        S @ y_hat_bottom\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuralforecast",
   "language": "python",
   "name": "neuralforecast"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
