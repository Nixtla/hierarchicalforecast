{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:DarkOrange\"> Reconciliation Methods </span>\n",
    "\n",
    "> Large collections of time series organized into structures at different aggregation levels often require their forecasts to follow their aggregation constraints, which poses the challenge of creating novel algorithms capable of coherent forecasts. <br><br> The `HierarchicalForecast` package provides the most comprehensive collection of Python implementations of hierarchical forecasting algorithms that follow classic hierarchical reconciliation. All the methods have a `reconcile` function capable of reconcile base forecasts using `numpy` arrays.<br><br> Most reconciliation methods can be described by the following convenient linear algebra notation: $\\tilde{\\mathbf{y}}_{[a,b],\\tau} = \\mathbf{S}_{[a,b][b]} \\mathbf{P}_{[b][a,b]} \\hat{\\mathbf{y}}_{[a,b],\\tau}$ <br> where $a, b$ represent the aggregate and bottom levels, $\\mathbf{S}_{[a,b][b]}$ contains the hierarchical aggregation constraints, and $\\mathbf{P}_{[b][a,b]}$ varies across \n",
    "reconciliation methods. The reconciled predictions are $\\tilde{\\mathbf{y}}_{[a,b],\\tau}$, and the base predictions $\\hat{\\mathbf{y}}_{[a,b],\\tau}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import warnings\n",
    "from collections import OrderedDict\n",
    "from copy import deepcopy\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "import numpy as np\n",
    "from numba import njit\n",
    "from quadprog import solve_qp\n",
    "from scipy.stats import norm\n",
    "from statsmodels.stats.moment_helpers import cov2corr\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from fastcore.test import ExceptionExpected, test_close, test_eq\n",
    "from nbdev.showdoc import add_docs, show_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _reconcile(S: np.ndarray, P: np.ndarray, W: np.ndarray, \n",
    "               y_hat: np.ndarray, SP: np.ndarray = None,\n",
    "               level: Optional[List[int]] = None,\n",
    "               intervals_method: str = 'normality',\n",
    "               sigmah: Optional[np.ndarray] = None, \n",
    "               samples: Optional[np.ndarray] = None):\n",
    "    if SP is None:\n",
    "        SP = S @ P\n",
    "    #mean reconciliation\n",
    "    res = {'mean': np.matmul(SP, y_hat)}\n",
    "    \n",
    "    if intervals_method in ['bootstrap', 'permbu'] and level is not None:\n",
    "        # calculate prediction intervals\n",
    "        # using bootstrap\n",
    "        # we are assuming that\n",
    "        # samples are calculated according to intervals_method\n",
    "        if samples is None:\n",
    "            raise Exception(f'you have to pass {intervals_method} samples')\n",
    "        # samples of shape (B, n_hiers, h)\n",
    "        samples = np.apply_along_axis(lambda path: np.matmul(SP, path), axis=1, arr=samples)\n",
    "        res = {'mean': samples.mean(axis=0)}\n",
    "        for lv in level:\n",
    "            min_q = (100 - lv) / 200 \n",
    "            max_q = min_q + lv / 100\n",
    "            res[f'lo-{lv}'] = np.quantile(samples, min_q, axis=0)\n",
    "            res[f'hi-{lv}'] = np.quantile(samples, max_q, axis=0)\n",
    "        return res\n",
    "    \n",
    "    if intervals_method == 'normality'and level is not None:\n",
    "        if sigmah is None:\n",
    "            raise Exception('you have to pass `sigmah`')\n",
    "        # then we calculate prediction intervals\n",
    "        # we assume normality\n",
    "        # we have to calculate the \"reconciled\" sigmah\n",
    "        # following\n",
    "        # https://otexts.com/fpp3/rec-prob.html\n",
    "        R1 = cov2corr(W)\n",
    "        W_h = [np.diag(sigma) @ R1 @ np.diag(sigma).T for sigma in sigmah.T]\n",
    "        sigmah = np.hstack([np.sqrt(np.diag(SP @ W @ SP.T))[:, None] for W in W_h])\n",
    "        res['sigmah'] = sigmah\n",
    "        # intervals calc\n",
    "        level = np.asarray(level)\n",
    "        z = norm.ppf(0.5 + level / 200)\n",
    "        for zs, lv in zip(z, level):\n",
    "            res[f'lo-{lv}'] = res['mean'] - zs * sigmah\n",
    "            res[f'hi-{lv}'] = res['mean'] + zs * sigmah\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:DarkBlue\"> 1. Bottom-Up </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BottomUp:\n",
    "    \"\"\"Bottom Up Reconciliation Class.\n",
    "    The most basic hierarchical reconciliation is performed using an Bottom-Up strategy. It was proposed for \n",
    "    the first time by Orcutt in 1968.\n",
    "    The corresponding hierarchical \\\"projection\\\" matrix is defined as:\n",
    "    $$\\mathbf{P}_{\\\\text{BU}} = [\\mathbf{0}_{\\mathrm{[b],[a]}}\\;|\\;\\mathbf{I}_{\\mathrm{[b][b]}}]$$\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    None\n",
    "\n",
    "    **References:**<br>\n",
    "    - [Orcutt, G.H., Watts, H.W., & Edwards, J.B.(1968). \\\"Data aggregation and information loss\\\". The American \n",
    "    Economic Review, 58 , 773{787)](http://www.jstor.org/stable/1815532).\n",
    "    \"\"\"\n",
    "    insample = False\n",
    "    \n",
    "    def reconcile(self,\n",
    "                  S: np.ndarray,\n",
    "                  y_hat: np.ndarray,\n",
    "                  idx_bottom: np.ndarray,\n",
    "                  level: Optional[List[int]] = None,\n",
    "                  intervals_method: str = 'normality',\n",
    "                  sigmah: Optional[np.ndarray] = None,\n",
    "                  samples: Optional[np.ndarray] = None):\n",
    "        \"\"\"Bottom Up Reconciliation Method.\n",
    "\n",
    "        **Parameters:**<br>\n",
    "        `S`: Summing matrix of size (`base`, `bottom`).<br>\n",
    "        `y_hat`: Forecast values of size (`base`, `horizon`).<br>\n",
    "        `idx_bottom`: Indices corresponding to the bottom level of `S`, size (`bottom`).<br>\n",
    "        `level`: float list 0-100, confidence levels for prediction intervals.<br>\n",
    "        `intervals_method`: str, method used to calculate prediction intervals, one of `normality`, `bootstrap`, `permbu`.<br>\n",
    "        `sigmah`: Estimate of the standard deviation of the h-step forecast of size (`base`, `horizon`)<br>\n",
    "        `samples`: Samples for prediction intevals of size (`n_samples`, `base`, `horizon`).<br>\n",
    "\n",
    "        **Returns:**<br>\n",
    "        `y_tilde`: Reconciliated y_hat using the Bottom Up approach.\n",
    "        \"\"\"\n",
    "        n_hiers, n_bottom = S.shape\n",
    "        P = np.zeros_like(S, dtype=np.float32)\n",
    "        P[idx_bottom] = S[idx_bottom]\n",
    "        P = P.T\n",
    "        W = np.eye(n_hiers, dtype=np.float32)\n",
    "        return _reconcile(S, P, W, y_hat, sigmah=sigmah, level=level, \n",
    "                          intervals_method=intervals_method, \n",
    "                          samples=samples)\n",
    "    \n",
    "    __call__ = reconcile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(BottomUp, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(BottomUp.reconcile, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "S = np.array([[1., 1., 1., 1.],\n",
    "              [1., 1., 0., 0.],\n",
    "              [0., 0., 1., 1.],\n",
    "              [0., 1., 0., 0.],\n",
    "              [1., 0., 0., 0.],\n",
    "              [0., 0., 1., 0.],\n",
    "              [0., 0., 0., 1.]])\n",
    "h = 2\n",
    "_y = np.array([10., 5., 4., 2., 1.])\n",
    "y_bottom = np.vstack([i * _y for i in range(1, 5)])\n",
    "y_hat_bottom_insample = np.roll(y_bottom, 1)\n",
    "y_hat_bottom_insample[:, 0] = np.nan\n",
    "y_hat_bottom = np.vstack([i * np.ones(h) for i in range(1, 5)])\n",
    "idx_bottom = [4, 3, 5, 6]\n",
    "tags = {'level1': np.array([0]),\n",
    "        'level2': np.array([1, 2]),\n",
    "        'level3': idx_bottom}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# sigmah for all levels in the hierarchy\n",
    "# sigmah for Naive method\n",
    "# as calculated here:\n",
    "#https://otexts.com/fpp3/prediction-intervals.html\n",
    "y_base = S @ y_bottom\n",
    "y_hat_base = S @ y_hat_bottom\n",
    "y_hat_base_insample = S @ y_hat_bottom_insample\n",
    "sigma = np.nansum((y_base - y_hat_base_insample) ** 2, axis=1) / (y_base.shape[1] - 1)\n",
    "sigma = np.sqrt(sigma)\n",
    "sigmah = sigma[:, None] * np.sqrt(np.vstack([np.arange(1, h + 1) for _ in range(y_base.shape[0])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _bootstrap_samples(\n",
    "        y_insample: np.ndarray, # Insample values of size (`base`, `insample_size`)\n",
    "        y_hat_insample: np.ndarray, # Insample forecasts of size (`base`, `insample_size`)\n",
    "        y_hat: np.ndarray, # Forecast values of size (`base`, `horizon`)\n",
    "        n_samples: int, # Number of bootstrap samples,\n",
    "        seed: int = 0, # seed\n",
    "    ):\n",
    "    residuals = y_insample - y_hat_insample\n",
    "    h = y_hat.shape[1]\n",
    "    #removing nas from residuals\n",
    "    residuals = residuals[:, np.isnan(residuals).sum(axis=0) == 0]\n",
    "    sample_idx = np.arange(residuals.shape[1] - h)\n",
    "    state = np.random.RandomState(seed)\n",
    "    samples_idx = state.choice(sample_idx, size=n_samples)\n",
    "    samples = [y_hat + residuals[:, idx:(idx + h)] for idx in samples_idx]\n",
    "    return np.stack(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "boot_samples = _bootstrap_samples(y_base, y_hat_base_insample, y_hat_base, 1_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "class PERMBU:\n",
    "    \"\"\"PERMBU Probabilistic Reconciliation Class.\n",
    "\n",
    "    The PERM-BU method leverages empirical bottom-level marginal distributions \n",
    "    with empirical copula functions (describing bottom-level dependencies) to \n",
    "    generate the distribution of aggregate-level distributions using BottomUp \n",
    "    reconciliation. The sample reordering technique in the PERM-BU method reinjects \n",
    "    multivariate dependencies into independent bottom-level samples.\n",
    "\n",
    "    **References:**<br>\n",
    "    - [Taieb, Souhaib Ben and Taylor, James W and Hyndman, Rob J. (2017). \n",
    "    Coherent probabilistic forecasts for hierarchical time series. \n",
    "    International conference on machine learning ICML.](https://proceedings.mlr.press/v70/taieb17a.html)\n",
    "    \"\"\"\n",
    "    def _obtain_ranks(self, array):\n",
    "        \"\"\" Vector ranks\n",
    "\n",
    "        Efficiently obtain vector ranks.\n",
    "        Example `array=[4,2,7,1]` -> `ranks=[2, 1, 3, 0]`.\n",
    "\n",
    "        **Parameters**<br>\n",
    "        `array`: np.array, matrix with floats or integers on which the \n",
    "                ranks will be computed on the second dimension.<br>\n",
    "\n",
    "        **Returns**<br>\n",
    "        `ranks`: np.array, matrix with ranks along the second dimension.<br>\n",
    "        \"\"\"\n",
    "        temp = array.argsort(axis=1)\n",
    "        ranks = np.empty_like(temp)\n",
    "        a_range = np.arange(temp.shape[1])\n",
    "        for iRow in range(temp.shape[0]):\n",
    "            ranks[iRow, temp[iRow,:]] = a_range\n",
    "        return ranks\n",
    "\n",
    "    def _permutate_samples(self, samples, permutations):\n",
    "        \"\"\" Permutate Samples\n",
    "\n",
    "        Applies efficient vectorized permutation on the samples.\n",
    "\n",
    "        **Parameters**<br>\n",
    "        `samples`: np.array [series,samples], independent base samples.<br>\n",
    "        `permutations`: np.array [series,samples], permutation ranks with wich\n",
    "                  which `samples` dependence will be restored see `_obtain_ranks`.<br>\n",
    "\n",
    "        **Returns**<br>\n",
    "        `permutated_samples`: np.array.<br>\n",
    "        \"\"\"\n",
    "        # Generate auxiliary and flat permutation indexes\n",
    "        n_rows, n_cols = permutations.shape\n",
    "        aux_row_idx = np.arange(n_rows)[:,None] * n_cols\n",
    "        aux_row_idx = np.repeat(aux_row_idx, repeats=n_cols, axis=1)\n",
    "        permutate_idxs = permutations.flatten() + aux_row_idx.flatten()\n",
    "\n",
    "        # Apply flat permutation indexes and recover original shape\n",
    "        permutated_samples = samples.flatten()\n",
    "        permutated_samples = permutated_samples[permutate_idxs]\n",
    "        permutated_samples = permutated_samples.reshape(n_rows, n_cols)\n",
    "        return permutated_samples\n",
    "    \n",
    "    def _permutate_predictions(self, prediction_samples, permutations):\n",
    "        \"\"\" Permutate Prediction Samples\n",
    "\n",
    "        Applies permutations to prediction_samples across the horizon.\n",
    "\n",
    "        **Parameters**<br>\n",
    "        `prediction_samples`: np.array [series,horizon,samples], independent \n",
    "                  base prediction samples.<br>\n",
    "        `permutations`: np.array [series, samples], permutation ranks with which\n",
    "                  `samples` dependence will be restored see `_obtain_ranks`.\n",
    "                  it can also apply a random permutation.<br>\n",
    "\n",
    "        **Returns**<br>\n",
    "        `permutated_prediction_samples`: np.array.<br>\n",
    "        \"\"\"\n",
    "        # Apply permutation throughout forecast horizon\n",
    "        permutated_prediction_samples = prediction_samples.copy()\n",
    "        \n",
    "        _, n_horizon, _ = prediction_samples.shape\n",
    "        for t in range(n_horizon):\n",
    "            permutated_prediction_samples[:,t,:] = \\\n",
    "                              self._permutate_samples(prediction_samples[:,t,:],\n",
    "                                                      permutations)\n",
    "        return permutated_prediction_samples\n",
    "\n",
    "    def _nonzero_indexes_by_row(self, M):\n",
    "        return [np.nonzero(M[row,:])[0] for row in range(len(M))]\n",
    "\n",
    "    def reconcile(self,\n",
    "                  S: np.ndarray,\n",
    "                  y_hat_mean: np.ndarray,\n",
    "                  y_hat_std: np.ndarray,                  \n",
    "                  y_insample: np.ndarray,\n",
    "                  y_hat_insample: np.ndarray,\n",
    "                  n_samples: int=None,\n",
    "                  seed: int=0):\n",
    "        \"\"\"PERMBU Sample Reconciliation Method.\n",
    "\n",
    "        Applies PERMBU reconciliation method as defined by Taieb et. al 2017.\n",
    "        Generating independent base prediction samples, restoring its multivariate\n",
    "        dependence using estimated copula with reordering and applying the BottomUp\n",
    "        aggregation to the new samples.\n",
    "\n",
    "        Algorithm:\n",
    "        1.   For all series compute conditional marginals distributions.\n",
    "        2.   Compute residuals $\\hat{\\epsilon}_{i,t}$ and obtain rank permutations.\n",
    "        2.   Obtain K-sample from the bottom-level series predictions.\n",
    "        3.   Apply recursively through the hierarchical structure:<br>\n",
    "            3.1.   For a given aggregate series $i$ and its children series:<br>\n",
    "            3.2.   Obtain children's empirical joint using sample reordering copula.<br>\n",
    "            3.2.   From the children's joint obtain the aggregate series's samples.        \n",
    "\n",
    "        **Parameters:**<br>\n",
    "        `S`: Summing matrix of size (`base`, `bottom`).<br>\n",
    "        `y_hat_mean`: Mean forecast values of size (`base`, `horizon`).<br>\n",
    "        `y_hat_std`: Forecast standard dev. of size (`base`, `horizon`).<br>\n",
    "        `y_insample`: Insample values of size (`base`, `insample_size`).<br>\n",
    "        `y_hat_insample`: Insample values of size (`base`, `insample_size`).<br>\n",
    "        `n_samples`: int, number of normal prediction samples generated.<br>\n",
    "\n",
    "        **Returns:**<br>\n",
    "        `rec_samples`: Reconciliated samples using the PERMBU approach.\n",
    "        \"\"\"\n",
    "\n",
    "        # Compute residuals and rank permutations\n",
    "        residuals = y_insample - y_hat_insample\n",
    "        #removing nas from residuals\n",
    "        residuals = residuals[:, np.isnan(residuals).sum(axis=0) == 0]\n",
    "        rank_permutations = self._obtain_ranks(residuals)\n",
    "        \n",
    "        # Sample h step-ahead base marginal distributions\n",
    "        if n_samples is None:\n",
    "            n_samples = residuals.shape[1]\n",
    "        state = np.random.RandomState(seed)\n",
    "        n_series, n_horizon = y_hat_mean.shape\n",
    "\n",
    "        base_samples = np.array([\n",
    "            state.normal(loc=m, scale=s, size=n_samples) for m, s in \\\n",
    "            zip(y_hat_mean.flatten(), y_hat_std.flatten())\n",
    "        ])\n",
    "        base_samples = base_samples.reshape(n_series, n_horizon, n_samples)\n",
    "\n",
    "        # Initialize PERMBU utility\n",
    "        rec_samples = base_samples.copy()\n",
    "        encoder = OneHotEncoder(sparse=False, dtype=np.float32)\n",
    "        hier_links = np.vstack(self._nonzero_indexes_by_row(S.T))\n",
    "\n",
    "        # BottomUp hierarchy traversing\n",
    "        hier_levels = hier_links.shape[1]-1\n",
    "        for level_idx in reversed(range(hier_levels)):\n",
    "            # Obtain aggregation matrix from parent/children links\n",
    "            children_links = np.unique(hier_links[:,level_idx:level_idx+2], \n",
    "                                       axis=0)\n",
    "            children_idxs = np.unique(children_links[:,1])\n",
    "            parent_idxs = np.unique(children_links[:,0])\n",
    "            Agg = encoder.fit_transform(children_links).T\n",
    "            Agg = Agg[:len(parent_idxs),:]\n",
    "\n",
    "            # Permute children_samples for each prediction step\n",
    "            children_permutations = rank_permutations[children_idxs, :]\n",
    "            children_samples = rec_samples[children_idxs,:,:]\n",
    "            children_samples = self._permutate_predictions(\n",
    "                                        prediction_samples=children_samples,\n",
    "                                        permutations=children_permutations)\n",
    "\n",
    "            # Overwrite hier_samples with BottomUp aggregation\n",
    "            # and randomly shuffle parent predictions after aggregation\n",
    "            parent_samples = np.einsum('ab,bhs->ahs', Agg, children_samples)\n",
    "            random_permutation = np.array([\n",
    "                                  np.random.permutation(np.arange(n_samples)) \\\n",
    "                                      for serie in range(len(parent_samples))])\n",
    "            parent_samples = self._permutate_predictions(\n",
    "                                        prediction_samples=parent_samples,\n",
    "                                        permutations=random_permutation)\n",
    "\n",
    "            rec_samples[parent_idxs,:,:] = parent_samples\n",
    "\n",
    "        return np.transpose(rec_samples, (2, 0, 1))\n",
    "\n",
    "    __call__ = reconcile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _permbu_samples(\n",
    "        y_insample: np.ndarray, # Insample values of size (`base`, `insample_size`)\n",
    "        y_hat_insample: np.ndarray, # Insample forecasts of size (`base`, `insample_size`)\n",
    "        y_hat: np.ndarray, # Forecast values of size (`base`, `horizon`)\n",
    "        sigmah: np.ndarray, # Estimate of the standard deviation of the h-step forecast of size (`base`, `horizon`)\n",
    "        S: np.ndarray, # Summing matrix\n",
    "        n_samples: int, # Number of bootstrap samples,\n",
    "        seed: int = 0, # seed\n",
    "    ):\n",
    "    return PERMBU().reconcile(S=S, y_hat_mean=y_hat, y_hat_std=sigmah,\n",
    "                              y_insample=y_insample, y_hat_insample=y_hat_insample, \n",
    "                              n_samples=n_samples, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "permbu_samples = _permbu_samples(y_insample=y_base, \n",
    "                                 y_hat_insample=y_hat_base_insample, \n",
    "                                 y_hat=y_hat_base,\n",
    "                                 sigmah=sigmah, \n",
    "                                 S=S, n_samples=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "cls_bottom_up = BottomUp()\n",
    "test_eq(\n",
    "    cls_bottom_up(S=S, y_hat=S @ y_hat_bottom, idx_bottom=idx_bottom)['mean'],\n",
    "    S @ y_hat_bottom\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test prediction intervals\n",
    "# we should recover the original sigmah\n",
    "# for the bottom time series\n",
    "cls_bottom_up = BottomUp()\n",
    "test_eq(\n",
    "    cls_bottom_up(S=S, y_hat=S @ y_hat_bottom, idx_bottom=idx_bottom, \n",
    "                  sigmah=sigmah, level=[80, 90])['sigmah'][idx_bottom],\n",
    "    sigmah[idx_bottom]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# we should recover the forecasts\n",
    "# using error=0\n",
    "cls_bottom_up = BottomUp()\n",
    "for method in ['bootstrap', 'permbu']:\n",
    "    test_eq(\n",
    "        cls_bottom_up(\n",
    "            S=S, y_hat=S @ y_hat_bottom, idx_bottom=idx_bottom, \n",
    "            level=[80, 90], intervals_method=method,\n",
    "            sigmah=sigmah, \n",
    "            samples=np.stack([y_hat_base for _ in range(1_000)])\n",
    "        )['mean'],\n",
    "        S @ y_hat_bottom\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def is_strictly_hierarchical(S: np.ndarray, \n",
    "                             tags: Dict[str, np.ndarray]):\n",
    "    # main idea:\n",
    "    # if S represents a strictly hierarchical structure\n",
    "    # the number of paths before the bottom level\n",
    "    # should be equal to the number of nodes\n",
    "    # of the previuos level\n",
    "    levels_ = dict(sorted(tags.items(), key=lambda x: len(x[1])))\n",
    "    # removing bottom level\n",
    "    levels_.popitem()\n",
    "    # making S categorical\n",
    "    hiers = [np.argmax(S[idx], axis=0) + 1 for _, idx in levels_.items()]\n",
    "    hiers = np.vstack(hiers)\n",
    "    paths = np.unique(hiers, axis=1).shape[1] \n",
    "    nodes = levels_.popitem()[1].size\n",
    "    return paths == nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "assert is_strictly_hierarchical(S, tags)\n",
    "S_non_hier = np.array([\n",
    "    [1., 1., 1., 1.], # total\n",
    "    [1., 1., 0., 0.], # city 1\n",
    "    [0., 0., 1., 1.], # city 2\n",
    "    [1., 0., 1., 0.], # transgender\n",
    "    [0., 1., 0., 1.], # no transgender\n",
    "    [1., 0., 0., 0.], #city 1 - transgender\n",
    "    [0., 1., 0., 0.], #city 1 - no transgender\n",
    "    [0., 0., 1., 0.], #city 2 - transgender\n",
    "    [0., 0., 0., 1.], #city 2 - no transgender\n",
    "])\n",
    "tags_non_hier = {\n",
    "    'Country': np.array([0]),\n",
    "    'Country/City': np.array([2, 1]),\n",
    "    'Country/Transgender': np.array([3, 4]),\n",
    "    'Country-City-Transgender': np.array([5, 6, 7, 8]),\n",
    "}\n",
    "assert not is_strictly_hierarchical(S_non_hier, tags_non_hier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _get_child_nodes(S: np.ndarray, tags: Dict[str, np.ndarray]):\n",
    "    level_names = list(tags.keys())\n",
    "    nodes = OrderedDict()\n",
    "    for i_level, level in enumerate(level_names[:-1]):\n",
    "        parent = tags[level]\n",
    "        child = np.zeros_like(S)\n",
    "        idx_child = tags[level_names[i_level+1]] \n",
    "        child[idx_child] = S[idx_child]\n",
    "        nodes_level = {}\n",
    "        for idx_parent_node in parent:\n",
    "            parent_node = S[idx_parent_node]\n",
    "            idx_node = child * parent_node.astype(bool)\n",
    "            idx_node, = np.where(idx_node.sum(axis=1) > 0)\n",
    "            nodes_level[idx_parent_node] = [idx for idx in idx_child if idx in idx_node]\n",
    "        nodes[level] = nodes_level\n",
    "    return nodes        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:DarkBlue\"> 2. Top-Down </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _reconcile_fcst_proportions(S: np.ndarray, y_hat: np.ndarray,\n",
    "                                tags: Dict[str, np.ndarray],\n",
    "                                nodes: Dict[str, Dict[int, np.ndarray]],\n",
    "                                idx_top: int):\n",
    "    reconciled = np.zeros_like(y_hat)\n",
    "    reconciled[idx_top] = y_hat[idx_top]\n",
    "    level_names = list(tags.keys())\n",
    "    for i_level, level in enumerate(level_names[:-1]):\n",
    "        nodes_level = nodes[level]\n",
    "        for idx_parent, idx_childs in nodes_level.items():\n",
    "            fcst_parent = reconciled[idx_parent]\n",
    "            childs_sum = y_hat[idx_childs].sum()\n",
    "            for idx_child in idx_childs:\n",
    "                reconciled[idx_child] = y_hat[idx_child] * fcst_parent / childs_sum\n",
    "    return reconciled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TopDown:\n",
    "    \"\"\"Top Down Reconciliation Class.\n",
    "\n",
    "    The Top Down hierarchical reconciliation method, distributes the total aggregate predictions and decomposes \n",
    "    it down the hierarchy using proportions $\\mathbf{p}_{\\mathrm{[b]}}$ that can be actual historical values \n",
    "    or estimated.\n",
    "\n",
    "    $$\\mathbf{P}=[\\mathbf{p}_{\\mathrm{[b]}}\\;|\\;\\mathbf{0}_{\\mathrm{[b][a,b\\;-1]}}]$$\n",
    "    **Parameters:**<br>\n",
    "    `method`: One of `forecast_proportions`, `average_proportions` and `proportion_averages`.<br>\n",
    "\n",
    "    **References:**<br>\n",
    "    - [CW. Gross (1990). \\\"Disaggregation methods to expedite product line forecasting\\\". Journal of Forecasting, 9 , 233–254. \n",
    "    doi:10.1002/for.3980090304](https://onlinelibrary.wiley.com/doi/abs/10.1002/for.3980090304).<br>\n",
    "    - [G. Fliedner (1999). \\\"An investigation of aggregate variable time series forecast strategies with specific subaggregate \n",
    "    time series statistical correlation\\\". Computers and Operations Research, 26 , 1133–1149. \n",
    "    doi:10.1016/S0305-0548(99)00017-9](https://doi.org/10.1016/S0305-0548(99)00017-9).\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 method: str):\n",
    "        self.method = method\n",
    "        self.insample = method in ['average_proportions', 'proportion_averages']\n",
    "    \n",
    "    def reconcile(self, \n",
    "                  S: np.ndarray,\n",
    "                  y_hat: np.ndarray,\n",
    "                  tags: Dict[str, np.ndarray],\n",
    "                  y_insample: Optional[np.ndarray] = None,\n",
    "                  level: Optional[List[int]] = None,\n",
    "                  intervals_method: str = 'normality',\n",
    "                  sigmah: Optional[np.ndarray] = None,\n",
    "                  samples: Optional[np.ndarray] = None):\n",
    "        \"\"\"Top Down Reconciliation Method.\n",
    "\n",
    "        **Parameters:**<br>\n",
    "        `S`: Summing matrix of size (`base`, `bottom`).<br>\n",
    "        `y_hat`: Forecast values of size (`base`, `horizon`).<br>\n",
    "        `tags`: Each key is a level and each value its `S` indices.<br>\n",
    "        `y_insample`: Insample values of size (`base`, `insample_size`). Optional for `forecast_proportions` method.<br>\n",
    "        `idx_bottom`: Indices corresponding to the bottom level of `S`, size (`bottom`).<br>\n",
    "        `level`: float list 0-100, confidence levels for prediction intervals.<br>\n",
    "        `intervals_method`: str, method used to calculate prediction intervals, one of `normality`, `bootstrap`, `permbu`.<br>\n",
    "        `sigmah`: Estimate of the standard deviation of the h-step forecast of size (`base`, `horizon`)<br>\n",
    "        `samples`: Samples for prediction intevals of size (`n_samples`, `base`, `horizon`).<br>\n",
    "\n",
    "        **Returns:**<br>\n",
    "        `y_tilde`: Reconciliated y_hat using the Top Down approach.\n",
    "        \"\"\"\n",
    "        if not is_strictly_hierarchical(S, tags):\n",
    "            raise ValueError('Top down reconciliation requires strictly hierarchical structures.')\n",
    "\n",
    "        n_hiers, n_bottom = S.shape\n",
    "        idx_top = int(S.sum(axis=1).argmax())\n",
    "        levels_ = dict(sorted(tags.items(), key=lambda x: len(x[1])))\n",
    "        idx_bottom = levels_[list(levels_)[-1]]\n",
    "\n",
    "        if self.method == 'forecast_proportions':\n",
    "            if level is not None:\n",
    "                warnings.warn('Prediction intervals not implement for `forecast_proportions`')\n",
    "            nodes = _get_child_nodes(S=S, tags=levels_)\n",
    "            reconciled = [_reconcile_fcst_proportions(S=S, y_hat=y_hat_[:, None], \n",
    "                                                      tags=levels_, \n",
    "                                                      nodes=nodes,\n",
    "                                                      idx_top=idx_top) \\\n",
    "                          for y_hat_ in y_hat.T]\n",
    "            reconciled = np.hstack(reconciled)\n",
    "            return {'mean': reconciled}\n",
    "        else:\n",
    "            y_top = y_insample[idx_top]\n",
    "            y_btm = y_insample[idx_bottom]\n",
    "            if self.method == 'average_proportions':\n",
    "                prop = np.mean(y_btm / y_top, axis=1)\n",
    "            elif self.method == 'proportion_averages':\n",
    "                prop = np.mean(y_btm, axis=1) / np.mean(y_top)\n",
    "            else:\n",
    "                raise Exception(f'Unknown method {method}')\n",
    "        P = np.zeros_like(S, np.float64).T #float 64 if prop is too small, happens with wiki2\n",
    "        P[:, idx_top] = prop\n",
    "        W = np.eye(n_hiers, dtype=np.float32)\n",
    "        return _reconcile(S, P, W, y_hat, sigmah=sigmah, level=level,\n",
    "                          intervals_method=intervals_method, \n",
    "                          samples=samples)\n",
    "    __call__ = reconcile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(TopDown, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(TopDown.reconcile, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# we are able to recover forecasts \n",
    "# from top_down in this example\n",
    "# because the time series\n",
    "# share the same proportion\n",
    "# across time\n",
    "# but it is not a general case\n",
    "for method in ['forecast_proportions', 'average_proportions', 'proportion_averages']:\n",
    "    cls_top_down = TopDown(method=method)\n",
    "    if cls_top_down.insample:\n",
    "        assert method in ['average_proportions', 'proportion_averages']\n",
    "        test_close(\n",
    "            cls_top_down(\n",
    "                S=S, \n",
    "                y_hat=S @ y_hat_bottom, \n",
    "                y_insample=S @ y_bottom, \n",
    "                tags=tags\n",
    "            )['mean'],\n",
    "            S @ y_hat_bottom\n",
    "        )\n",
    "    else:\n",
    "        test_close(\n",
    "            cls_top_down(\n",
    "                S=S, \n",
    "                y_hat=S @ y_hat_bottom, \n",
    "                tags=tags\n",
    "            )['mean'],\n",
    "            S @ y_hat_bottom\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test_levels\n",
    "for method in ['forecast_proportions', 'average_proportions', 'proportion_averages']:\n",
    "    cls_top_down = TopDown(method=method)\n",
    "    cls_top_down(\n",
    "        S=S, \n",
    "        y_hat=S @ y_hat_bottom, \n",
    "        y_insample=S @ y_bottom, \n",
    "        tags=tags,\n",
    "        sigmah=sigmah,\n",
    "        level=[80, 90]\n",
    "    )\n",
    "    cls_top_down(\n",
    "        S=S, \n",
    "        y_hat=S @ y_hat_bottom, \n",
    "        y_insample=S @ y_bottom, \n",
    "        tags=tags,\n",
    "        sigmah=sigmah,\n",
    "        level=[80, 90],\n",
    "        intervals_method='bootstrap',\n",
    "        samples=boot_samples\n",
    "    )\n",
    "    cls_top_down(\n",
    "        S=S, \n",
    "        y_hat=S @ y_hat_bottom, \n",
    "        y_insample=S @ y_bottom, \n",
    "        tags=tags,\n",
    "        sigmah=sigmah,\n",
    "        level=[80, 90],\n",
    "        intervals_method='permbu',\n",
    "        samples=permbu_samples\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:DarkBlue\"> 3. Middle-Out </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MiddleOut:\n",
    "    \"\"\"Middle Out Reconciliation Class.\n",
    "    \n",
    "    This method is only available for **strictly hierarchical structures**. It anchors the base predictions \n",
    "    in a middle level. The levels above the base predictions use the Bottom-Up approach, while the levels \n",
    "    below use a Top-Down.\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `middle_level`: Middle level.<br>\n",
    "    `top_down_method`: One of `forecast_proportions`, `average_proportions` and `proportion_averages`.<br>\n",
    "\n",
    "    **References:**<br>\n",
    "    - [Hyndman, R.J., & Athanasopoulos, G. (2021). \\\"Forecasting: principles and practice, 3rd edition: \n",
    "    Chapter 11: Forecasting hierarchical and grouped series.\\\". OTexts: Melbourne, Australia. OTexts.com/fpp3 \n",
    "    Accessed on July 2022.](https://otexts.com/fpp3/hierarchical.html)\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 middle_level: str,\n",
    "                 top_down_method: str):\n",
    "        self.middle_level = middle_level\n",
    "        self.top_down_method = top_down_method \n",
    "        self.insample = top_down_method in ['average_proportions', 'proportion_averages']\n",
    "    \n",
    "    def reconcile(self, \n",
    "                  S: np.ndarray,\n",
    "                  y_hat: np.ndarray,\n",
    "                  tags: Dict[str, np.ndarray],\n",
    "                  y_insample: Optional[np.ndarray] = None):\n",
    "        \"\"\"Middle Out Reconciliation Method.\n",
    "\n",
    "        **Parameters:**<br>\n",
    "        `S`: Summing matrix of size (`base`, `bottom`).<br>\n",
    "        `y_hat`: Forecast values of size (`base`, `horizon`).<br>\n",
    "        `tags`: Each key is a level and each value its `S` indices.<br>\n",
    "        `y_insample`: Insample values of size (`base`, `insample_size`). Only used for `forecast_proportions`<br>\n",
    "\n",
    "        **Returns:**<br>\n",
    "        `y_tilde`: Reconciliated y_hat using the Middle Out approach.\n",
    "        \"\"\"\n",
    "        if not is_strictly_hierarchical(S, tags):\n",
    "            raise ValueError('Middle out reconciliation requires strictly hierarchical structures.')\n",
    "        if self.middle_level not in tags.keys():\n",
    "            raise ValueError('You have to provide a `middle_level` in `tags`.')\n",
    "        levels_ = dict(sorted(tags.items(), key=lambda x: len(x[1])))\n",
    "        reconciled = np.full_like(y_hat, fill_value=np.nan)\n",
    "        cut_nodes = levels_[self.middle_level]\n",
    "        # bottom up reconciliation\n",
    "        idxs_bu = []\n",
    "        for node, idx_node in levels_.items():\n",
    "            idxs_bu.append(idx_node)\n",
    "            if node == self.middle_level:\n",
    "                break\n",
    "        idxs_bu = np.hstack(idxs_bu)\n",
    "        #bottom up forecasts\n",
    "        bu = BottomUp().reconcile(\n",
    "            S=np.unique(S[idxs_bu], axis=1), \n",
    "            y_hat=y_hat[idxs_bu], \n",
    "            idx_bottom=np.arange(len(idxs_bu))[-len(cut_nodes):]\n",
    "        )\n",
    "        reconciled[idxs_bu] = bu['mean']\n",
    "\n",
    "        #top down\n",
    "        child_nodes = _get_child_nodes(S, levels_)\n",
    "        # parents contains each node in the middle out level\n",
    "        # as key. The values of each node are the levels that\n",
    "        # are conected to that node.\n",
    "        parents = {node: {self.middle_level: np.array([node])} for node in cut_nodes}\n",
    "        level_names = list(levels_.keys())\n",
    "        for lv, lv_child in zip(level_names[:-1], level_names[1:]):\n",
    "            # if lv is not part of the middle out to bottom\n",
    "            # structure we continue\n",
    "            if lv not in list(parents.values())[0].keys():\n",
    "                continue\n",
    "            for idx_middle_out in parents.keys():\n",
    "                idxs_parents = parents[idx_middle_out].values()\n",
    "                complete_idxs_child = []\n",
    "                for idx_parent, idxs_child in child_nodes[lv].items():\n",
    "                    if any(idx_parent in val for val in idxs_parents):\n",
    "                        complete_idxs_child.append(idxs_child)\n",
    "                parents[idx_middle_out][lv_child] = np.hstack(complete_idxs_child)\n",
    "\n",
    "        for node, levels_node in parents.items():\n",
    "            idxs_node = np.hstack(list(levels_node.values()))\n",
    "            S_node = S[idxs_node]\n",
    "            S_node = S_node[:,~np.all(S_node == 0, axis=0)]\n",
    "            counter = 0\n",
    "            levels_node_ = deepcopy(levels_node)\n",
    "            for lv_name, idxs_level in levels_node_.items():\n",
    "                idxs_len = len(idxs_level)\n",
    "                levels_node_[lv_name] = np.arange(counter, idxs_len + counter)\n",
    "                counter += idxs_len\n",
    "            td = TopDown(self.top_down_method).reconcile(\n",
    "                S=S_node, \n",
    "                y_hat=y_hat[idxs_node], \n",
    "                y_insample=y_insample[idxs_node] if y_insample is not None else None, \n",
    "                tags=levels_node_, \n",
    "            )\n",
    "            reconciled[idxs_node] = td['mean']\n",
    "        return {'mean': reconciled}\n",
    "    __call__ = reconcile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(MiddleOut, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(MiddleOut.reconcile, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# we are able to recover forecasts \n",
    "# from middle out in this example\n",
    "# because the time series\n",
    "# share the same proportion\n",
    "# across time\n",
    "# but it is not a general case\n",
    "for method in ['forecast_proportions', 'average_proportions', 'proportion_averages']:\n",
    "    cls_middle_out = MiddleOut(middle_level='level2', top_down_method=method)\n",
    "    if cls_middle_out.insample:\n",
    "        assert method in ['average_proportions', 'proportion_averages']\n",
    "        test_close(\n",
    "            cls_middle_out(\n",
    "                S=S, \n",
    "                y_hat=S @ y_hat_bottom, \n",
    "                y_insample=S @ y_bottom, \n",
    "                tags=tags\n",
    "            )['mean'],\n",
    "            S @ y_hat_bottom\n",
    "        )\n",
    "    else:\n",
    "        test_close(\n",
    "            cls_middle_out(\n",
    "                S=S, \n",
    "                y_hat=S @ y_hat_bottom, \n",
    "                tags=tags\n",
    "            )['mean'],\n",
    "            S @ y_hat_bottom\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:DarkBlue\"> 4. Min-Trace </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def crossprod(x):\n",
    "    return x.T @ x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MinTrace:\n",
    "    \"\"\"MinTrace Reconciliation Class.\n",
    "\n",
    "    This reconciliation algorithm proposed by Wickramasuriya et al. depends on a generalized least squares estimator \n",
    "    and an estimator of the covariance matrix of the coherency errors $\\mathbf{W}_{h}$. The Min Trace algorithm \n",
    "    minimizes the squared errors for the coherent forecasts under an unbiasedness assumption; the solution has a \n",
    "    closed form.<br>\n",
    "\n",
    "    $$\\mathbf{P}_{\\\\text{MinT}}=\\\\left(\\mathbf{S}^{\\intercal}\\mathbf{W}_{h}\\mathbf{S}\\\\right)^{-1}\n",
    "    \\mathbf{S}^{\\intercal}\\mathbf{W}^{-1}_{h}$$\n",
    "    \n",
    "    **Parameters:**<br>\n",
    "    `method`: str, one of `ols`, `wls_struct`, `wls_var`, `mint_shrink`, `mint_cov`.<br>\n",
    "    `nonnegative`: bool, reconciled forecasts should be nonnegative?<br>\n",
    "\n",
    "    **References:**<br>\n",
    "    - [Wickramasuriya, S. L., Athanasopoulos, G., & Hyndman, R. J. (2019). \\\"Optimal forecast reconciliation for\n",
    "    hierarchical and grouped time series through trace minimization\\\". Journal of the American Statistical Association, \n",
    "    114 , 804–819. doi:10.1080/01621459.2018.1448825.](https://robjhyndman.com/publications/mint/).\n",
    "    - [Wickramasuriya, S.L., Turlach, B.A. & Hyndman, R.J. (2020). \\\"Optimal non-negative\n",
    "    forecast reconciliation\". Stat Comput 30, 1167–1182, \n",
    "    https://doi.org/10.1007/s11222-020-09930-0](https://robjhyndman.com/publications/nnmint/).\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 method: str,\n",
    "                 nonnegative: bool = False):\n",
    "        self.method = method\n",
    "        self.nonnegative = nonnegative\n",
    "        self.insample = method in ['wls_var', 'mint_cov', 'mint_shrink']\n",
    "\n",
    "    def reconcile(self, \n",
    "                  S: np.ndarray,\n",
    "                  y_hat: np.ndarray,\n",
    "                  y_insample: Optional[np.ndarray] = None,\n",
    "                  y_hat_insample: Optional[np.ndarray] = None,\n",
    "                  idx_bottom: Optional[List[int]] = None,\n",
    "                  level: Optional[List[int]] = None,\n",
    "                  intervals_method: str = 'normality',\n",
    "                  sigmah: Optional[np.ndarray] = None,\n",
    "                  samples: Optional[np.ndarray] = None):\n",
    "        \"\"\"MinTrace Reconciliation Method.\n",
    "\n",
    "        **Parameters:**<br>\n",
    "        `S`: Summing matrix of size (`base`, `bottom`).<br>\n",
    "        `y_hat`: Forecast values of size (`base`, `horizon`).<br>\n",
    "        `y_insample`: Insample values of size (`base`, `insample_size`). Only used by `wls_var`, `mint_cov`, `mint_shrink`<br>\n",
    "        `y_hat_insample`: Insample fitted values of size (`base`, `insample_size`). Only used by `wls_var`, `mint_cov`, `mint_shrink`<br>\n",
    "        `idx_bottom`: Indices corresponding to the bottom level of `S`, size (`bottom`).<br>\n",
    "        `level`: float list 0-100, confidence levels for prediction intervals.<br>\n",
    "        `intervals_method`: str, method used to calculate prediction intervals, one of `normality`, `bootstrap`, `permbu`.<br>\n",
    "        `sigmah`: Estimate of the standard deviation of the h-step forecast of size (`base`, `horizon`)<br>\n",
    "        `samples`: Samples for prediction intevals of size (`n_samples`, `base`, `horizon`).<br>\n",
    "\n",
    "        **Returns:**<br>\n",
    "        `y_tilde`: Reconciliated y_hat using the MinTrace approach.\n",
    "        \"\"\"\n",
    "    # shape residuals_insample (n_hiers, obs)\n",
    "        res_methods = ['wls_var', 'mint_cov', 'mint_shrink']\n",
    "        if self.method in res_methods and y_insample is None and y_hat_insample is None:\n",
    "            raise ValueError(f\"For methods {', '.join(res_methods)} you need to pass residuals\")\n",
    "        n_hiers, n_bottom = S.shape\n",
    "        if self.method == 'ols':\n",
    "            W = np.eye(n_hiers)\n",
    "        elif self.method == 'wls_struct':\n",
    "            W = np.diag(S @ np.ones((n_bottom,)))\n",
    "        elif self.method in res_methods:\n",
    "            #we need residuals with shape (obs, n_hiers)\n",
    "            residuals = (y_insample - y_hat_insample).T\n",
    "            n, _ = residuals.shape\n",
    "            masked_res = np.ma.array(residuals, mask=np.isnan(residuals))\n",
    "            covm = np.ma.cov(masked_res, rowvar=False, allow_masked=True).data\n",
    "            if self.method == 'wls_var':\n",
    "                W = np.diag(np.diag(covm))\n",
    "            elif self.method == 'mint_cov':\n",
    "                W = covm\n",
    "            elif self.method == 'mint_shrink':\n",
    "                tar = np.diag(np.diag(covm))\n",
    "                corm = cov2corr(covm)\n",
    "                xs = np.divide(residuals, np.sqrt(np.diag(covm)))\n",
    "                xs = xs[~np.isnan(xs).any(axis=1), :]\n",
    "                v = (1 / (n * (n - 1))) * (crossprod(xs ** 2) - (1 / n) * (crossprod(xs) ** 2))\n",
    "                np.fill_diagonal(v, 0)\n",
    "                corapn = cov2corr(tar)\n",
    "                d = (corm - corapn) ** 2\n",
    "                lmd = v.sum() / d.sum()\n",
    "                lmd = max(min(lmd, 1), 0)\n",
    "                W = lmd * tar + (1 - lmd) * covm\n",
    "        else:\n",
    "            raise ValueError(f'Unkown reconciliation method {self.method}')\n",
    "\n",
    "        eigenvalues, _ = np.linalg.eig(W)\n",
    "        if any(eigenvalues < 1e-8):\n",
    "            raise Exception(f'min_trace ({self.method}) needs covariance matrix to be positive definite.')\n",
    "\n",
    "        W_inv = np.linalg.pinv(W)\n",
    "        if self.nonnegative:\n",
    "            if intervals_method == 'bootstrap':\n",
    "                raise Exception('nonnegative reconciliation is not compatible with bootstrap forecasts')\n",
    "            if idx_bottom is None:\n",
    "                raise Exception('idx_bottom needed for nonnegative reconciliation')\n",
    "            warnings.warn('Replacing negative forecasts with zero.')\n",
    "            y_hat = np.copy(y_hat)\n",
    "            y_hat[y_hat < 0] = 0.\n",
    "            # Quadratic progamming formulation\n",
    "            # here we are solving the quadratic programming problem\n",
    "            # formulated in the origial paper\n",
    "            # https://robjhyndman.com/publications/nnmint/\n",
    "            # The library quadprog was chosen\n",
    "            # based on these benchmarks:\n",
    "            # https://scaron.info/blog/quadratic-programming-in-python.html\n",
    "            a = S.T @ W_inv\n",
    "            G = a @ S\n",
    "            C = np.eye(n_bottom)\n",
    "            b = np.zeros(n_bottom)\n",
    "            # the quadratic programming problem\n",
    "            # returns the forecasts of the bottom series\n",
    "            bottom_fcts = np.apply_along_axis(lambda y_hat: solve_qp(G=G, a=a @ y_hat, C=C, b=b)[0], \n",
    "                                              axis=0, \n",
    "                                              arr=y_hat)\n",
    "            if not np.all(bottom_fcts > -1e-8):\n",
    "                raise Exception('nonnegative optimization failed')\n",
    "            # remove negative values close to zero\n",
    "            bottom_fcts = np.clip(np.float32(bottom_fcts), a_min=0, a_max=None)\n",
    "            y_hat = S @ bottom_fcts\n",
    "            return BottomUp().reconcile(S=S, y_hat=y_hat, idx_bottom=idx_bottom, sigmah=sigmah, level=level)\n",
    "        else:\n",
    "            # compute P for free reconciliation\n",
    "            R = S.T @ np.linalg.pinv(W)\n",
    "            P = np.linalg.pinv(R @ S) @ R\n",
    "\n",
    "        return _reconcile(S, P, W, y_hat, sigmah=sigmah, level=level,\n",
    "                          intervals_method=intervals_method,\n",
    "                          samples=samples)\n",
    "\n",
    "    __call__ = reconcile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(MinTrace, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(MinTrace.reconcile, title_level=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This reconciliation algorithm proposed by Wickramasuriya et al. depends on a generalized least squares estimator and an estimator of the covariance matrix of the coherency errors $\\mathbf{W}_{h}$. The Min Trace algorithm minimizes the squared errors for the coherent forecasts under an unbiasedness assumption; the solution has a closed form.\n",
    "\n",
    "$$\\mathbf{P}_{\\text{MinT}} = \\left(\\mathbf{S}^{\\intercal}\\mathbf{W}_{h}\\mathbf{S}\\right)^{-1} \\mathbf{S}^{\\intercal} \\mathbf{W}^{-1}_{h}$$\n",
    "\n",
    "- [Wickramasuriya, S. L., Athanasopoulos, G., & Hyndman, R. J. (2019). Optimal forecast reconciliation for hierarchical and grouped time series through trace minimization. Journal of the American Statistical Association, 114 , 804–819. doi:10.1080/01621459.2018.1448825.](https://robjhyndman.com/publications/mint/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "for method in ['ols', 'wls_struct', 'wls_var', 'mint_shrink']:\n",
    "    for nonnegative in [False, True]:\n",
    "        #test nonnegative behavior\n",
    "        # we should be able to recover the same forecasts\n",
    "        # in this example\n",
    "        cls_min_trace = MinTrace(method=method, nonnegative=nonnegative)\n",
    "        assert cls_min_trace.nonnegative is nonnegative\n",
    "        if cls_min_trace.insample:\n",
    "            assert method in ['wls_var', 'mint_cov', 'mint_shrink']\n",
    "            test_close(\n",
    "                cls_min_trace(\n",
    "                    S=S, \n",
    "                    y_hat=S @ y_hat_bottom, \n",
    "                    y_insample=S @ y_bottom,\n",
    "                    y_hat_insample=S @ y_hat_bottom_insample,\n",
    "                    idx_bottom=idx_bottom if nonnegative else None\n",
    "                )['mean'],\n",
    "                S @ y_hat_bottom\n",
    "            )\n",
    "        else:\n",
    "            test_close(\n",
    "                cls_min_trace(\n",
    "                    S=S, \n",
    "                    y_hat=S @ y_hat_bottom,\n",
    "                    idx_bottom=idx_bottom if nonnegative else None\n",
    "                )['mean'],\n",
    "                S @ y_hat_bottom\n",
    "            )\n",
    "with ExceptionExpected(regex='min_trace (mint_cov)*'):\n",
    "    for nonnegative in [False, True]:\n",
    "        cls_min_trace = MinTrace(method='mint_cov', nonnegative=nonnegative)\n",
    "        cls_min_trace(\n",
    "            S=S, \n",
    "            y_hat=S @ y_hat_bottom, \n",
    "            y_insample=S @ y_bottom,\n",
    "            y_hat_insample=S @ y_hat_bottom_insample,\n",
    "            idx_bottom=idx_bottom if nonnegative else None\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#test levels\n",
    "for method in ['ols', 'wls_struct', 'wls_var', 'mint_shrink']:\n",
    "    for nonnegative in [False, True]:\n",
    "        cls_min_trace = MinTrace(method=method, nonnegative=nonnegative)\n",
    "        test_close(\n",
    "            cls_min_trace(\n",
    "                S=S, \n",
    "                y_hat=S @ y_hat_bottom, \n",
    "                y_insample=S @ y_bottom,\n",
    "                y_hat_insample=S @ y_hat_bottom_insample,\n",
    "                idx_bottom=idx_bottom if nonnegative else None,\n",
    "                sigmah=sigmah,\n",
    "                level=[80,90]\n",
    "            )['mean'],\n",
    "            S @ y_hat_bottom\n",
    "        )\n",
    "        if not nonnegative:\n",
    "            cls_min_trace(\n",
    "                S=S, \n",
    "                y_hat=S @ y_hat_bottom, \n",
    "                y_insample=S @ y_bottom,\n",
    "                y_hat_insample=S @ y_hat_bottom_insample,\n",
    "                sigmah=sigmah,\n",
    "                level=[80,90],\n",
    "                intervals_method='bootstrap',\n",
    "                samples=boot_samples\n",
    "            )\n",
    "            cls_min_trace(\n",
    "                S=S, \n",
    "                y_hat=S @ y_hat_bottom, \n",
    "                y_insample=S @ y_bottom,\n",
    "                y_hat_insample=S @ y_hat_bottom_insample,\n",
    "                sigmah=sigmah,\n",
    "                level=[80,90],\n",
    "                intervals_method='permbu',\n",
    "                samples=permbu_samples\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:DarkBlue\"> 5. Optimal Combination </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def optimal_combination(S: np.ndarray, \n",
    "                        y_hat: np.ndarray,\n",
    "                        method: str,\n",
    "                        idx_bottom: List[int] = None,\n",
    "                        nonnegative: bool = False,\n",
    "                        y_insample: np.ndarray = None,\n",
    "                        y_hat_insample: np.ndarray = None,\n",
    "                        level: Optional[List[int]] = None,\n",
    "                        intervals_method: str = 'normality',\n",
    "                        sigmah: Optional[np.ndarray] = None, \n",
    "                        samples: Optional[np.ndarray] = None):\n",
    "    \n",
    "    return min_trace(S=S, y_hat=y_hat, \n",
    "                     y_insample=y_insample,\n",
    "                     y_hat_insample=y_hat_insample,\n",
    "                     method=method, idx_bottom=idx_bottom,\n",
    "                     nonnegative=nonnegative,\n",
    "                     sigmah=sigmah, level=level,\n",
    "                     intervals_method=intervals_method, \n",
    "                     samples=samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class OptimalCombination(MinTrace):\n",
    "    \"\"\"Optimal Combination Reconciliation Class.\n",
    "\n",
    "    This reconciliation algorithm was proposed by Hyndman et al. 2011, the method uses generalized least squares \n",
    "    estimator using the coherency errors covariance matrix. Consider the covariance of the base forecast \n",
    "    $\\\\textrm{Var}(\\epsilon_{h}) = \\Sigma_{h}$, the $\\mathbf{P}$ matrix of this method is defined by:\n",
    "    $$ \\mathbf{P} = \\\\left(\\mathbf{S}^{\\intercal}\\Sigma_{h}^{\\dagger}\\mathbf{S}\\\\right)^{-1}\\mathbf{S}^{\\intercal}\\Sigma^{\\dagger}_{h}$$\n",
    "    where $\\Sigma_{h}^{\\dagger}$ denotes the variance pseudo-inverse. The method was later proven equivalent to \n",
    "    `MinTrace` variants.\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `method`: str, allowed optimal combination methods: 'ols', 'wls_struct'.<br>\n",
    "    `nonnegative`: bool, reconciled forecasts should be nonnegative?<br>\n",
    "\n",
    "    **References:**<br>\n",
    "    - [Rob J. Hyndman, Roman A. Ahmed, George Athanasopoulos, Han Lin Shang (2010). \\\"Optimal Combination Forecasts for \n",
    "    Hierarchical Time Series\\\".](https://robjhyndman.com/papers/Hierarchical6.pdf).<br>\n",
    "    - [Shanika L. Wickramasuriya, George Athanasopoulos and Rob J. Hyndman (2010). \\\"Optimal Combination Forecasts for \n",
    "    Hierarchical Time Series\\\".](https://robjhyndman.com/papers/MinT.pdf).\n",
    "    - [Wickramasuriya, S.L., Turlach, B.A. & Hyndman, R.J. (2020). \\\"Optimal non-negative\n",
    "    forecast reconciliation\". Stat Comput 30, 1167–1182, \n",
    "    https://doi.org/10.1007/s11222-020-09930-0](https://robjhyndman.com/publications/nnmint/).\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 method: str,\n",
    "                 nonnegative: bool = False):\n",
    "        comb_methods = ['ols', 'wls_struct']\n",
    "        if method not in comb_methods:\n",
    "            raise ValueError(f\"Optimal Combination class does not support method: \\\"{method}\\\"\")\n",
    "\n",
    "        self.method = method\n",
    "        self.nonnegative = nonnegative\n",
    "        self.insample = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(OptimalCombination, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(OptimalCombination.reconcile, title_level=3, name='OptimalCombination.reconcile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "for method in ['ols', 'wls_struct']:\n",
    "    for nonnegative in [False, True]:\n",
    "        #test nonnegative behavior\n",
    "        # we should be able to recover the same forecasts\n",
    "        # in this example\n",
    "        cls_optimal_combination = OptimalCombination(method=method, nonnegative=nonnegative)\n",
    "        test_close(\n",
    "            cls_optimal_combination(\n",
    "                S=S, \n",
    "                y_hat=S @ y_hat_bottom,\n",
    "                idx_bottom=idx_bottom if nonnegative else None\n",
    "            )['mean'],\n",
    "            S @ y_hat_bottom\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#test intervals\n",
    "for method in ['ols', 'wls_struct']:\n",
    "    for nonnegative in [False, True]:\n",
    "        #test nonnegative behavior\n",
    "        # we should be able to recover the same forecasts\n",
    "        # in this example\n",
    "        cls_optimal_combination = OptimalCombination(method=method, nonnegative=nonnegative)\n",
    "        test_close(\n",
    "            cls_optimal_combination(\n",
    "                S=S, \n",
    "                y_hat=S @ y_hat_bottom,\n",
    "                idx_bottom=idx_bottom if nonnegative else None,\n",
    "                sigmah=sigmah,\n",
    "                level=[80, 90]\n",
    "            )['mean'],\n",
    "            S @ y_hat_bottom\n",
    "        )\n",
    "        if not nonnegative:\n",
    "            cls_optimal_combination(\n",
    "                S=S, \n",
    "                y_hat=S @ y_hat_bottom,\n",
    "                sigmah=sigmah,\n",
    "                level=[80, 90],\n",
    "                intervals_method='bootstrap',\n",
    "                samples=boot_samples\n",
    "            )\n",
    "            cls_optimal_combination(\n",
    "                S=S, \n",
    "                y_hat=S @ y_hat_bottom,\n",
    "                sigmah=sigmah,\n",
    "                level=[80, 90],\n",
    "                intervals_method='permbu',\n",
    "                samples=permbu_samples\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:DarkBlue\"> 6. Emp. Risk Minimization </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "@njit\n",
    "def lasso(X: np.ndarray, y: np.ndarray, \n",
    "          lambda_reg: float, max_iters: int = 1_000,\n",
    "          tol: float = 1e-4):\n",
    "    # lasso cyclic coordinate descent\n",
    "    n, feats = X.shape\n",
    "    norms = (X ** 2).sum(axis=0)\n",
    "    beta = np.zeros(feats, dtype=np.float32)\n",
    "    beta_changes = np.zeros(feats, dtype=np.float32)\n",
    "    residuals = y.copy()\n",
    "    \n",
    "    for it in range(max_iters):\n",
    "        for i, betai in enumerate(beta):\n",
    "            # is feature is close to zero, we \n",
    "            # continue to the next.\n",
    "            # in this case is optimal betai= 0\n",
    "            if abs(norms[i]) < 1e-8:\n",
    "                continue\n",
    "            xi = X[:, i]\n",
    "            #we calculate the normalized derivative\n",
    "            rho = betai + xi.flatten().dot(residuals) / norms[i] #(norms[i] + 1e-3)\n",
    "            #soft threshold\n",
    "            beta[i] = np.sign(rho) * max(np.abs(rho) - lambda_reg * n / norms[i], 0.)#(norms[i] + 1e-3), 0.)\n",
    "            beta_changes[i] = np.abs(betai - beta[i])\n",
    "            if beta[i] != betai:\n",
    "                residuals += (betai - beta[i]) * xi\n",
    "        if max(beta_changes) < tol:\n",
    "            break\n",
    "    #print(it)\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ERM:\n",
    "    \"\"\"Optimal Combination Reconciliation Class.\n",
    "\n",
    "    The Empirical Risk Minimization reconciliation strategy relaxes the unbiasedness assumptions from\n",
    "    previous reconciliation methods like MinT and optimizes square errors between the reconciled predictions\n",
    "    and the validation data to obtain an optimal reconciliation matrix P.\n",
    "    \n",
    "    The exact solution for $\\mathbf{P}$ (`method='closed'`) follows the expression:\n",
    "    $$\\mathbf{P}^{*} = \\\\left(\\mathbf{S}^{\\intercal}\\mathbf{S}\\\\right)^{-1}\\mathbf{Y}^{\\intercal}\\hat{\\mathbf{Y}}\\\\left(\\hat{\\mathbf{Y}}\\hat{\\mathbf{Y}}\\\\right)^{-1}$$\n",
    "\n",
    "    The alternative Lasso regularized $\\mathbf{P}$ solution (`method='reg_bu'`) is useful when the observations \n",
    "    of validation data is limited or the exact solution has low numerical stability.\n",
    "    $$\\mathbf{P}^{*} = \\\\text{argmin}_{\\mathbf{P}} ||\\mathbf{Y}-\\mathbf{S} \\mathbf{P} \\hat{Y} ||^{2}_{2} + \\lambda ||\\mathbf{P}-\\mathbf{P}_{\\\\text{BU}}||_{1}$$\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `method`: str, one of `closed`, `reg` and `reg_bu`.<br>\n",
    "    `lambda_reg`: float, l1 regularizer for `reg` and `reg_bu`.<br>\n",
    "\n",
    "    **References:**<br>\n",
    "    - [Ben Taieb, S., & Koo, B. (2019). Regularized regression for hierarchical forecasting without \n",
    "    unbiasedness conditions. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge \n",
    "    Discovery & Data Mining KDD '19 (p. 1337{1347). New York, NY, USA: Association for Computing Machinery.](https://doi.org/10.1145/3292500.3330976).<br>\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 method: str,\n",
    "                 lambda_reg: float = 1e-2):\n",
    "        self.method = method\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.insample = True\n",
    "\n",
    "    def reconcile(self, \n",
    "                  S: np.ndarray,\n",
    "                  y_hat: np.ndarray,\n",
    "                  y_insample: np.ndarray,\n",
    "                  y_hat_insample: np.ndarray,\n",
    "                  idx_bottom: np.ndarray,\n",
    "                  level: Optional[List[int]] = None,\n",
    "                  intervals_method: str = 'normality',\n",
    "                  sigmah: Optional[np.ndarray] = None,\n",
    "                  samples: Optional[np.ndarray] = None):\n",
    "        \"\"\"ERM Reconciliation Method.\n",
    "\n",
    "        **Parameters:**<br>\n",
    "        `S`: Summing matrix of size (`base`, `bottom`).<br>\n",
    "        `y_hat`: Forecast values of size (`base`, `horizon`).<br>\n",
    "        `y_insample`: Train values of size (`base`, `insample_size`).<br>\n",
    "        `y_hat_insample`: Insample train predictions of size (`base`, `insample_size`).<br>\n",
    "        `idx_bottom`: Indices corresponding to the bottom level of `S`, size (`bottom`).<br>\n",
    "        `level`: float list 0-100, confidence levels for prediction intervals.<br>\n",
    "        `intervals_method`: str, method used to calculate prediction intervals, one of `normality`, `bootstrap`, `permbu`.<br>\n",
    "        `sigmah`: Estimate of the standard deviation of the h-step forecast of size (`base`, `horizon`)<br>\n",
    "        `samples`: Samples for prediction intevals of size (`n_samples`, `base`, `horizon`).<br>\n",
    "\n",
    "        **Returns:**<br>\n",
    "        `y_tilde`: Reconciliated y_hat using the ERM approach.\n",
    "        \"\"\"\n",
    "        n_hiers, n_bottom = S.shape\n",
    "        # y_hat_insample shape (n_hiers, obs)\n",
    "        # remove obs with nan values\n",
    "        nan_idx = np.isnan(y_hat_insample).any(axis=0)\n",
    "        y_insample = y_insample[:, ~nan_idx]\n",
    "        y_hat_insample = y_hat_insample[:, ~nan_idx]\n",
    "        #only using h validation steps to avoid \n",
    "        #computational burden\n",
    "        #print(y_hat.shape)\n",
    "        h = min(y_hat.shape[1], y_hat_insample.shape[1])\n",
    "        y_hat_insample = y_hat_insample[:, -h:] # shape (h, n_hiers)\n",
    "        y_insample = y_insample[:, -h:]\n",
    "        if self.method == 'closed':\n",
    "            B = np.linalg.inv(S.T @ S) @ S.T @ y_insample\n",
    "            B = B.T\n",
    "            P = np.linalg.pinv(y_hat_insample.T) @ B\n",
    "            P = P.T\n",
    "        elif self.method in ['reg', 'reg_bu']:\n",
    "            X = np.kron(np.array(S, order='F'), np.array(y_hat_insample.T, order='F'))\n",
    "            Pbu = np.zeros_like(S)\n",
    "            if method == 'reg_bu':\n",
    "                Pbu[idx_bottom] = S[idx_bottom]\n",
    "            Pbu = Pbu.T\n",
    "            Y = y_insample.T.flatten(order='F') - X @ Pbu.T.flatten(order='F')\n",
    "            if self.lambda_reg is None:\n",
    "                lambda_reg = np.max(np.abs(X.T.dot(Y)))\n",
    "            else:\n",
    "                lambda_reg = self.lambda_reg\n",
    "            P = lasso(X, Y, lambda_reg)\n",
    "            P = P + Pbu.T.flatten(order='F')\n",
    "            P = P.reshape(-1, n_bottom, order='F').T\n",
    "        else:\n",
    "            raise ValueError(f'Unkown reconciliation method {method}')\n",
    "\n",
    "        W = np.eye(n_hiers, dtype=np.float32)\n",
    "\n",
    "        return _reconcile(S, P, W, y_hat, sigmah=sigmah, level=level, \n",
    "                          intervals_method=intervals_method, \n",
    "                          samples=samples)\n",
    "\n",
    "    __call__ = reconcile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(ERM, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(ERM.reconcile, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "for method in ['reg_bu']:\n",
    "    cls_erm = ERM(method=method, lambda_reg=None)\n",
    "    test_close(\n",
    "        cls_erm(\n",
    "            S=S, \n",
    "            y_hat=S @ y_hat_bottom,\n",
    "            y_insample=S @ y_bottom,\n",
    "            y_hat_insample=S @ y_hat_bottom_insample,\n",
    "            idx_bottom=idx_bottom\n",
    "        )['mean'],\n",
    "        S @ y_hat_bottom\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test intervals\n",
    "for method in ['reg_bu']:\n",
    "    cls_erm = ERM(method=method, lambda_reg=None)\n",
    "    test_close(\n",
    "        cls_erm(\n",
    "            S=S, \n",
    "            y_hat=S @ y_hat_bottom,\n",
    "            y_insample=S @ y_bottom,\n",
    "            y_hat_insample=S @ y_hat_bottom_insample,\n",
    "            idx_bottom=idx_bottom,\n",
    "            sigmah=sigmah,\n",
    "            level=[80, 90]\n",
    "        )['mean'],\n",
    "        S @ y_hat_bottom\n",
    "    )\n",
    "    cls_erm(\n",
    "        S=S, \n",
    "        y_hat=S @ y_hat_bottom,\n",
    "        y_insample=S @ y_bottom,\n",
    "        y_hat_insample=S @ y_hat_bottom_insample,\n",
    "        idx_bottom=idx_bottom,\n",
    "        sigmah=sigmah,\n",
    "        level=[80, 90],\n",
    "        intervals_method='normality',\n",
    "        samples=boot_samples\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:DarkBlue\"> References </span>\n",
    "\n",
    "### General Reconciliation\n",
    "- [Orcutt, G.H., Watts, H.W., & Edwards, J.B.(1968). Data aggregation and information loss. The American \n",
    "Economic Review, 58 , 773{787)](http://www.jstor.org/stable/1815532).\n",
    "- [Disaggregation methods to expedite product line forecasting. Journal of Forecasting, 9 , 233–254. \n",
    "doi:10.1002/for.3980090304](https://onlinelibrary.wiley.com/doi/abs/10.1002/for.3980090304).<br>\n",
    "- [An investigation of aggregate variable time series forecast strategies with specific subaggregate \n",
    "time series statistical correlation. Computers and Operations Research, 26 , 1133–1149. \n",
    "doi:10.1016/S0305-0548(99)00017-9](https://doi.org/10.1016/S0305-0548(99)00017-9).\n",
    "- [Hyndman, R.J., & Athanasopoulos, G. (2021). \"Forecasting: principles and practice, 3rd edition: \n",
    "Chapter 11: Forecasting hierarchical and grouped series.\". OTexts: Melbourne, Australia. OTexts.com/fpp3 \n",
    "Accessed on July 2022.](https://otexts.com/fpp3/hierarchical.html)\n",
    "\n",
    "### Optimal Reconciliation\n",
    "- [Rob J. Hyndman, Roman A. Ahmed, George Athanasopoulos, Han Lin Shang. \"Optimal Combination Forecasts for \n",
    "Hierarchical Time Series\" (2010).](https://robjhyndman.com/papers/Hierarchical6.pdf).<br>\n",
    "- [Shanika L. Wickramasuriya, George Athanasopoulos and Rob J. Hyndman. \"Optimal Combination Forecasts for \n",
    "Hierarchical Time Series\" (2010).](https://robjhyndman.com/papers/MinT.pdf).\n",
    "- [Ben Taieb, S., & Koo, B. (2019). Regularized regression for hierarchical forecasting without \n",
    "unbiasedness conditions. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge \n",
    "Discovery & Data Mining KDD '19 (p. 1337{1347). New York, NY, USA: Association for Computing Machinery.](https://doi.org/10.1145/3292500.3330976).\n",
    "\n",
    "### Hierarchical Probabilistic Coherent Predictions\n",
    "- [Puwasala Gamakumara Ph. D. dissertation. Monash University, Econometrics and Business Statistics. \"Probabilistic Forecast Reconciliation\"](https://bridges.monash.edu/articles/thesis/Probabilistic_Forecast_Reconciliation_Theory_and_Applications/11869533)\n",
    "- [Taieb, Souhaib Ben and Taylor, James W and Hyndman, Rob J. (2017). Coherent probabilistic forecasts for hierarchical time series. International conference on machine learning ICML.](https://proceedings.mlr.press/v70/taieb17a.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
