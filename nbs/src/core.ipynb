{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HierarchicalForecast contains pure Python implementations of hierarchical reconciliation methods as well as a `core.HierarchicalReconciliation` wrapper class that enables easy interaction with these methods through pandas DataFrames containing the hierarchical time series and the base predictions.\n",
    "\n",
    "The `core.HierarchicalReconciliation` reconciliation class operates with the hierarchical time series pd.DataFrame `Y_df`, the base predictions pd.DataFrame `Y_hat_df`, the aggregation constraints matrix `S`. For more information on the creation of aggregation constraints matrix see the utils [aggregation method](https://nixtla.github.io/hierarchicalforecast/utils.html#aggregate).<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import copy\n",
    "import re\n",
    "import reprlib\n",
    "import time\n",
    "\n",
    "from hierarchicalforecast.methods import HReconciler\n",
    "from inspect import signature\n",
    "from narwhals.typing import Frame, FrameT\n",
    "from scipy.stats import norm\n",
    "from scipy import sparse\n",
    "from typing import Optional\n",
    "\n",
    "import narwhals as nw\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from fastcore.test import test_close, test_eq, test_fail\n",
    "from nbdev.showdoc import show_doc\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _build_fn_name(fn) -> str:\n",
    "    fn_name = type(fn).__name__\n",
    "    func_params = fn.__dict__\n",
    "\n",
    "    # Take default parameter out of names\n",
    "    args_to_remove = ['insample', 'num_threads']\n",
    "    if not func_params.get('nonnegative', False):\n",
    "        args_to_remove.append('nonnegative')\n",
    "\n",
    "    if fn_name == 'MinTrace' and \\\n",
    "        func_params['method']=='mint_shrink':\n",
    "        if func_params['mint_shr_ridge'] == 2e-8:\n",
    "            args_to_remove.append('mint_shr_ridge')\n",
    "\n",
    "    func_params = [f'{name}-{value}' for name, value in func_params.items() if name not in args_to_remove]\n",
    "    if func_params:\n",
    "        fn_name += '_' + '_'.join(func_params)\n",
    "    return fn_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test fn name\n",
    "from hierarchicalforecast.methods import BottomUp, MinTrace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "test_eq(_build_fn_name(BottomUp()), 'BottomUp')\n",
    "test_eq(\n",
    "    _build_fn_name(MinTrace(method='ols')), \n",
    "    'MinTrace_method-ols'\n",
    ")\n",
    "test_eq(\n",
    "    _build_fn_name(MinTrace(method='ols', nonnegative=True)), \n",
    "    'MinTrace_method-ols_nonnegative-True'\n",
    ")\n",
    "test_eq(\n",
    "    _build_fn_name(MinTrace(method='mint_shr')), \n",
    "    'MinTrace_method-mint_shr'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HierarchicalReconciliation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _reverse_engineer_sigmah(Y_hat_df: Frame, \n",
    "                             y_hat: np.ndarray, \n",
    "                             model_name: str,\n",
    "                             id_col: str = \"unique_id\",\n",
    "                             time_col: str = \"ds\",\n",
    "                             target_col: str = \"y\",\n",
    "                             num_samples: int = 200) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    This function assumes that the model creates prediction intervals\n",
    "    under a normality with the following the Equation:\n",
    "    $\\hat{y}_{t+h} + c \\hat{sigma}_{h}$\n",
    "\n",
    "    In the future, we might deprecate this function in favor of a \n",
    "    direct usage of an estimated $\\hat{sigma}_{h}$\n",
    "    \"\"\"\n",
    "\n",
    "    drop_cols = [time_col]\n",
    "    if target_col in Y_hat_df.columns:\n",
    "        drop_cols.append(target_col)\n",
    "    if model_name+'-median' in Y_hat_df.columns:\n",
    "        drop_cols.append(model_name+'-median')\n",
    "    model_names = [c for c in Y_hat_df.columns if c not in drop_cols]\n",
    "    pi_model_names = [name for name in model_names if ('-lo' in name or '-hi' in name)]\n",
    "    pi_model_name = [pi_name for pi_name in pi_model_names if model_name in pi_name]\n",
    "    pi = len(pi_model_name) > 0\n",
    "\n",
    "    n_series = Y_hat_df[id_col].n_unique()\n",
    "\n",
    "    if not pi:\n",
    "        raise ValueError(f'Please include `{model_name}` prediction intervals in `Y_hat_df`')\n",
    "\n",
    "    pi_col = pi_model_name[0]\n",
    "    sign = -1 if 'lo' in pi_col else 1\n",
    "    level_cols = re.findall('[\\d]+[.,\\d]+|[\\d]*[.][\\d]+|[\\d]+', pi_col)\n",
    "    level_col = float(level_cols[-1])\n",
    "    z = norm.ppf(0.5 + level_col / num_samples)\n",
    "    sigmah = Y_hat_df[pi_col].to_numpy().reshape(n_series,-1)\n",
    "    sigmah = sign * (sigmah - y_hat) / z\n",
    "\n",
    "    return sigmah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class HierarchicalReconciliation:\n",
    "    \"\"\"Hierarchical Reconciliation Class.\n",
    "\n",
    "    The `core.HierarchicalReconciliation` class allows you to efficiently fit multiple \n",
    "    HierarchicaForecast methods for a collection of time series and base predictions stored in \n",
    "    pandas DataFrames. The `Y_df` dataframe identifies series and datestamps with the unique_id and ds columns while the\n",
    "    y column denotes the target time series variable. The `Y_h` dataframe stores the base predictions, \n",
    "    example ([AutoARIMA](https://nixtla.github.io/statsforecast/models.html#autoarima), [ETS](https://nixtla.github.io/statsforecast/models.html#autoets), etc.).\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `reconcilers`: A list of instantiated classes of the [reconciliation methods](https://nixtla.github.io/hierarchicalforecast/methods.html) module .<br>\n",
    "\n",
    "    **References:**<br>\n",
    "    [Rob J. Hyndman and George Athanasopoulos (2018). \\\"Forecasting principles and practice, Hierarchical and Grouped Series\\\".](https://otexts.com/fpp3/hierarchical.html)\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 reconcilers: list[HReconciler]):\n",
    "        self.reconcilers = reconcilers\n",
    "        self.orig_reconcilers = copy.deepcopy(reconcilers) # TODO: elegant solution\n",
    "    \n",
    "    def _prepare_fit(self,\n",
    "                     Y_hat_nw: Frame,\n",
    "                     S_nw: Frame,\n",
    "                     Y_nw: Optional[Frame],\n",
    "                     tags: dict[str, np.ndarray],\n",
    "                     level: Optional[list[int]] = None,\n",
    "                     intervals_method: str = 'normality',\n",
    "                     id_col: str = \"unique_id\",\n",
    "                     time_col: str = \"ds\", \n",
    "                     target_col: str = \"y\",                      \n",
    "                     ) -> tuple[FrameT, FrameT, FrameT, list[str]]:\n",
    "        \"\"\"\n",
    "        Performs preliminary wrangling and protections\n",
    "        \"\"\"\n",
    "        Y_hat_nw_cols = Y_hat_nw.columns\n",
    "        S_nw_cols = S_nw.columns\n",
    "\n",
    "        #-------------------------------- Match Y_hat/Y/S index order --------------------------------#\n",
    "        # TODO: This is now a bit slow as we always sort.\n",
    "        S_nw = S_nw.with_columns(**{f\"{id_col}_id\": np.arange(len(S_nw))})\n",
    "\n",
    "        Y_hat_nw = Y_hat_nw.join(S_nw[[id_col, f\"{id_col}_id\"]], on=id_col, how='left')\n",
    "        Y_hat_nw = Y_hat_nw.sort(by=[f\"{id_col}_id\", time_col])\n",
    "        Y_hat_nw = Y_hat_nw[Y_hat_nw_cols]\n",
    "\n",
    "        if Y_nw is not None:\n",
    "            Y_nw_cols = Y_nw.columns\n",
    "            Y_nw = Y_nw.join(S_nw[[id_col, f\"{id_col}_id\"]], on=id_col, how='left')\n",
    "            Y_nw = Y_nw.sort(by=[f\"{id_col}_id\", time_col])\n",
    "            Y_nw = Y_nw[Y_nw_cols]\n",
    "\n",
    "        S_nw = S_nw[S_nw_cols]\n",
    "\n",
    "        #----------------------------------- Check Input's Validity ----------------------------------#\n",
    "\n",
    "        # Check input's validity\n",
    "        if intervals_method not in ['normality', 'bootstrap', 'permbu']:\n",
    "            raise ValueError(f'Unknown interval method: {intervals_method}')\n",
    "\n",
    "        if Y_nw is None:\n",
    "            for reconciler in self.orig_reconcilers:\n",
    "                if reconciler.insample:\n",
    "                    reconciler_name = _build_fn_name(reconciler)\n",
    "                    raise ValueError(f'You need to provide `Y_df` for reconciler {reconciler_name}')\n",
    "            if intervals_method in ['bootstrap', 'permbu']:\n",
    "                raise ValueError('You need to provide `Y_df`.')\n",
    "        \n",
    "        # Protect level list\n",
    "        if (level is not None):\n",
    "            level_outside_domain = not all(0 <= x < 100 for x in level)\n",
    "            if level_outside_domain and (intervals_method in ['normality', 'permbu']):\n",
    "                raise ValueError(\"Level must be a list containing floating values in the interval [0, 100).\")\n",
    "\n",
    "        # Declare output names\n",
    "        model_names = [col for col in Y_hat_nw.columns if col not in [id_col, time_col, target_col]]\n",
    "\n",
    "        # Ensure numeric columns\n",
    "        for model in model_names:\n",
    "            if not Y_hat_nw.schema[model].is_numeric():\n",
    "                raise ValueError(f\"Column `{model}` in `Y_hat_df` contains non-numeric values. Make sure no column in `Y_hat_df` contains non-numeric values.\")\n",
    "            if Y_hat_nw[model].is_null().any():\n",
    "                raise ValueError(f\"Column `{model}` in `Y_hat_df` contains null values. Make sure no column in `Y_hat_df` contains null values.\")\n",
    "\n",
    "        # TODO: Complete y_hat_insample protection\n",
    "        model_names = [name for name in model_names if not ('-lo' in name or '-hi' in name or '-median' in name)]        \n",
    "        if intervals_method in ['bootstrap', 'permbu'] and Y_nw is not None:\n",
    "            missing_models = set(model_names) - set(Y_nw.columns)\n",
    "            if len(missing_models) > 0:\n",
    "                raise ValueError(f\"Check `Y_df` columns, {reprlib.repr(missing_models)} must be in `Y_df` columns.\")\n",
    "\n",
    "        # Assert S is an identity matrix at the bottom\n",
    "        S_nw_cols.remove(id_col)\n",
    "        if not np.allclose(S_nw[S_nw_cols][-len(S_nw_cols):], np.eye(len(S_nw_cols))):\n",
    "            raise ValueError(f\"The bottom {S_nw.shape[1]}x{S_nw.shape[1]} part of S must be an identity matrix.\")\n",
    "\n",
    "        # Check Y_hat_df\\S_df series difference\n",
    "        # TODO: this logic should be method specific\n",
    "        S_diff = set(S_nw[id_col]) - set(Y_hat_nw[id_col])\n",
    "        Y_hat_diff = set(Y_hat_nw[id_col]) - set(S_nw[id_col])\n",
    "        if S_diff:\n",
    "            raise ValueError(f'There are unique_ids in S_df that are not in Y_hat_df: {reprlib.repr(S_diff)}')\n",
    "        if Y_hat_diff:\n",
    "            raise ValueError(f'There are unique_ids in Y_hat_df that are not in S_df: {reprlib.repr(Y_hat_diff)}')\n",
    "\n",
    "        if Y_nw is not None:\n",
    "            Y_diff = set(Y_nw[id_col]) - set(Y_hat_nw[id_col])\n",
    "            Y_hat_diff = set(Y_hat_nw[id_col]) - set(Y_nw[id_col])\n",
    "            if Y_diff:\n",
    "                raise ValueError(f'There are unique_ids in Y_df that are not in Y_hat_df: {reprlib.repr(Y_diff)}')\n",
    "            if Y_hat_diff:\n",
    "                raise ValueError(f'There are unique_ids in Y_hat_df that are not in Y_df: {reprlib.repr(Y_hat_diff)}')\n",
    "\n",
    "        # Same Y_hat_df/S_df/Y_df's unique_ids. Order is guaranteed by sorting.\n",
    "        # TODO: this logic should be method specific\n",
    "        unique_ids = Y_hat_nw[id_col].unique().to_numpy()\n",
    "        S_nw = S_nw.filter(nw.col(id_col).is_in(unique_ids))\n",
    "\n",
    "        return Y_hat_nw, S_nw, Y_nw, model_names\n",
    "\n",
    "    def _prepare_Y(self, \n",
    "                          Y_nw: Frame, \n",
    "                          S_nw: Frame, \n",
    "                          is_balanced: bool = True,\n",
    "                          id_col: str = \"unique_id\",\n",
    "                          time_col: str = \"ds\", \n",
    "                          target_col: str = \"y\", \n",
    "                          ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Prepare Y data.\n",
    "        \"\"\"\n",
    "        if is_balanced:\n",
    "            Y = Y_nw[target_col].to_numpy().reshape(len(S_nw), -1)\n",
    "        else:\n",
    "            Y_pivot = Y_nw.pivot(on=time_col, index=id_col, values=target_col, sort_columns=True).sort(by=id_col)\n",
    "            Y_pivot_cols_ex_id_col = Y_pivot.columns\n",
    "            Y_pivot_cols_ex_id_col.remove(id_col)\n",
    "\n",
    "            # TODO: check if this is the best way to do it - it's reasonably fast to ensure Y_pivot has same order as S_nw\n",
    "            pos_in_Y = np.searchsorted(Y_pivot[id_col].to_numpy(), S_nw[id_col].to_numpy())\n",
    "            Y_pivot = Y_pivot.select(nw.col(Y_pivot_cols_ex_id_col))\n",
    "            Y_pivot = Y_pivot[pos_in_Y]\n",
    "            Y = Y_pivot.to_numpy()\n",
    "\n",
    "        # TODO: the result is a Fortran contiguous array, see if we can avoid the below copy (I don't think so)\n",
    "        Y = np.ascontiguousarray(Y, dtype=np.float64)\n",
    "        return Y\n",
    "\n",
    "\n",
    "    def reconcile(self, \n",
    "                  Y_hat_df: Frame,\n",
    "                  S: Frame,\n",
    "                  tags: dict[str, np.ndarray],\n",
    "                  Y_df: Optional[Frame] = None,\n",
    "                  level: Optional[list[int]] = None,\n",
    "                  intervals_method: str = 'normality',\n",
    "                  num_samples: int = -1,\n",
    "                  seed: int = 0,\n",
    "                  is_balanced: bool = False,\n",
    "                  id_col: str = \"unique_id\",\n",
    "                  time_col: str = \"ds\", \n",
    "                  target_col: str = \"y\",                   \n",
    "        ) -> FrameT:\n",
    "        \"\"\"Hierarchical Reconciliation Method.\n",
    "\n",
    "        The `reconcile` method is analogous to SKLearn `fit_predict` method, it \n",
    "        applies different reconciliation techniques instantiated in the `reconcilers` list.\n",
    "\n",
    "        Most reconciliation methods can be described by the following convenient \n",
    "        linear algebra notation:\n",
    "\n",
    "        $$\\\\tilde{\\mathbf{y}}_{[a,b],\\\\tau} = \\mathbf{S}_{[a,b][b]} \\mathbf{P}_{[b][a,b]} \\hat{\\mathbf{y}}_{[a,b],\\\\tau}$$\n",
    "\n",
    "        where $a, b$ represent the aggregate and bottom levels, $\\mathbf{S}_{[a,b][b]}$ contains\n",
    "        the hierarchical aggregation constraints, and $\\mathbf{P}_{[b][a,b]}$ varies across \n",
    "        reconciliation methods. The reconciled predictions are $\\\\tilde{\\mathbf{y}}_{[a,b],\\\\tau}$, and the \n",
    "        base predictions $\\hat{\\mathbf{y}}_{[a,b],\\\\tau}$.\n",
    "\n",
    "        **Parameters:**<br>\n",
    "        `Y_hat_df`: DataFrame, base forecasts with columns ['unique_id', 'ds'] and models to reconcile.<br>\n",
    "        `Y_df`: DataFrame, training set of base time series with columns `['unique_id', 'ds', 'y']`.<br>\n",
    "        If a class of `self.reconciles` receives `y_hat_insample`, `Y_df` must include them as columns.<br>\n",
    "        `S`: DataFrame with summing matrix of size `(base, bottom)`, see [aggregate method](https://nixtla.github.io/hierarchicalforecast/utils.html#aggregate).<br>\n",
    "        `tags`: Each key is a level and its value contains tags associated to that level.<br>\n",
    "        `level`: positive float list [0,100), confidence levels for prediction intervals.<br>\n",
    "        `intervals_method`: str, method used to calculate prediction intervals, one of `normality`, `bootstrap`, `permbu`.<br>\n",
    "        `num_samples`: int=-1, if positive return that many probabilistic coherent samples.\n",
    "        `seed`: int=0, random seed for numpy generator's replicability.<br>\n",
    "        `is_balanced`: bool=False, wether `Y_df` is balanced, set it to True to speed things up if `Y_df` is balanced.<br>\n",
    "        `id_col` : str='unique_id', column that identifies each serie.<br>\n",
    "        `time_col` : str='ds', column that identifies each timestep, its values can be timestamps or integers.<br>\n",
    "        `target_col` : str='y', column that contains the target.<br>\n",
    "\n",
    "        **Returns:**<br>\n",
    "        `Y_tilde_df`: DataFrame, with reconciled predictions.\n",
    "        \"\"\"\n",
    "        # To Narwhals\n",
    "        Y_hat_nw = nw.from_native(Y_hat_df)\n",
    "        S_nw = nw.from_native(S)\n",
    "        if Y_df is not None:\n",
    "            Y_nw = nw.from_native(Y_df)\n",
    "        else:\n",
    "            Y_nw = None\n",
    "\n",
    "        # Check input's validity and sort dataframes\n",
    "        Y_hat_nw, S_nw, Y_nw, self.model_names = \\\n",
    "                    self._prepare_fit(Y_hat_nw=Y_hat_nw,\n",
    "                                      S_nw=S_nw,\n",
    "                                      Y_nw=Y_nw,\n",
    "                                      tags=tags,\n",
    "                                      level=level,\n",
    "                                      intervals_method=intervals_method,\n",
    "                                      id_col=id_col,\n",
    "                                      time_col=time_col,\n",
    "                                      target_col=target_col,                                     \n",
    "                                      )\n",
    "\n",
    "        # Initialize reconciler arguments\n",
    "        reconciler_args = dict(\n",
    "            idx_bottom=np.arange(len(S_nw))[-S_nw.shape[1]:],\n",
    "            tags={key: S_nw.with_columns(nw.col(id_col).is_in(val).alias(\"in_cols\"))[\"in_cols\"].to_numpy().nonzero()[0] for key, val in tags.items()},\n",
    "        )\n",
    "\n",
    "        any_sparse = any([method.is_sparse_method for method in self.reconcilers])\n",
    "        S_nw_cols_ex_id_col = S_nw.columns\n",
    "        S_nw_cols_ex_id_col.remove(id_col)\n",
    "        if any_sparse:\n",
    "            if not nw.dependencies.is_pandas_dataframe(Y_hat_df) or not nw.dependencies.is_pandas_dataframe(S):\n",
    "                raise ValueError(\"You have one or more sparse reconciliation methods. Please convert `S` and `Y_hat_df` to a pandas DataFrame.\")\n",
    "            try:\n",
    "                S_for_sparse = sparse.csr_matrix(S_nw.select(nw.col(S_nw_cols_ex_id_col)).to_native().sparse.to_coo())                \n",
    "            except AttributeError:\n",
    "                S_for_sparse = sparse.csr_matrix(S_nw.select(nw.col(S_nw_cols_ex_id_col)).to_numpy().astype(np.float64, copy=False))\n",
    "\n",
    "        if Y_nw is not None:\n",
    "            if any_sparse and not nw.dependencies.is_pandas_dataframe(Y_df):\n",
    "                raise ValueError(\"You have one or more sparse reconciliation methods. Please convert `Y_df` to a pandas DataFrame.\")      \n",
    "            y_insample = self._prepare_Y(Y_nw=Y_nw, \n",
    "                                         S_nw=S_nw, \n",
    "                                         is_balanced=is_balanced, \n",
    "                                         id_col=id_col, \n",
    "                                         time_col=time_col, \n",
    "                                         target_col=target_col)     \n",
    "            reconciler_args['y_insample'] = y_insample\n",
    "\n",
    "        Y_tilde_nw = nw.maybe_reset_index(Y_hat_nw.clone())\n",
    "        self.execution_times = {}\n",
    "        self.level_names = {}\n",
    "        self.sample_names = {}\n",
    "        for reconciler, name_copy in zip(self.reconcilers, self.orig_reconcilers):\n",
    "            reconcile_fn_name = _build_fn_name(name_copy)\n",
    "\n",
    "            if reconciler.is_sparse_method:\n",
    "                reconciler_args[\"S\"] = S_for_sparse\n",
    "            else:\n",
    "                reconciler_args[\"S\"] = S_nw.select(nw.col(S_nw_cols_ex_id_col))\\\n",
    "                                           .to_numpy()\\\n",
    "                                           .astype(np.float64, copy=False)\n",
    "\n",
    "            for model_name in self.model_names:\n",
    "                start = time.time()\n",
    "                recmodel_name = f'{model_name}/{reconcile_fn_name}'\n",
    "\n",
    "                model_cols = [id_col, time_col, model_name]\n",
    "\n",
    "                # TODO: the below should be method specific\n",
    "                y_hat = self._prepare_Y(Y_nw=Y_hat_nw[model_cols], \n",
    "                                        S_nw=S_nw, \n",
    "                                        is_balanced=True, \n",
    "                                        id_col=id_col, \n",
    "                                        time_col=time_col, \n",
    "                                        target_col=model_name)\n",
    "                reconciler_args['y_hat'] = y_hat\n",
    "\n",
    "                if Y_nw is not None and model_name in Y_nw.columns:\n",
    "                    y_hat_insample = self._prepare_Y(Y_nw=Y_nw[model_cols], \n",
    "                                        S_nw=S_nw, \n",
    "                                        is_balanced=is_balanced, \n",
    "                                        id_col=id_col, \n",
    "                                        time_col=time_col, \n",
    "                                        target_col=model_name)   \n",
    "                    reconciler_args['y_hat_insample'] = y_hat_insample\n",
    "\n",
    "                if level is not None:\n",
    "                    reconciler_args['intervals_method'] = intervals_method\n",
    "                    reconciler_args['num_samples'] = 200\n",
    "                    reconciler_args['seed'] = seed\n",
    "\n",
    "                    if intervals_method in ['normality', 'permbu']:\n",
    "                        sigmah = _reverse_engineer_sigmah(Y_hat_df=Y_hat_nw,\n",
    "                                    y_hat=y_hat, model_name=model_name, \n",
    "                                    id_col=id_col, time_col=time_col, \n",
    "                                    target_col=target_col, num_samples=reconciler_args['num_samples'])\n",
    "                        reconciler_args['sigmah'] = sigmah\n",
    "\n",
    "\n",
    "                # Mean and Probabilistic reconciliation\n",
    "                kwargs_ls = [key for key in signature(reconciler.fit_predict).parameters if key in reconciler_args.keys()]\n",
    "                kwargs = {key: reconciler_args[key] for key in kwargs_ls}\n",
    "                \n",
    "                if (level is not None) and (num_samples > 0):\n",
    "                    # Store reconciler's memory to generate samples\n",
    "                    reconciler = reconciler.fit(**kwargs)\n",
    "                    fcsts_model = reconciler.predict(S=reconciler_args['S'], \n",
    "                                                     y_hat=reconciler_args['y_hat'], level=level)\n",
    "                else:\n",
    "                    # Memory efficient reconciler's fit_predict\n",
    "                    fcsts_model = reconciler(**kwargs, level=level)\n",
    "\n",
    "                # Parse final outputs\n",
    "                Y_tilde_nw = Y_tilde_nw.with_columns(**{recmodel_name: fcsts_model[\"mean\"].flatten()})\n",
    "                \n",
    "                if intervals_method in ['bootstrap', 'normality', 'permbu'] and level is not None:\n",
    "                    level.sort()\n",
    "                    lo_names = [f'{recmodel_name}-lo-{lv}' for lv in reversed(level)]\n",
    "                    hi_names = [f'{recmodel_name}-hi-{lv}' for lv in level]\n",
    "                    self.level_names[recmodel_name] = lo_names + hi_names\n",
    "                    sorted_quantiles = np.reshape(fcsts_model['quantiles'], (len(Y_tilde_nw), -1))\n",
    "                    y_tilde = dict(zip(self.level_names[recmodel_name], sorted_quantiles.T))\n",
    "                    Y_tilde_nw = Y_tilde_nw.with_columns(**y_tilde)\n",
    "\n",
    "                    if num_samples > 0:\n",
    "                        samples = reconciler.sample(num_samples=num_samples)\n",
    "                        self.sample_names[recmodel_name] = [f'{recmodel_name}-sample-{i}' for i in range(num_samples)]\n",
    "                        samples = np.reshape(samples, (len(Y_tilde_nw),-1)) \n",
    "                        y_tilde = dict(zip(self.sample_names[recmodel_name], samples.T))\n",
    "                        Y_tilde_nw = Y_tilde_nw.with_columns(**y_tilde)\n",
    "                      \n",
    "                end = time.time()\n",
    "                self.execution_times[f'{model_name}/{reconcile_fn_name}'] = (end - start)\n",
    "\n",
    "        Y_tilde_df = Y_tilde_nw.to_native()\n",
    "\n",
    "        return Y_tilde_df\n",
    "\n",
    "    def bootstrap_reconcile(self,\n",
    "                            Y_hat_df: Frame,\n",
    "                            S_df: Frame,\n",
    "                            tags: dict[str, np.ndarray],\n",
    "                            Y_df: Optional[Frame] = None,\n",
    "                            level: Optional[list[int]] = None,\n",
    "                            intervals_method: str = 'normality',\n",
    "                            num_samples: int = -1,\n",
    "                            num_seeds: int = 1,\n",
    "                            id_col: str = \"unique_id\",\n",
    "                            time_col: str = \"ds\", \n",
    "                            target_col: str = \"y\",                                 \n",
    "                            ) -> FrameT:\n",
    "        \"\"\"Bootstraped Hierarchical Reconciliation Method.\n",
    "\n",
    "        Applies N times, based on different random seeds, the `reconcile` method \n",
    "        for the different reconciliation techniques instantiated in the `reconcilers` list. \n",
    "\n",
    "        **Parameters:**<br>\n",
    "        `Y_hat_df`: DataFrame, base forecasts with columns ['unique_id', 'ds'] and models to reconcile.<br>\n",
    "        `Y_df`: DataFrame, training set of base time series with columns `['unique_id', 'ds', 'y']`.<br>\n",
    "        If a class of `self.reconciles` receives `y_hat_insample`, `Y_df` must include them as columns.<br>\n",
    "        `S`: DataFrame with summing matrix of size `(base, bottom)`, see [aggregate method](https://nixtla.github.io/hierarchicalforecast/utils.html#aggregate).<br>\n",
    "        `tags`: Each key is a level and its value contains tags associated to that level.<br>\n",
    "        `level`: positive float list [0,100), confidence levels for prediction intervals.<br>\n",
    "        `intervals_method`: str, method used to calculate prediction intervals, one of `normality`, `bootstrap`, `permbu`.<br>\n",
    "        `num_samples`: int=-1, if positive return that many probabilistic coherent samples.\n",
    "        `num_seeds`: int=1, random seed for numpy generator's replicability.<br>\n",
    "        `id_col` : str='unique_id', column that identifies each serie.<br>\n",
    "        `time_col` : str='ds', column that identifies each timestep, its values can be timestamps or integers.<br>\n",
    "        `target_col` : str='y', column that contains the target.<br>        \n",
    "\n",
    "        **Returns:**<br>\n",
    "        `Y_bootstrap_df`: DataFrame, with bootstraped reconciled predictions.\n",
    "        \"\"\"\n",
    "        # Bootstrap reconciled predictions\n",
    "        Y_tilde_list = []\n",
    "        for seed in range(num_seeds):\n",
    "            Y_tilde_df = self.reconcile(Y_hat_df=Y_hat_df,\n",
    "                                        S=S_df,\n",
    "                                        tags=tags,\n",
    "                                        Y_df=Y_df,\n",
    "                                        level=level,\n",
    "                                        intervals_method=intervals_method,\n",
    "                                        num_samples=num_samples,\n",
    "                                        seed=seed,\n",
    "                                        id_col=id_col,\n",
    "                                        time_col=time_col,\n",
    "                                        target_col=target_col,\n",
    "                                        )\n",
    "            Y_tilde_nw = nw.from_native(Y_tilde_df)\n",
    "            Y_tilde_nw = Y_tilde_nw.with_columns(nw.lit(seed).alias('seed'))\n",
    "\n",
    "            # TODO: fix broken recmodel_names\n",
    "            if seed==0:\n",
    "                first_columns = Y_tilde_nw.columns\n",
    "            Y_tilde_nw = Y_tilde_nw.rename({col: first_columns[i] for i, col in enumerate(first_columns)})\n",
    "            Y_tilde_list.append(Y_tilde_nw)\n",
    "\n",
    "        Y_bootstrap_nw = nw.concat(Y_tilde_list, how=\"vertical\")\n",
    "        Y_bootstrap_df = Y_bootstrap_nw.to_native()\n",
    "\n",
    "        return Y_bootstrap_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(HierarchicalReconciliation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(HierarchicalReconciliation.reconcile, name='reconcile', title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(HierarchicalReconciliation.bootstrap_reconcile, name='bootstrap_reconcile', title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from hierarchicalforecast.methods import (\n",
    "    BottomUp, TopDown, MiddleOut, MinTrace, ERM,\n",
    ")\n",
    "from hierarchicalforecast.utils import aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/Nixtla/transfer-learning-time-series/main/datasets/tourism.csv')\n",
    "df = df.rename({'Trips': 'y', 'Quarter': 'ds'}, axis=1)\n",
    "df.insert(0, 'Country', 'Australia')\n",
    "df['ds'] = df['ds'].str.replace(r'(\\d+) (Q\\d)', r'\\1-\\2', regex=True)\n",
    "df['ds'] = pd.to_datetime(df['ds'])\n",
    "\n",
    "# non strictly hierarchical structure\n",
    "hierS_grouped_df = [\n",
    "    ['Country'],\n",
    "    ['Country', 'State'], \n",
    "    ['Country', 'Purpose'], \n",
    "    ['Country', 'State', 'Region'], \n",
    "    ['Country', 'State', 'Purpose'], \n",
    "    ['Country', 'State', 'Region', 'Purpose']\n",
    "]\n",
    "# strictly hierarchical structure\n",
    "hiers_strictly = [\n",
    "    ['Country'],\n",
    "    ['Country', 'State'], \n",
    "    ['Country', 'State', 'Region'], \n",
    "]\n",
    "\n",
    "# getting df\n",
    "hier_grouped_df, S_grouped_df, tags_grouped = aggregate(df, hierS_grouped_df)\n",
    "hier_strict_df, S_strict, tags_strict = aggregate(df, hiers_strictly)\n",
    "\n",
    "# check categorical input produces same output\n",
    "df2 = df.copy()\n",
    "for col in ['Country', 'State', 'Purpose', 'Region']:\n",
    "    df2[col] = df2[col].astype('category')\n",
    "\n",
    "for spec in [hierS_grouped_df, hiers_strictly]:\n",
    "    Y_orig, S_orig, tags_orig = aggregate(df, spec)\n",
    "    Y_cat, S_cat, tags_cat = aggregate(df2, spec)\n",
    "    pd.testing.assert_frame_equal(Y_cat, Y_orig)\n",
    "    pd.testing.assert_frame_equal(S_cat, S_orig)\n",
    "    assert all(np.array_equal(tags_orig[k], tags_cat[k]) for k in tags_orig.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# polars\n",
    "import polars as pl\n",
    "import polars.testing as pltest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# polars\n",
    "df_pl = pl.DataFrame(df)\n",
    "\n",
    "# getting df\n",
    "hier_grouped_df_pl, S_grouped_df_pl, tags_grouped_pl = aggregate(df_pl, hierS_grouped_df)\n",
    "hier_strict_df_pl, S_strict_pl, tags_strict_pl = aggregate(df_pl, hiers_strictly)\n",
    "\n",
    "# check categorical input produces same output\n",
    "df2_pl = df_pl.clone()\n",
    "for col in ['Country', 'State', 'Purpose', 'Region']:\n",
    "    df2_pl = df2_pl.with_columns(pl.col(col).cast(pl.Categorical))\n",
    "\n",
    "for spec in [hierS_grouped_df, hiers_strictly]:\n",
    "    Y_orig_pl, S_orig_pl, tags_orig_pl = aggregate(df_pl, spec)\n",
    "    Y_cat_pl, S_cat_pl, tags_cat_pl = aggregate(df2_pl, spec)\n",
    "    pltest.assert_frame_equal(Y_cat_pl, Y_orig_pl)\n",
    "    pltest.assert_frame_equal(S_cat_pl, S_orig_pl)\n",
    "    assert all(np.array_equal(tags_orig_pl[k], tags_cat_pl[k]) for k in tags_orig_pl.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "hier_grouped_df['y_model'] = hier_grouped_df['y']\n",
    "# we should be able to recover y using the methods\n",
    "hier_grouped_hat_df = hier_grouped_df.groupby('unique_id').tail(12)\n",
    "ds_h = hier_grouped_hat_df['ds'].unique()\n",
    "hier_grouped_df_filtered = hier_grouped_df.query('~(ds in @ds_h)').copy()\n",
    "# adding noise to `y_model` to avoid perfect fited values\n",
    "hier_grouped_df_filtered['y_model'] += np.random.uniform(-1, 1, len(hier_grouped_df_filtered))\n",
    "\n",
    "#hierachical reconciliation\n",
    "hrec = HierarchicalReconciliation(reconcilers=[\n",
    "    #these methods should reconstruct the original y\n",
    "    BottomUp(),\n",
    "    MinTrace(method='ols'),\n",
    "    MinTrace(method='wls_struct'),\n",
    "    MinTrace(method='wls_var'),\n",
    "    MinTrace(method='mint_shrink'),\n",
    "    MinTrace(method='ols', nonnegative=True),\n",
    "    MinTrace(method='wls_struct', nonnegative=True),\n",
    "    MinTrace(method='wls_var', nonnegative=True),\n",
    "    MinTrace(method='mint_shrink', nonnegative=True),\n",
    "])\n",
    "reconciled = hrec.reconcile(Y_hat_df=hier_grouped_hat_df, \n",
    "                            Y_df=hier_grouped_df_filtered, \n",
    "                            S=S_grouped_df, tags=tags_grouped)\n",
    "for model in reconciled.drop(columns=[\"unique_id\", \"ds\", \"y\"]).columns:\n",
    "    if 'ERM' in model:\n",
    "        eps = 3\n",
    "    elif 'nonnegative' in model:\n",
    "        eps = 1e-1\n",
    "    else:\n",
    "        eps = 1e-1\n",
    "    test_close(reconciled['y'], reconciled[model], eps=eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# polars\n",
    "hier_grouped_hat_df_pl = pl.from_pandas(hier_grouped_hat_df)\n",
    "hier_grouped_df_filtered_pl = pl.from_pandas(hier_grouped_df_filtered)\n",
    "S_grouped_df_pl = pl.from_pandas(S_grouped_df)\n",
    "\n",
    "reconciled_pl = hrec.reconcile(Y_hat_df=hier_grouped_hat_df_pl, \n",
    "                            Y_df=hier_grouped_df_filtered_pl, \n",
    "                            S=S_grouped_df_pl, \n",
    "                            tags=tags_grouped)\n",
    "\n",
    "for model in reconciled_pl.drop([\"unique_id\", \"ds\", \"y\"]).columns:\n",
    "    if 'ERM' in model:\n",
    "        eps = 3\n",
    "    elif 'nonnegative' in model:\n",
    "        eps = 1e-1\n",
    "    else:\n",
    "        eps = 1e-1\n",
    "    test_close(reconciled_pl['y'], reconciled_pl[model], eps=eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test incorrect Y_hat_df datatypes\n",
    "hier_grouped_hat_df_nan = hier_grouped_hat_df.copy()\n",
    "hier_grouped_hat_df_idx_changed = hier_grouped_hat_df_nan.query(\"unique_id == 'Australia'\").index\n",
    "hier_grouped_hat_df_nan.loc[hier_grouped_hat_df_idx_changed, 'y_model'] = float('nan')\n",
    "test_fail(\n",
    "    hrec.reconcile,\n",
    "    contains='null values',\n",
    "    args=(hier_grouped_hat_df_nan, S_grouped_df, tags_grouped, hier_grouped_df),\n",
    ")\n",
    "\n",
    "hier_grouped_hat_df_none = hier_grouped_hat_df.copy()\n",
    "hier_grouped_hat_df_idx_changed = hier_grouped_hat_df_none.query(\"unique_id == 'Australia'\").index\n",
    "hier_grouped_hat_df_none.loc[hier_grouped_hat_df_idx_changed, 'y_model'] = None\n",
    "test_fail(\n",
    "    hrec.reconcile,\n",
    "    contains='null values',\n",
    "    args=(hier_grouped_hat_df_none, S_grouped_df, tags_grouped, hier_grouped_df),\n",
    ")\n",
    "\n",
    "hier_grouped_hat_df_str = hier_grouped_hat_df.copy()\n",
    "hier_grouped_hat_df_str['y_model'] = hier_grouped_hat_df_str['y_model'].astype(str)\n",
    "test_fail(\n",
    "    hrec.reconcile,\n",
    "    contains='numeric values',\n",
    "    args=(hier_grouped_hat_df_str, S_grouped_df, tags_grouped, hier_grouped_df),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# polars\n",
    "# test incorrect Y_hat_df datatypes\n",
    "hier_grouped_hat_df_nan_pl = pl.from_pandas(hier_grouped_hat_df_nan)\n",
    "test_fail(\n",
    "    hrec.reconcile,\n",
    "    contains='null values',\n",
    "    args=(hier_grouped_hat_df_nan_pl, S_grouped_df_pl, tags_grouped_pl, hier_grouped_df_pl),\n",
    ")\n",
    "\n",
    "hier_grouped_hat_df_none_pl = pl.from_pandas(hier_grouped_hat_df_none)\n",
    "test_fail(\n",
    "    hrec.reconcile,\n",
    "    contains='null values',\n",
    "    args=(hier_grouped_hat_df_none_pl, S_grouped_df_pl, tags_grouped_pl, hier_grouped_df_pl),\n",
    ")\n",
    "\n",
    "hier_grouped_hat_df_str_pl = pl.from_pandas(hier_grouped_hat_df_str)\n",
    "test_fail(\n",
    "    hrec.reconcile,\n",
    "    contains='numeric values',\n",
    "    args=(hier_grouped_hat_df_str_pl, S_grouped_df_pl, tags_grouped_pl, hier_grouped_df_pl),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test expected error\n",
    "# different series S and Y_hat_df\n",
    "drop_idx = hier_grouped_hat_df.query(\"unique_id == 'Australia'\").index\n",
    "test_fail(\n",
    "    hrec.reconcile,\n",
    "    contains='There are unique_ids in S_df that are not in Y_hat_df',\n",
    "    args=(hier_grouped_hat_df.drop(index=drop_idx), S_grouped_df, tags_grouped, hier_grouped_df),\n",
    "    \n",
    ")\n",
    "\n",
    "drop_idx = S_grouped_df.query(\"unique_id == 'Australia'\").index\n",
    "test_fail(\n",
    "    hrec.reconcile,\n",
    "    contains='There are unique_ids in Y_hat_df that are not in S_df',\n",
    "    args=(hier_grouped_hat_df, S_grouped_df.drop(index=drop_idx), tags_grouped, hier_grouped_df),\n",
    ")\n",
    "\n",
    "drop_idx = hier_grouped_df.query(\"unique_id == 'Australia'\").index\n",
    "test_fail(\n",
    "    hrec.reconcile,\n",
    "    contains='There are unique_ids in Y_hat_df that are not in Y_df',\n",
    "    args=(hier_grouped_hat_df, S_grouped_df, tags_grouped, hier_grouped_df.drop(index=drop_idx)),   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# polars\n",
    "# test expected error\n",
    "# different series S and Y_hat_df\n",
    "test_fail(\n",
    "    hrec.reconcile,\n",
    "    contains='There are unique_ids in S_df that are not in Y_hat_df',\n",
    "    args=(hier_grouped_hat_df_pl.filter(pl.col(\"unique_id\") != \"Australia\"), S_grouped_df_pl, tags_grouped_pl, hier_grouped_df_pl),\n",
    ")\n",
    "\n",
    "test_fail(\n",
    "    hrec.reconcile,\n",
    "    contains='There are unique_ids in Y_hat_df that are not in S_df',\n",
    "    args=(hier_grouped_hat_df_pl, S_grouped_df_pl.filter(pl.col(\"unique_id\") != \"Australia\"), tags_grouped_pl, hier_grouped_df_pl),\n",
    ")\n",
    "\n",
    "test_fail(\n",
    "    hrec.reconcile,\n",
    "    contains='There are unique_ids in Y_hat_df that are not in Y_df',\n",
    "    args=(hier_grouped_hat_df_pl, S_grouped_df_pl, tags_grouped_pl, hier_grouped_df_pl.filter(pl.col(\"unique_id\") != \"Australia\")),   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "# test expected error\n",
    "# different columns Y_df and Y_hat_df\n",
    "hrec = HierarchicalReconciliation(\n",
    "            reconcilers=[ERM(method='reg_bu', lambda_reg=100)])\n",
    "test_fail(\n",
    "    hrec.reconcile,\n",
    "    contains='Please include ',\n",
    "    args=(hier_grouped_hat_df, S_grouped_df, tags_grouped, \n",
    "          hier_grouped_df, [80], 'permbu'), # permbu needs y_hat_insample\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "# polars\n",
    "# test expected error\n",
    "# different columns Y_df and Y_hat_df\n",
    "hier_grouped_hat_df_pl = pl.from_pandas(hier_grouped_hat_df)\n",
    "hier_grouped_df_pl = pl.from_pandas(hier_grouped_df)\n",
    "S_grouped_df_pl = pl.from_pandas(S_grouped_df)\n",
    "\n",
    "test_fail(\n",
    "    hrec.reconcile,\n",
    "    contains='Please include ',\n",
    "    args=(hier_grouped_hat_df_pl, S_grouped_df_pl, tags_grouped_pl, \n",
    "          hier_grouped_df_pl, [80], 'permbu'), # permbu needs y_hat_insample\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test reconcile method without insample\n",
    "hrec = HierarchicalReconciliation(reconcilers=[\n",
    "    #these methods should reconstruct the original y\n",
    "    BottomUp(),\n",
    "    MinTrace(method='ols'),\n",
    "    MinTrace(method='wls_struct'),\n",
    "    MinTrace(method='ols', nonnegative=True),\n",
    "    MinTrace(method='wls_struct', nonnegative=True),\n",
    "])\n",
    "reconciled = hrec.reconcile(Y_hat_df=hier_grouped_hat_df,\n",
    "                            S=S_grouped_df, tags=tags_grouped)\n",
    "for model in reconciled.drop(columns=['ds', 'y', 'unique_id']).columns:\n",
    "    if 'ERM' in model:\n",
    "        eps = 3\n",
    "    elif 'nonnegative' in model:\n",
    "        eps = 1e-1\n",
    "    else:\n",
    "        eps = 1e-1\n",
    "    test_close(reconciled['y'], reconciled[model], eps=eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# polars\n",
    "# test reconcile method without insample\n",
    "hrec = HierarchicalReconciliation(reconcilers=[\n",
    "    #these methods should reconstruct the original y\n",
    "    BottomUp(),\n",
    "    MinTrace(method='ols'),\n",
    "    MinTrace(method='wls_struct'),\n",
    "    MinTrace(method='ols', nonnegative=True),\n",
    "    MinTrace(method='wls_struct', nonnegative=True),\n",
    "])\n",
    "reconciled = hrec.reconcile(Y_hat_df=hier_grouped_hat_df_pl,\n",
    "                            S=S_grouped_df_pl, \n",
    "                            tags=tags_grouped_pl)\n",
    "for model in reconciled.drop(['ds', 'y', 'unique_id']).columns:\n",
    "    if 'ERM' in model:\n",
    "        eps = 3\n",
    "    elif 'nonnegative' in model:\n",
    "        eps = 1e-1\n",
    "    else:\n",
    "        eps = 1e-1\n",
    "    test_close(reconciled['y'], reconciled[model], eps=eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# top down should break\n",
    "# with non strictly hierarchical structures\n",
    "hrec = HierarchicalReconciliation([TopDown(method='average_proportions')])\n",
    "test_fail(\n",
    "    hrec.reconcile,\n",
    "    contains='requires strictly hierarchical structures',\n",
    "    args=(hier_grouped_hat_df, S_grouped_df, tags_grouped,  hier_grouped_df,)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# polars\n",
    "# top down should break\n",
    "# with non strictly hierarchical structures\n",
    "hrec = HierarchicalReconciliation([TopDown(method='average_proportions')])\n",
    "test_fail(\n",
    "    hrec.reconcile,\n",
    "    contains='requires strictly hierarchical structures',\n",
    "    args=(hier_grouped_hat_df_pl, S_grouped_df_pl, tags_grouped_pl,  hier_grouped_df_pl,)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# methods should work with strictly hierarchical structures\n",
    "hier_strict_df['y_model'] = hier_strict_df['y']\n",
    "# we should be able to recover y using the methods\n",
    "hier_strict_df_h = hier_strict_df.groupby('unique_id').tail(12)\n",
    "ds_h = hier_strict_df_h['ds'].unique()\n",
    "hier_strict_df = hier_strict_df.query('~(ds in @ds_h)')\n",
    "#adding noise to `y_model` to avoid perfect fited values\n",
    "hier_strict_df['y_model'] += np.random.uniform(-1, 1, len(hier_strict_df))\n",
    "\n",
    "middle_out_level = 'Country/State'\n",
    "# hierarchical reconciliation\n",
    "hrec = HierarchicalReconciliation(reconcilers=[\n",
    "    #these methods should reconstruct the original y\n",
    "    BottomUp(),\n",
    "    MinTrace(method='ols'),\n",
    "    MinTrace(method='wls_struct'),\n",
    "    MinTrace(method='wls_var'),\n",
    "    MinTrace(method='mint_shrink'),\n",
    "    MinTrace(method='ols', nonnegative=True),\n",
    "    MinTrace(method='wls_struct', nonnegative=True),\n",
    "    MinTrace(method='wls_var', nonnegative=True),\n",
    "    MinTrace(method='mint_shrink', nonnegative=True),\n",
    "    # top down doesnt recover the original y\n",
    "    # but it should recover the total level\n",
    "    TopDown(method='forecast_proportions'),\n",
    "    TopDown(method='average_proportions'),\n",
    "    TopDown(method='proportion_averages'),\n",
    "    # middle out doesnt recover the original y\n",
    "    # but it should recover the total level\n",
    "    MiddleOut(middle_level=middle_out_level, top_down_method='forecast_proportions'),\n",
    "    MiddleOut(middle_level=middle_out_level, top_down_method='average_proportions'),\n",
    "    MiddleOut(middle_level=middle_out_level, top_down_method='proportion_averages'),\n",
    "    # ERM recovers but needs bigger eps\n",
    "    #ERM(method='reg_bu', lambda_reg=None),\n",
    "])\n",
    "reconciled = hrec.reconcile(\n",
    "    Y_hat_df=hier_strict_df_h, \n",
    "    Y_df=hier_strict_df, \n",
    "    S=S_strict, \n",
    "    tags=tags_strict\n",
    ")\n",
    "for model in reconciled.drop(columns=['ds', 'y', 'unique_id']).columns:\n",
    "    if 'ERM' in model:\n",
    "        eps = 3\n",
    "    elif 'nonnegative' in model:\n",
    "        eps = 1e-1\n",
    "    else:\n",
    "        eps = 1e-1\n",
    "    if 'TopDown' in model:\n",
    "        if 'forecast_proportions' in model:\n",
    "            test_close(reconciled['y'], reconciled[model], eps)\n",
    "        else:\n",
    "            # top down doesnt recover the original y\n",
    "            test_fail(\n",
    "                test_close,\n",
    "                args=(reconciled['y'], reconciled[model], eps),\n",
    "            )\n",
    "        # but it should recover the total level\n",
    "        total_tag = tags_strict['Country']\n",
    "        test_close(reconciled[[\"unique_id\", \"y\"]].query(\"unique_id == @total_tag[0]\")[\"y\"], \n",
    "                   reconciled[[\"unique_id\", model]].query(\"unique_id == @total_tag[0]\")[model], 1e-2)\n",
    "    elif 'MiddleOut' in model:\n",
    "        if 'forecast_proportions' in model:\n",
    "            test_close(reconciled['y'], reconciled[model], eps)\n",
    "        else:\n",
    "            # top down doesnt recover the original y\n",
    "            test_fail(\n",
    "                test_close,\n",
    "                args=(reconciled['y'], reconciled[model], eps),\n",
    "            )\n",
    "        # but it should recover the total level\n",
    "        total_tag = tags_strict[middle_out_level]\n",
    "        test_close(reconciled[[\"unique_id\", \"y\"]].query(\"unique_id == @total_tag[0]\")[\"y\"], \n",
    "                   reconciled[[\"unique_id\", model]].query(\"unique_id == @total_tag[0]\")[model], 1e-2)\n",
    "    else:\n",
    "        test_close(reconciled['y'], reconciled[model], eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# polars\n",
    "# methods should work with strictly hierarchical structures\n",
    "hier_strict_df_pl = hier_strict_df_pl.with_columns(hier_strict_df_pl['y'].alias('y_model'))\n",
    "# we should be able to recover y using the methods\n",
    "hier_strict_df_h_pl = hier_strict_df_pl.group_by('unique_id').tail(12)\n",
    "ds_h = set(hier_strict_df_h_pl['ds'])\n",
    "hier_strict_df_pl = hier_strict_df_pl.filter(~pl.col(\"ds\").is_in(ds_h))\n",
    "#adding noise to `y_model` to avoid perfect fited values\n",
    "hier_strict_df_pl = hier_strict_df_pl.with_columns(pl.col('y_model') + np.random.uniform(-1, 1, len(hier_strict_df_pl)))\n",
    "\n",
    "middle_out_level = 'Country/State'\n",
    "# hierarchical reconciliation\n",
    "hrec = HierarchicalReconciliation(reconcilers=[\n",
    "    #these methods should reconstruct the original y\n",
    "    BottomUp(),\n",
    "    MinTrace(method='ols'),\n",
    "    MinTrace(method='wls_struct'),\n",
    "    MinTrace(method='wls_var'),\n",
    "    MinTrace(method='mint_shrink'),\n",
    "    MinTrace(method='ols', nonnegative=True),\n",
    "    MinTrace(method='wls_struct', nonnegative=True),\n",
    "    MinTrace(method='wls_var', nonnegative=True),\n",
    "    MinTrace(method='mint_shrink', nonnegative=True),\n",
    "    # top down doesnt recover the original y\n",
    "    # but it should recover the total level\n",
    "    TopDown(method='forecast_proportions'),\n",
    "    TopDown(method='average_proportions'),\n",
    "    TopDown(method='proportion_averages'),\n",
    "    # middle out doesnt recover the original y\n",
    "    # but it should recover the total level\n",
    "    MiddleOut(middle_level=middle_out_level, top_down_method='forecast_proportions'),\n",
    "    MiddleOut(middle_level=middle_out_level, top_down_method='average_proportions'),\n",
    "    MiddleOut(middle_level=middle_out_level, top_down_method='proportion_averages'),\n",
    "    # ERM recovers but needs bigger eps\n",
    "    #ERM(method='reg_bu', lambda_reg=None),\n",
    "])\n",
    "reconciled_pl = hrec.reconcile(\n",
    "    Y_hat_df=hier_strict_df_h_pl, \n",
    "    Y_df=hier_strict_df_pl, \n",
    "    S=S_strict_pl, \n",
    "    tags=tags_strict_pl\n",
    ")\n",
    "for model in reconciled_pl.drop(['ds', 'y', 'unique_id']).columns:\n",
    "    if 'ERM' in model:\n",
    "        eps = 3\n",
    "    elif 'nonnegative' in model:\n",
    "        eps = 1e-1\n",
    "    else:\n",
    "        eps = 1e-1\n",
    "    if 'TopDown' in model:\n",
    "        if 'forecast_proportions' in model:\n",
    "            test_close(reconciled_pl['y'], reconciled_pl[model], eps)\n",
    "        else:\n",
    "            # top down doesnt recover the original y\n",
    "            test_fail(\n",
    "                test_close,\n",
    "                args=(reconciled_pl['y'], reconciled_pl[model], eps),\n",
    "            )\n",
    "        # but it should recover the total level\n",
    "        total_tag = tags_strict['Country']\n",
    "        test_close(reconciled_pl[[\"unique_id\", \"y\"]].filter(pl.col(\"unique_id\") == total_tag[0])[\"y\"], \n",
    "                   reconciled_pl[[\"unique_id\", model]].filter(pl.col(\"unique_id\") == total_tag[0])[model], 1e-2)\n",
    "    elif 'MiddleOut' in model:\n",
    "        if 'forecast_proportions' in model:\n",
    "            test_close(reconciled_pl['y'], reconciled_pl[model], eps)\n",
    "        else:\n",
    "            # top down doesnt recover the original y\n",
    "            test_fail(\n",
    "                test_close,\n",
    "                args=(reconciled_pl['y'], reconciled_pl[model], eps),\n",
    "            )\n",
    "        # but it should recover the total level\n",
    "        total_tag = tags_strict[middle_out_level]\n",
    "        test_close(reconciled_pl[[\"unique_id\", \"y\"]].filter(pl.col(\"unique_id\") == total_tag[0])[\"y\"], \n",
    "                   reconciled_pl[[\"unique_id\", model]].filter(pl.col(\"unique_id\") == total_tag[0])[model], 1e-2)\n",
    "    else:\n",
    "        test_close(reconciled_pl['y'], reconciled_pl[model], eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test is_balanced behaviour\n",
    "reconciled_balanced = hrec.reconcile(\n",
    "    Y_hat_df=hier_strict_df_h, \n",
    "    Y_df=hier_strict_df, \n",
    "    S=S_strict, \n",
    "    tags=tags_strict,\n",
    "    is_balanced=True,\n",
    ")\n",
    "test_close(reconciled.drop(columns=[\"unique_id\", \"ds\"]).values, reconciled_balanced.drop(columns=[\"unique_id\", \"ds\"]).values, eps=1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# polars\n",
    "# test is_balanced behaviour\n",
    "reconciled_balanced = hrec.reconcile(\n",
    "    Y_hat_df=hier_strict_df_h_pl, \n",
    "    Y_df=hier_strict_df_pl, \n",
    "    S=S_strict_pl, \n",
    "    tags=tags_strict_pl,\n",
    "    is_balanced=True,\n",
    ")\n",
    "test_close(reconciled_pl.drop([\"unique_id\", \"ds\"]).to_numpy(), reconciled_balanced.drop([\"unique_id\", \"ds\"]).to_numpy(), eps=1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from statsforecast import StatsForecast\n",
    "from statsforecast.utils import generate_series\n",
    "from statsforecast.models import RandomWalkWithDrift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test unbalanced dataset\n",
    "max_tenure = 24\n",
    "dates = pd.date_range(start='2019-01-31', freq='ME', periods=max_tenure)\n",
    "cohort_tenure = [24, 23, 22, 21]\n",
    "\n",
    "ts_list = []\n",
    "\n",
    "# Create ts for each cohort\n",
    "for i in range(len(cohort_tenure)):\n",
    "    ts_list.append(\n",
    "        generate_series(n_series=1, freq='ME', min_length=cohort_tenure[i], max_length=cohort_tenure[i]).reset_index() \\\n",
    "            .assign(ult=i) \\\n",
    "            .assign(ds=dates[-cohort_tenure[i]:]) \\\n",
    "            .drop(columns=['unique_id'])\n",
    "    )\n",
    "df = pd.concat(ts_list, ignore_index=True)\n",
    "\n",
    "# Create categories\n",
    "df.loc[df['ult'] < 2, 'pen'] = 'a'\n",
    "df.loc[df['ult'] >= 2, 'pen'] = 'b'\n",
    "# Note that unique id requires strings\n",
    "df['ult'] = df['ult'].astype(str)\n",
    "\n",
    "hier_levels = [\n",
    "    ['pen'],\n",
    "    ['pen', 'ult'],\n",
    "]\n",
    "hier_df, S_df, tags = aggregate(df=df, spec=hier_levels)\n",
    "\n",
    "train_df = hier_df.query(\"ds <= @pd.to_datetime('2019-12-31')\")\n",
    "test_df = hier_df.query(\"ds > @pd.to_datetime('2019-12-31')\")\n",
    "\n",
    "fcst = StatsForecast(\n",
    "    models=[\n",
    "        RandomWalkWithDrift(),\n",
    "    ],\n",
    "    freq='ME',\n",
    "    n_jobs=1,\n",
    ")\n",
    "\n",
    "hrec = HierarchicalReconciliation(\n",
    "    reconcilers=[\n",
    "        BottomUp(),\n",
    "        MinTrace(method='mint_shrink'),\n",
    "    ]\n",
    ")\n",
    "\n",
    "fcst_df = fcst.forecast(df=train_df, h=12, fitted=True)\n",
    "fitted_df = fcst.forecast_fitted_values()\n",
    "\n",
    "fcst_df = hrec.reconcile(\n",
    "    Y_hat_df=fcst_df,\n",
    "    Y_df=fitted_df,\n",
    "    S=S_df,\n",
    "    tags=tags,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# polars\n",
    "# test unbalanced dataset\n",
    "df_pl = pl.from_pandas(df)\n",
    "hier_df_pl, S_df_pl, tags_pl = aggregate(df=df_pl, spec=hier_levels)\n",
    "\n",
    "train_df = hier_df_pl.filter(pl.col(\"ds\") <= pl.lit('2019-12-31').str.to_date())\n",
    "test_df = hier_df_pl.filter(pl.col(\"ds\") > pl.lit('2019-12-31').str.to_date())\n",
    "\n",
    "fcst = StatsForecast(\n",
    "    models=[\n",
    "        RandomWalkWithDrift(),\n",
    "    ],\n",
    "    freq='1mo',\n",
    "    n_jobs=1,\n",
    ")\n",
    "\n",
    "hrec = HierarchicalReconciliation(\n",
    "    reconcilers=[\n",
    "        BottomUp(),\n",
    "        MinTrace(method='mint_shrink'),\n",
    "    ]\n",
    ")\n",
    "\n",
    "fcst_df_pl = fcst.forecast(df=train_df, h=12, fitted=True)\n",
    "fitted_df = fcst.forecast_fitted_values()\n",
    "\n",
    "fcst_df_pl = hrec.reconcile(\n",
    "    Y_hat_df=fcst_df_pl,\n",
    "    Y_df=fitted_df,\n",
    "    S=S_df_pl,\n",
    "    tags=tags,\n",
    ")\n",
    "\n",
    "# Test equivalence\n",
    "pd.testing.assert_frame_equal(fcst_df, fcst_df_pl.to_pandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# MinTrace should break\n",
    "# with extremely overfitted model, y_model==y\n",
    "zero_df = hier_grouped_df.copy()\n",
    "zero_df['y'] = 0\n",
    "zero_df['y_model'] = 0\n",
    "hrec = HierarchicalReconciliation([MinTrace(method='mint_shrink')])\n",
    "test_fail(\n",
    "    hrec.reconcile,\n",
    "    contains='Insample residuals',\n",
    "    args=(hier_grouped_hat_df, S_grouped_df, tags_grouped,  zero_df)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# polars\n",
    "# MinTrace should break\n",
    "# with extremely overfitted model, y_model==y\n",
    "zero_df_pl = pl.from_pandas(zero_df)    \n",
    "hrec = HierarchicalReconciliation([MinTrace(method='mint_shrink')])\n",
    "test_fail(\n",
    "    hrec.reconcile,\n",
    "    contains='Insample residuals',\n",
    "    args=(hier_grouped_hat_df_pl, S_grouped_df_pl, tags_grouped_pl,  zero_df_pl)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#test methods that dont use residuals\n",
    "#even if their signature includes\n",
    "#that argument\n",
    "hrec = HierarchicalReconciliation([MinTrace(method='ols')])\n",
    "reconciled = hrec.reconcile(\n",
    "    Y_hat_df=hier_grouped_hat_df, \n",
    "    Y_df=hier_grouped_df.drop(columns=['y_model']), \n",
    "    S=S_grouped_df, \n",
    "    tags=tags_grouped\n",
    ")\n",
    "for model in reconciled.drop(columns=['ds', 'y', 'unique_id']).columns:\n",
    "    test_close(reconciled['y'], reconciled[model], eps=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# polars\n",
    "#test methods that dont use residuals\n",
    "#even if their signature includes\n",
    "#that argument\n",
    "hrec = HierarchicalReconciliation([MinTrace(method='ols')])\n",
    "reconciled = hrec.reconcile(\n",
    "    Y_hat_df=hier_grouped_hat_df_pl, \n",
    "    Y_df=hier_grouped_df_pl.drop(['y_model']), \n",
    "    S=S_grouped_df_pl, \n",
    "    tags=tags_grouped_pl\n",
    ")\n",
    "for model in reconciled.drop(['ds', 'y', 'unique_id']).columns:\n",
    "    test_close(reconciled['y'], reconciled[model], eps=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test methods with bootstrap prediction intervals\n",
    "hrec = HierarchicalReconciliation([BottomUp()])\n",
    "reconciled = hrec.reconcile(Y_hat_df=hier_grouped_hat_df, \n",
    "                            Y_df=hier_grouped_df, S=S_grouped_df, tags=tags_grouped,\n",
    "                            level=[80, 90], \n",
    "                            intervals_method='bootstrap')\n",
    "total = reconciled.query(\"unique_id in @tags_grouped['Country/State/Region/Purpose']\").groupby('ds').sum().reset_index()\n",
    "pd.testing.assert_frame_equal(\n",
    "    total[['ds', 'y_model/BottomUp']],\n",
    "    reconciled.query(\"unique_id == 'Australia'\")[['ds', 'y_model/BottomUp']].reset_index(drop=True)\n",
    ")\n",
    "assert 'y_model/BottomUp-lo-80' in reconciled.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# polars\n",
    "# test methods with bootstrap prediction intervals\n",
    "hrec = HierarchicalReconciliation([BottomUp()])\n",
    "reconciled = hrec.reconcile(Y_hat_df=hier_grouped_hat_df_pl, \n",
    "                            Y_df=hier_grouped_df_pl, \n",
    "                            S=S_grouped_df_pl, \n",
    "                            tags=tags_grouped_pl,\n",
    "                            level=[80, 90], \n",
    "                            intervals_method='bootstrap')\n",
    "total = reconciled.filter(pl.col(\"unique_id\").is_in(tags_grouped['Country/State/Region/Purpose'])).group_by('ds', maintain_order=True).sum()\n",
    "pltest.assert_frame_equal(\n",
    "    total[['ds', 'y_model/BottomUp']],\n",
    "    reconciled.filter(pl.col(\"unique_id\") == 'Australia')[['ds', 'y_model/BottomUp']]\n",
    ")\n",
    "assert 'y_model/BottomUp-lo-80' in reconciled.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test methods with  normality prediction intervals\n",
    "hier_grouped_hat_df['y_model-lo-80'] = hier_grouped_hat_df['y_model'] - 1.96\n",
    "hier_grouped_hat_df['y_model-hi-80'] = hier_grouped_hat_df['y_model'] + 1.96\n",
    "hrec = HierarchicalReconciliation([BottomUp()])\n",
    "reconciled = hrec.reconcile(Y_hat_df=hier_grouped_hat_df,\n",
    "                            Y_df=hier_grouped_df, S=S_grouped_df, tags=tags_grouped,\n",
    "                            level=[80, 90], \n",
    "                            intervals_method='normality')\n",
    "total = reconciled.query(\"unique_id in @tags_grouped['Country/State/Region/Purpose']\").groupby('ds').sum().reset_index()\n",
    "pd.testing.assert_frame_equal(\n",
    "    total[['ds', 'y_model/BottomUp']],\n",
    "    reconciled.query(\"unique_id == 'Australia'\")[['ds', 'y_model/BottomUp']].reset_index(drop=True)\n",
    ")\n",
    "assert 'y_model/BottomUp-lo-80' in reconciled.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# polars\n",
    "# test methods with  normality prediction intervals\n",
    "hier_grouped_hat_df_pl = pl.from_pandas(hier_grouped_hat_df)\n",
    "hrec = HierarchicalReconciliation([BottomUp()])\n",
    "reconciled = hrec.reconcile(Y_hat_df=hier_grouped_hat_df_pl,\n",
    "                            Y_df=hier_grouped_df_pl, \n",
    "                            S=S_grouped_df_pl, \n",
    "                            tags=tags_grouped_pl,\n",
    "                            level=[80, 90], \n",
    "                            intervals_method='normality')\n",
    "total = reconciled.filter(pl.col(\"unique_id\").is_in(tags_grouped['Country/State/Region/Purpose'])).group_by('ds', maintain_order=True).sum()\n",
    "pltest.assert_frame_equal(\n",
    "    total[['ds', 'y_model/BottomUp']],\n",
    "    reconciled.filter(pl.col(\"unique_id\") == 'Australia')[['ds', 'y_model/BottomUp']]\n",
    ")\n",
    "assert 'y_model/BottomUp-lo-80' in reconciled.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test methods with PERMBU prediction intervals\n",
    "\n",
    "# test expect error with grouped structure\n",
    "# (non strictly hierarchical)\n",
    "hier_grouped_hat_df['y_model-lo-80'] = hier_grouped_hat_df['y_model'] - 1.96\n",
    "hier_grouped_hat_df['y_model-hi-80'] = hier_grouped_hat_df['y_model'] + 1.96\n",
    "hrec = HierarchicalReconciliation([BottomUp()])\n",
    "test_fail(\n",
    "    hrec.reconcile,\n",
    "    contains='requires strictly hierarchical structures',\n",
    "    args=(hier_grouped_hat_df, S_grouped_df, tags_grouped, hier_grouped_df, [80, 90], 'permbu',)\n",
    ")\n",
    "\n",
    "# test PERMBU\n",
    "hier_strict_df_h['y_model-lo-80'] = hier_strict_df_h['y_model'] - 1.96\n",
    "hier_strict_df_h['y_model-hi-80'] = hier_strict_df_h['y_model'] + 1.96\n",
    "hrec = HierarchicalReconciliation([BottomUp()])\n",
    "reconciled = hrec.reconcile(Y_hat_df=hier_strict_df_h,\n",
    "                            Y_df=hier_strict_df, \n",
    "                            S=S_strict, \n",
    "                            tags=tags_strict,\n",
    "                            level=[80, 90], \n",
    "                            intervals_method='permbu')\n",
    "total = reconciled.query(\"unique_id in @tags_grouped['Country/State/Region']\").groupby('ds').sum().reset_index()\n",
    "pd.testing.assert_frame_equal(\n",
    "    total[['ds', 'y_model/BottomUp']],\n",
    "    reconciled.query(\"unique_id == 'Australia'\")[['ds', 'y_model/BottomUp']].reset_index(drop=True)\n",
    ")\n",
    "assert 'y_model/BottomUp-lo-80' in reconciled.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# polars\n",
    "# test methods with PERMBU prediction intervals\n",
    "\n",
    "# test expect error with grouped structure\n",
    "# (non strictly hierarchical)\n",
    "hier_grouped_hat_df_pl = pl.from_pandas(hier_grouped_hat_df)\n",
    "\n",
    "hrec = HierarchicalReconciliation([BottomUp()])\n",
    "test_fail(\n",
    "    hrec.reconcile,\n",
    "    contains='requires strictly hierarchical structures',\n",
    "    args=(hier_grouped_hat_df_pl, S_grouped_df_pl, tags_grouped_pl, hier_grouped_df_pl, [80, 90], 'permbu',)\n",
    ")\n",
    "\n",
    "# test PERMBU\n",
    "hier_strict_df_h_pl = pl.from_pandas(hier_strict_df_h)\n",
    "hrec = HierarchicalReconciliation([BottomUp()])\n",
    "reconciled = hrec.reconcile(Y_hat_df=hier_strict_df_h_pl,\n",
    "                            Y_df=hier_strict_df_pl, \n",
    "                            S=S_strict_pl, \n",
    "                            tags=tags_strict_pl,\n",
    "                            level=[80, 90], \n",
    "                            intervals_method='permbu')\n",
    "total = reconciled.filter(pl.col(\"unique_id\").is_in(tags_grouped['Country/State/Region'])).group_by('ds', maintain_order=True).sum()\n",
    "pltest.assert_frame_equal(\n",
    "    total[['ds', 'y_model/BottomUp']],\n",
    "    reconciled.filter(pl.col(\"unique_id\") == 'Australia')[['ds', 'y_model/BottomUp']]\n",
    ")\n",
    "assert 'y_model/BottomUp-lo-80' in reconciled.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test methods with Bootraped Bootstap prediction intervals\n",
    "hrec = HierarchicalReconciliation([BottomUp()])\n",
    "bootstrap_df = hrec.bootstrap_reconcile(Y_hat_df=hier_grouped_hat_df,\n",
    "                                        Y_df=hier_grouped_df, S_df=S_grouped_df, tags=tags_grouped,\n",
    "                                        level=[80, 90],\n",
    "                                        intervals_method='bootstrap',\n",
    "                                        num_seeds=2)\n",
    "assert 'y_model/BottomUp-lo-80' in bootstrap_df.columns\n",
    "assert 'seed' in bootstrap_df.columns\n",
    "assert len(set(bootstrap_df[\"seed\"]))==2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# polars\n",
    "# test methods with Bootraped Bootstap prediction intervals\n",
    "hrec = HierarchicalReconciliation([BottomUp()])\n",
    "bootstrap_df = hrec.bootstrap_reconcile(Y_hat_df=hier_grouped_hat_df_pl,\n",
    "                                        Y_df=hier_grouped_df_pl, \n",
    "                                        S_df=S_grouped_df_pl, \n",
    "                                        tags=tags_grouped_pl,\n",
    "                                        level=[80, 90],\n",
    "                                        intervals_method='bootstrap',\n",
    "                                        num_seeds=2)\n",
    "assert 'y_model/BottomUp-lo-80' in bootstrap_df.columns\n",
    "assert 'seed' in bootstrap_df.columns\n",
    "assert len(set(bootstrap_df[\"seed\"]))==2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test level protection for PERMBU and Normality probabilistic methods\n",
    "hrec = HierarchicalReconciliation([BottomUp()])\n",
    "test_fail(\n",
    "    hrec.reconcile,\n",
    "    contains='Level must be a list containing floating values in the interval [0, 100',\n",
    "    args=(hier_grouped_hat_df, S_grouped_df, tags_grouped, hier_grouped_df, [-1, 80, 90], 'permbu',)\n",
    ")\n",
    "test_fail(\n",
    "    hrec.reconcile,\n",
    "    contains='Level must be a list containing floating values in the interval [0, 100',\n",
    "    args=(hier_grouped_hat_df, S_grouped_df, tags_grouped, hier_grouped_df, [80, 90, 101], 'normality',)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# polars\n",
    "# test level protection for PERMBU and Normality probabilistic methods\n",
    "hrec = HierarchicalReconciliation([BottomUp()])\n",
    "test_fail(\n",
    "    hrec.reconcile,\n",
    "    contains='Level must be a list containing floating values in the interval [0, 100',\n",
    "    args=(hier_grouped_hat_df_pl, S_grouped_df_pl, tags_grouped_pl, hier_grouped_df_pl, [-1, 80, 90], 'permbu',)\n",
    ")\n",
    "test_fail(\n",
    "    hrec.reconcile,\n",
    "    contains='Level must be a list containing floating values in the interval [0, 100',\n",
    "    args=(hier_grouped_hat_df_pl, S_grouped_df_pl, tags_grouped_pl, hier_grouped_df_pl, [80, 90, 101], 'normality',)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "import pandas as pd\n",
    "\n",
    "from hierarchicalforecast.core import HierarchicalReconciliation\n",
    "from hierarchicalforecast.methods import BottomUp, MinTrace\n",
    "from hierarchicalforecast.utils import aggregate\n",
    "from hierarchicalforecast.evaluation import evaluate\n",
    "from statsforecast.core import StatsForecast\n",
    "from statsforecast.models import AutoETS\n",
    "from utilsforecast.losses import mase, rmse\n",
    "from functools import partial\n",
    "\n",
    "# Load TourismSmall dataset\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/Nixtla/transfer-learning-time-series/main/datasets/tourism.csv')\n",
    "df = df.rename({'Trips': 'y', 'Quarter': 'ds'}, axis=1)\n",
    "df.insert(0, 'Country', 'Australia')\n",
    "qs = df['ds'].str.replace(r'(\\d+) (Q\\d)', r'\\1-\\2', regex=True)\n",
    "df['ds'] = pd.PeriodIndex(qs, freq='Q').to_timestamp()\n",
    "\n",
    "# Create hierarchical seires based on geographic levels and purpose\n",
    "# And Convert quarterly ds string to pd.datetime format\n",
    "hierarchy_levels = [['Country'],\n",
    "                    ['Country', 'State'], \n",
    "                    ['Country', 'Purpose'], \n",
    "                    ['Country', 'State', 'Region'], \n",
    "                    ['Country', 'State', 'Purpose'], \n",
    "                    ['Country', 'State', 'Region', 'Purpose']]\n",
    "\n",
    "Y_df, S_df, tags = aggregate(df=df, spec=hierarchy_levels)\n",
    "\n",
    "# Split train/test sets\n",
    "Y_test_df  = Y_df.groupby('unique_id').tail(8)\n",
    "Y_train_df = Y_df.drop(Y_test_df.index)\n",
    "\n",
    "# Compute base auto-ETS predictions\n",
    "# Careful identifying correct data freq, this data quarterly 'Q'\n",
    "fcst = StatsForecast(models=[AutoETS(season_length=4, model='ZZA')], freq='QS', n_jobs=-1)\n",
    "Y_hat_df = fcst.forecast(df=Y_train_df, h=8, fitted=True)\n",
    "Y_fitted_df = fcst.forecast_fitted_values()\n",
    "\n",
    "reconcilers = [\n",
    "                BottomUp(),\n",
    "                MinTrace(method='ols'),\n",
    "                MinTrace(method='mint_shrink'),\n",
    "               ]\n",
    "hrec = HierarchicalReconciliation(reconcilers=reconcilers)\n",
    "Y_rec_df = hrec.reconcile(Y_hat_df=Y_hat_df, \n",
    "                          Y_df=Y_fitted_df,\n",
    "                          S=S_df, tags=tags)\n",
    "\n",
    "# Evaluate\n",
    "eval_tags = {}\n",
    "eval_tags['Total'] = tags['Country']\n",
    "eval_tags['Purpose'] = tags['Country/Purpose']\n",
    "eval_tags['State'] = tags['Country/State']\n",
    "eval_tags['Regions'] = tags['Country/State/Region']\n",
    "eval_tags['Bottom'] = tags['Country/State/Region/Purpose']\n",
    "\n",
    "Y_rec_df_with_y = Y_rec_df.merge(Y_test_df, on=['unique_id', 'ds'], how='left')\n",
    "mase_p = partial(mase, seasonality=4)\n",
    "\n",
    "evaluation = evaluate(Y_rec_df_with_y, \n",
    "         metrics=[mase_p, rmse], \n",
    "         tags=eval_tags, \n",
    "         train_df=Y_train_df)\n",
    "\n",
    "numeric_cols = evaluation.select_dtypes(include=\"number\").columns\n",
    "evaluation[numeric_cols] = evaluation[numeric_cols].map('{:.2f}'.format)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
