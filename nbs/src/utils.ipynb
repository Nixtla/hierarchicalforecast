{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3226c32-707e-45a6-ab7f-9d8f33924670",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5955e6c8-f4cd-49a6-b6c8-b91c4392a6d3",
   "metadata": {},
   "source": [
    "# Aggregation/Visualization Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc33c1e",
   "metadata": {},
   "source": [
    "The `HierarchicalForecast` package contains utility functions to wrangle and visualize \n",
    "hierarchical series datasets. The `aggregate` function of the module allows you to create\n",
    "a hierarchy from categorical variables representing the structure levels, returning also\n",
    "the aggregation contraints matrix $\\mathbf{S}$.\n",
    "\n",
    "In addition, `HierarchicalForecast` ensures compatibility of its reconciliation methods with other popular machine-learning libraries via its external forecast adapters that transform output base forecasts from external libraries into a compatible data frame format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74967eef-4a18-433e-9dc8-d5e8e6d7dd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import itertools\n",
    "import reprlib\n",
    "import sys\n",
    "import timeit\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import narwhals as nw\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from narwhals.typing import Frame, FrameT\n",
    "from numba import njit, prange\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from typing import Optional, Union, Sequence\n",
    "import utilsforecast.feature_engineering as ufe\n",
    "import utilsforecast.processing as ufp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a501424-ebfd-403c-982e-280cb859bd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import show_doc\n",
    "from fastcore.test import test_eq, test_close, test_fail\n",
    "from statsforecast.utils import generate_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62779465",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "# Global variables\n",
    "NUMBA_NOGIL = True\n",
    "NUMBA_CACHE = True\n",
    "NUMBA_PARALLEL = True\n",
    "NUMBA_FASTMATH = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ba2391",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "class CodeTimer:\n",
    "    def __init__(self, name=None, verbose=True):\n",
    "        self.name = \" '\"  + name + \"'\" if name else ''\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.start = timeit.default_timer()\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        self.took = (timeit.default_timer() - self.start)\n",
    "        if self.verbose:\n",
    "            print('Code block' + self.name + \\\n",
    "                  ' took:\\t{0:.5f}'.format(self.took) + ' seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e585c763-cba5-48f5-a021-822622f3b4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def is_strictly_hierarchical(S: np.ndarray, \n",
    "                             tags: dict[str, np.ndarray]) -> bool:\n",
    "    # main idea:\n",
    "    # if S represents a strictly hierarchical structure\n",
    "    # the number of paths before the bottom level\n",
    "    # should be equal to the number of nodes\n",
    "    # of the previuos level\n",
    "    levels_ = dict(sorted(tags.items(), key=lambda x: len(x[1])))\n",
    "    # removing bottom level\n",
    "    levels_.popitem()\n",
    "    # making S categorical\n",
    "    hiers = [np.argmax(S[idx], axis=0) + 1 for _, idx in levels_.items()]\n",
    "    hiers = np.vstack(hiers)\n",
    "    paths = np.unique(hiers, axis=1).shape[1] \n",
    "    nodes = levels_.popitem()[1].size\n",
    "    return paths == nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1f4267",
   "metadata": {},
   "source": [
    "# Aggregate Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a233df7-50d1-4a70-9f07-afb8bff2aa6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _to_upper_hierarchy(bottom_split: list[str], bottom_values: str, upper_key: str) -> list[str]:\n",
    "    upper_split = upper_key.split('/')\n",
    "    upper_idxs = [bottom_split.index(i) for i in upper_split]\n",
    "\n",
    "    def join_upper(bottom_value):\n",
    "        bottom_parts = bottom_value.split('/')\n",
    "        return '/'.join(bottom_parts[i] for i in upper_idxs)\n",
    "\n",
    "    return [join_upper(val) for val in bottom_values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73d1052",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def aggregate(\n",
    "    df: Frame,\n",
    "    spec: list[list[str]],\n",
    "    exog_vars: Optional[dict[str, Union[str, list[str]]]] = None,\n",
    "    sparse_s: bool = False,\n",
    "    id_col: str = \"unique_id\",\n",
    "    time_col: str = \"ds\", \n",
    "    target_cols: list[str] = [\"y\"],      \n",
    ") -> tuple[FrameT, FrameT, dict]:\n",
    "    \"\"\"Utils Aggregation Function.\n",
    "    Aggregates bottom level series contained in the DataFrame `df` according\n",
    "    to levels defined in the `spec` list.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        Dataframe with columns `[time_col, *target_cols]`, columns to aggregate and optionally exog_vars.\n",
    "    spec : list of list of str\n",
    "        list of levels. Each element of the list should contain a list of columns of `df` to aggregate.\n",
    "    exog_vars: dictionary of string keys & values that can either be a list of strings or a single string\n",
    "        keys correspond to column names and the values represent the aggregation(s) that will be applied to each column. Accepted values are those from Pandas or Polars aggregation Functions, check the respective docs for guidance\n",
    "    is_balanced : bool (default=False)\n",
    "        Deprecated.        \n",
    "    sparse_s : bool (default=False)\n",
    "        Return `S_df` as a sparse Pandas dataframe.\n",
    "    id_col : str (default='unique_id')\n",
    "        Column that will identify each serie after aggregation.\n",
    "    time_col : str (default='ds')\n",
    "        Column that identifies each timestep, its values can be timestamps or integers.\n",
    "    target_cols : (default=['y'])\n",
    "        list of columns that contains the targets to aggregate.        \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Y_df : DataFrame\n",
    "        Hierarchically structured series.\n",
    "    S_df : DataFrame\n",
    "        Summing dataframe.\n",
    "    tags : dict\n",
    "        Aggregation indices.\n",
    "    \"\"\"\n",
    "    # To Narwhals\n",
    "    df_nw = nw.from_native(df)\n",
    "    native_namespace = nw.get_native_namespace(df_nw)\n",
    "\n",
    "    # Checks\n",
    "    # Generate order-preserving list of unique cols based on spec\n",
    "    seen = set()\n",
    "    spec_cols = [col for cols in spec for col in cols if col not in seen and not seen.add(col)]  # type: ignore[func-returns-value]\n",
    "    \n",
    "    # Check if last level in spec contains all levels\n",
    "    missing_cols_in_bottom_spec = set(spec_cols) - set(spec[-1])\n",
    "    if len(missing_cols_in_bottom_spec) > 0:\n",
    "        raise ValueError(f\"Check the last (bottom) level of spec, it has missing columns: {reprlib.repr(missing_cols_in_bottom_spec)}\")\n",
    "\n",
    "    if sparse_s and not nw.dependencies.is_pandas_dataframe(df):\n",
    "        raise ValueError(\"Sparse output is only supported for Pandas DataFrames.\")\n",
    "    \n",
    "    for col in df_nw.columns:\n",
    "        if df_nw[col].is_null().any():\n",
    "            raise ValueError(f\"Column {col} contains null values. Make sure no column in the DataFrame contains null values.\")\n",
    "\n",
    "    # Check whether all columns in the spec are in the df\n",
    "    aggregation_cols_in_spec = list(dict.fromkeys([col for cols in spec for col in cols]))\n",
    "    for col in aggregation_cols_in_spec:\n",
    "        if col not in df_nw.columns:\n",
    "            raise ValueError(f\"Column {col} in spec not present in df\")\n",
    "\n",
    "    # Prepare the aggregation dictionary  \n",
    "    agg_dict = dict(zip(target_cols, tuple(zip(target_cols, len(target_cols)*[\"sum\"]))))\n",
    "\n",
    "    # Check if exog_vars are present in df & add to the aggregation dictionary if it is not None\n",
    "    exog_var_names = []\n",
    "    if exog_vars is not None:\n",
    "        missing_vars = [var for var in exog_vars.keys() if var not in df.columns]\n",
    "        if missing_vars:\n",
    "            raise ValueError(f\"The following exogenous variables are not present in the DataFrame: {', '.join(missing_vars)}\")    \n",
    "        else:\n",
    "          # Update agg_dict to handle multiple aggregations for each exog_vars key\n",
    "            for key, agg_func in exog_vars.items():\n",
    "                # Ensure agg_func is a list\n",
    "                if isinstance(agg_func, str):  # If it's a single string, convert to list\n",
    "                    agg_func = [agg_func]\n",
    "                elif not isinstance(agg_func, list):  # Raise an error if it's neither\n",
    "                    raise ValueError(f\"Aggregation functions for '{key}' must be a string or a list of strings.\")\n",
    "                \n",
    "                for func in agg_func:\n",
    "                    agg_dict[f\"{key}_{func}\"] = (key, func)  # Update the agg_dict with the new naming structure\n",
    "                    exog_var_names.append(f\"{key}_{func}\")\n",
    "\n",
    "    # compute aggregations and tags\n",
    "    spec = sorted(spec, key=len)\n",
    "\n",
    "    tags = {}\n",
    "    Y_nws = []\n",
    "    category_list = []\n",
    "    level_sep = \"/\"\n",
    "    # Perform the aggregation\n",
    "    for level in spec:\n",
    "        level_name = '/'.join(level)\n",
    "\n",
    "        # Create Y_df\n",
    "        Y_level = df_nw.group_by(level + [time_col]).agg(\n",
    "            *[getattr(nw.col(col), agg)().alias(col_name) for col_name, (col, agg) in agg_dict.items()]\n",
    "        )\n",
    "        Y_level = Y_level.select(nw.concat_str( [nw.col(col) for col in level],\n",
    "                separator=level_sep).alias(id_col), \n",
    "                            nw.all())\n",
    "        Y_level = Y_level[[id_col, time_col, *target_cols] + exog_var_names]\n",
    "        Y_level = Y_level.sort(by=[id_col, time_col])\n",
    "        Y_nws.append(Y_level)\n",
    "\n",
    "        tags[level_name] = Y_level[id_col].unique().sort().to_numpy()\n",
    "        category_list.extend(tags[level_name])\n",
    "\n",
    "    Y_nw = nw.concat(Y_nws, how=\"vertical\")\n",
    "    Y_nw = nw.maybe_reset_index(Y_nw)\n",
    "    Y_df = Y_nw.to_native()\n",
    "    \n",
    "    # construct S\n",
    "    bottom = spec[-1]\n",
    "    bottom_levels = tags[level_name]\n",
    "    S = np.empty((len(bottom_levels), len(spec)), dtype=object)\n",
    "\n",
    "    for j, levels in enumerate(spec[:-1]):\n",
    "        S[:, j] = _to_upper_hierarchy(bottom, bottom_levels, '/'.join(levels))\n",
    "    S[:, -1] = tags[level_name]\n",
    "    categories = list(tags.values())\n",
    "    \n",
    "    encoder = OneHotEncoder(categories=categories, sparse_output=sparse_s, dtype=np.float64)  \n",
    "    S_dum = encoder.fit_transform(S)\n",
    "    \n",
    "    if not sparse_s:\n",
    "        S_nw = nw.from_dict({\n",
    "                            **{id_col: category_list},\n",
    "                            **dict(zip(tags[level_name], S_dum)),\n",
    "                            }, \n",
    "                            native_namespace=native_namespace)\n",
    "        S_nw = nw.maybe_reset_index(S_nw)\n",
    "        S_df = S_nw.to_native()\n",
    "    else:\n",
    "        S_df = pd.DataFrame.sparse.from_spmatrix(S_dum.T, columns=list(bottom_levels),\n",
    "                                                 index=category_list)\n",
    "        S_df = S_df.reset_index(names=id_col)\n",
    "\n",
    "    return Y_df, S_df, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cea2f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/hierarchicalforecast/blob/main/hierarchicalforecast/utils.py#L81){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### aggregate\n",
       "\n",
       ">      aggregate\n",
       ">                 (df:Union[ForwardRef('DataFrame[Any]'),ForwardRef('LazyFrame[A\n",
       ">                 ny]')], spec:list[list[str]],\n",
       ">                 exog_vars:Optional[dict[str,Union[str,list[str]]]]=None,\n",
       ">                 sparse_s:bool=False, id_col:str='unique_id',\n",
       ">                 time_col:str='ds', target_cols:list[str]=['y'])\n",
       "\n",
       "*Utils Aggregation Function.\n",
       "Aggregates bottom level series contained in the DataFrame `df` according\n",
       "to levels defined in the `spec` list.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| df | Union |  | Dataframe with columns `[time_col, *target_cols]`, columns to aggregate and optionally exog_vars. |\n",
       "| spec | list |  | list of levels. Each element of the list should contain a list of columns of `df` to aggregate. |\n",
       "| exog_vars | Optional | None |  |\n",
       "| sparse_s | bool | False | Return `S_df` as a sparse Pandas dataframe. |\n",
       "| id_col | str | unique_id | Column that will identify each serie after aggregation. |\n",
       "| time_col | str | ds | Column that identifies each timestep, its values can be timestamps or integers. |\n",
       "| target_cols | list | ['y'] | list of columns that contains the targets to aggregate.         |\n",
       "| **Returns** | **tuple** |  | **Hierarchically structured series.** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/hierarchicalforecast/blob/main/hierarchicalforecast/utils.py#L81){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### aggregate\n",
       "\n",
       ">      aggregate\n",
       ">                 (df:Union[ForwardRef('DataFrame[Any]'),ForwardRef('LazyFrame[A\n",
       ">                 ny]')], spec:list[list[str]],\n",
       ">                 exog_vars:Optional[dict[str,Union[str,list[str]]]]=None,\n",
       ">                 sparse_s:bool=False, id_col:str='unique_id',\n",
       ">                 time_col:str='ds', target_cols:list[str]=['y'])\n",
       "\n",
       "*Utils Aggregation Function.\n",
       "Aggregates bottom level series contained in the DataFrame `df` according\n",
       "to levels defined in the `spec` list.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| df | Union |  | Dataframe with columns `[time_col, *target_cols]`, columns to aggregate and optionally exog_vars. |\n",
       "| spec | list |  | list of levels. Each element of the list should contain a list of columns of `df` to aggregate. |\n",
       "| exog_vars | Optional | None |  |\n",
       "| sparse_s | bool | False | Return `S_df` as a sparse Pandas dataframe. |\n",
       "| id_col | str | unique_id | Column that will identify each serie after aggregation. |\n",
       "| time_col | str | ds | Column that identifies each timestep, its values can be timestamps or integers. |\n",
       "| target_cols | list | ['y'] | list of columns that contains the targets to aggregate.         |\n",
       "| **Returns** | **tuple** |  | **Hierarchically structured series.** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(aggregate, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e70572-9c01-466d-a3e9-7667b92def2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# simple case\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        'cat1': ['a', 'a', 'a', 'b'],\n",
    "        'cat2': ['1', '2', '3', '2'],\n",
    "        'y': [10, 20, 30, 40],\n",
    "        'ds': ['2020-01-01', '2020-02-01', '2020-03-01', '2020-02-01']\n",
    "    }\n",
    ")\n",
    "df['country'] = 'COUNTRY'\n",
    "spec = [['country'], ['country', 'cat1'], ['country', 'cat1', 'cat2']]\n",
    "Y_df, S_df, tags = aggregate(df, spec)\n",
    "test_eq(\n",
    "    list(Y_df[\"unique_id\"]), \n",
    "    3 * ['COUNTRY'] +\n",
    "    3 * ['COUNTRY/a'] +\n",
    "    ['COUNTRY/b'] +\n",
    "    ['COUNTRY/a/1', 'COUNTRY/a/2', 'COUNTRY/a/3'] +\n",
    "    ['COUNTRY/b/2']\n",
    ")\n",
    "test_eq(Y_df.query(\"unique_id == 'COUNTRY'\")['y'].values, [10, 60, 30])\n",
    "test_eq(\n",
    "    list(S_df[\"unique_id\"]),\n",
    "    ['COUNTRY', 'COUNTRY/a', 'COUNTRY/b', 'COUNTRY/a/1', 'COUNTRY/a/2', 'COUNTRY/a/3', 'COUNTRY/b/2'],\n",
    ")\n",
    "test_eq(\n",
    "    S_df.columns,\n",
    "    ['unique_id', 'COUNTRY/a/1', 'COUNTRY/a/2', 'COUNTRY/a/3', 'COUNTRY/b/2'],\n",
    ")\n",
    "expected_tags = {\n",
    "    'country': ['COUNTRY'],\n",
    "    'country/cat1': ['COUNTRY/a', 'COUNTRY/b'],\n",
    "    'country/cat1/cat2': ['COUNTRY/a/1', 'COUNTRY/a/2', 'COUNTRY/a/3','COUNTRY/b/2'],\n",
    "}\n",
    "for k, actual in tags.items():\n",
    "    test_eq(actual, expected_tags[k])\n",
    "\n",
    "# test categoricals don't produce all combinations\n",
    "df2 = df.copy()\n",
    "for col in ('country', 'cat1', 'cat2'):\n",
    "    df2[col] = df2[col].astype('category')\n",
    "\n",
    "Y_df2, *_ = aggregate(df2, spec)\n",
    "assert Y_df.shape[0] == Y_df2.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fc6197",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import polars as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a81057",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# Testing equivalence of other dataframe libs to pandas results. \n",
    "# TODO: extend for other frameworks\n",
    "def test_eq_agg_dataframe(df, spec, frameworks=[\"polars\"]):\n",
    "    for framework in frameworks:\n",
    "        if framework == \"polars\":\n",
    "            df_f = pl.from_pandas(df)\n",
    "        else:\n",
    "            raise ValueError(f\"Framework {framework} not recognized\")\n",
    "        \n",
    "        Y_df, S_df, tags = aggregate(df=df, spec=spec)\n",
    "        Y_df_f, S_df_f, tags_f = aggregate(df=df_f, spec=spec)\n",
    "\n",
    "        pd.testing.assert_frame_equal(Y_df, Y_df_f.to_pandas())\n",
    "        pd.testing.assert_frame_equal(S_df, S_df_f.to_pandas())\n",
    "        for tag in tags:\n",
    "            test_eq(tags[tag], tags_f[tag])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b109ad63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# polars\n",
    "test_eq_agg_dataframe(df, spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae76480-44d9-45ec-b50a-f8b666cc0200",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ospra\\AppData\\Local\\Temp\\ipykernel_7984\\2654088726.py:4: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "  dates = pd.date_range(start='2019-01-31', freq='M', periods=max_tenure)\n",
      "c:\\Users\\ospra\\miniconda3\\envs\\hierarchicalforecast-backup\\lib\\site-packages\\utilsforecast\\data.py:108: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "  freq = pd.tseries.frequencies.to_offset(freq)\n",
      "c:\\Users\\ospra\\miniconda3\\envs\\hierarchicalforecast-backup\\lib\\site-packages\\utilsforecast\\data.py:108: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "  freq = pd.tseries.frequencies.to_offset(freq)\n",
      "c:\\Users\\ospra\\miniconda3\\envs\\hierarchicalforecast-backup\\lib\\site-packages\\utilsforecast\\data.py:108: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "  freq = pd.tseries.frequencies.to_offset(freq)\n",
      "c:\\Users\\ospra\\miniconda3\\envs\\hierarchicalforecast-backup\\lib\\site-packages\\utilsforecast\\data.py:108: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "  freq = pd.tseries.frequencies.to_offset(freq)\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "# test unbalanced dataset\n",
    "max_tenure = 24\n",
    "dates = pd.date_range(start='2019-01-31', freq='M', periods=max_tenure)\n",
    "cohort_tenure = [24, 23, 22, 21]\n",
    "\n",
    "ts_list = []\n",
    "\n",
    "# Create ts for each cohort\n",
    "for i in range(len(cohort_tenure)):\n",
    "    ts_list.append(\n",
    "        generate_series(n_series=1, freq='M', min_length=cohort_tenure[i], max_length=cohort_tenure[i]).reset_index() \\\n",
    "            .assign(ult=i) \\\n",
    "            .assign(ds=dates[-cohort_tenure[i]:]) \\\n",
    "            .drop(columns=['unique_id'])\n",
    "    )\n",
    "df = pd.concat(ts_list, ignore_index=True)\n",
    "\n",
    "# Create categories\n",
    "df['pen'] = np.where(df['ult'] < 2, 'a', 'b')\n",
    "# Note that unique id requires strings\n",
    "df['ult'] = df['ult'].astype(str)\n",
    "\n",
    "hier_levels = [\n",
    "    ['pen'],\n",
    "    ['pen', 'ult'],\n",
    "]\n",
    "\n",
    "hier_df, S_df, tags = aggregate(df=df, spec=hier_levels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a415c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# polars\n",
    "test_eq_agg_dataframe(df, hier_levels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fd8bd9-d7e8-4602-a1ad-021f404532f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/Nixtla/transfer-learning-time-series/main/datasets/tourism.csv')\n",
    "df = df.rename({'Trips': 'y', 'Quarter': 'ds'}, axis=1)\n",
    "df.insert(0, 'Country', 'Australia')\n",
    "\n",
    "# strictly hierarchical structure\n",
    "hiers_strictly = [['Country'],\n",
    "                  ['Country', 'State'], \n",
    "                  ['Country', 'State', 'Region']]\n",
    "\n",
    "# test strict\n",
    "hier_df, S_df, tags = aggregate(df=df, spec=hiers_strictly)\n",
    "test_eq(len(hier_df), 6800)\n",
    "test_eq(hier_df[\"unique_id\"].nunique(), 85)\n",
    "test_eq(S_df.shape, (85, 77))\n",
    "test_eq(hier_df[\"unique_id\"].unique(), S_df[\"unique_id\"])\n",
    "test_eq(len(tags), len(hiers_strictly))                \n",
    "\n",
    "# grouped structure\n",
    "hiers_grouped = [['Country'],\n",
    "                 ['Country', 'State'], \n",
    "                 ['Country', 'Purpose'], \n",
    "                 ['Country', 'State', 'Region'], \n",
    "                 ['Country', 'State', 'Purpose'], \n",
    "                 ['Country', 'State', 'Region', 'Purpose'],\n",
    "                 ]\n",
    "\n",
    "# test grouped\n",
    "hier_df, S_df, tags = aggregate(df=df, spec=hiers_grouped)\n",
    "test_eq(len(hier_df), 34_000)\n",
    "test_eq(hier_df[\"unique_id\"].nunique(), 425)\n",
    "test_eq(S_df.shape, (425, 305))\n",
    "test_eq(hier_df[\"unique_id\"].unique(), S_df[\"unique_id\"])\n",
    "test_eq(len(tags), len(hiers_grouped))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e423d4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# polars\n",
    "test_eq_agg_dataframe(df, hiers_strictly)\n",
    "test_eq_agg_dataframe(df, hiers_grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1cb923",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/Nixtla/transfer-learning-time-series/main/datasets/tourism.csv')\n",
    "df = df.rename({'Trips': 'y', 'Quarter': 'ds'}, axis=1)\n",
    "df.insert(0, 'Country', 'Australia')\n",
    "\n",
    "#Unit Test NaN Values\n",
    "df_nan = df.copy()\n",
    "df_nan.loc[0, 'Region'] = float('nan')\n",
    "test_fail(\n",
    "    aggregate,\n",
    "    contains='null values',\n",
    "    args=(df_nan, hiers_strictly),\n",
    ")\n",
    "\n",
    "#Unit Test None Values\n",
    "df_none = df.copy()\n",
    "df_none.loc[0, 'Region'] = None\n",
    "test_fail(\n",
    "    aggregate,\n",
    "    contains='null values',\n",
    "    args=(df_none, hiers_strictly),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aaf9294",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# polars\n",
    "\n",
    "#Unit Test NaN Values\n",
    "df_nan_pl = pl.DataFrame(df_nan)\n",
    "test_fail(\n",
    "    aggregate,\n",
    "    contains='null values',\n",
    "    args=(df_nan_pl, hiers_strictly),\n",
    ")\n",
    "\n",
    "#Unit Test None Values\n",
    "df_none_pl = pl.DataFrame(df_none)\n",
    "test_fail(\n",
    "    aggregate,\n",
    "    contains='null values',\n",
    "    args=(df_none_pl, hiers_strictly),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7688d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code block 'strict non-sparse aggregate' took:\t0.03771 seconds\n",
      "Code block 'strict sparse aggregate' took:\t0.04140 seconds\n",
      "Code block 'grouped non-sparse aggregate' took:\t0.09591 seconds\n",
      "Code block 'grouped sparse aggregate' took:\t0.09406 seconds\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "# Test equality of sparse and non-sparse aggregation\n",
    "with CodeTimer('strict non-sparse aggregate'):\n",
    "    Y_df, S_df, tags = aggregate(df=df, sparse_s=False, spec=hiers_strictly)\n",
    "\n",
    "with CodeTimer('strict sparse aggregate'):\n",
    "    Y_df_sparse, S_df_sparse, tags_sparse = aggregate(df=df, sparse_s=True, spec=hiers_strictly)\n",
    "\n",
    "test_close(Y_df.y.values, Y_df_sparse.y.values)\n",
    "test_eq(S_df.values, S_df_sparse.values)\n",
    "\n",
    "test_eq(S_df.columns, S_df_sparse.columns)\n",
    "test_eq(S_df.index, S_df_sparse.index)\n",
    "\n",
    "test_eq(Y_df.columns, Y_df_sparse.columns)\n",
    "test_eq(Y_df.index, Y_df_sparse.index)\n",
    "\n",
    "with CodeTimer('grouped non-sparse aggregate'):\n",
    "    Y_df, S_df, tags = aggregate(df=df, sparse_s=False, spec=hiers_grouped)\n",
    "\n",
    "with CodeTimer('grouped sparse aggregate'):\n",
    "    Y_df_sparse, S_df_sparse, tags_sparse = aggregate(df=df, sparse_s=True, spec=hiers_grouped)\n",
    "\n",
    "test_close(Y_df.y.values, Y_df_sparse.y.values)\n",
    "test_eq(S_df.values, S_df_sparse.values)\n",
    "\n",
    "test_eq(S_df.columns, S_df_sparse.columns)\n",
    "test_eq(S_df.index, S_df_sparse.index)\n",
    "\n",
    "test_eq(Y_df.columns, Y_df_sparse.columns)\n",
    "test_eq(Y_df.index, Y_df_sparse.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449090c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def aggregate_temporal(\n",
    "    df: Frame,\n",
    "    spec: list[list[str]],\n",
    "    freq: Union[str, int],\n",
    "    exog_vars: Optional[dict[str, Union[str, list[str]]]] = None,\n",
    "    sparse_s: bool = False,\n",
    "    id_col: str = \"unique_id\",\n",
    "    time_col: str = \"ds\", \n",
    "    id_time_col: str = \"temporal_id\",\n",
    "    target_cols: list[str] = [\"y\"],      \n",
    ") -> tuple[FrameT, FrameT, dict]:\n",
    "    \"\"\"Utils Aggregation Function for Temporal aggregations.\n",
    "    Aggregates bottom level timesteps contained in the DataFrame `df` according\n",
    "    to temporal levels defined in the `spec` list.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        Dataframe with columns `[time_col, target_cols]` and columns to aggregate.\n",
    "    spec : list of list of str\n",
    "        List of temporal levels. May only contain string aliases of timestamp attributes.\n",
    "    freq : str or int\n",
    "        Frequency of the data. Must be a valid pandas or polars offset alias, or an integer.\n",
    "    exog_vars: dictionary of string keys & values that can either be a list of strings or a single string\n",
    "        keys correspond to column names and the values represent the aggregation(s) that will be applied to each column. Accepted values are those from Pandas or Polars aggregation Functions, check the respective docs for guidance\n",
    "    sparse_s : bool (default=False)\n",
    "        Return `S_df` as a sparse Pandas dataframe.\n",
    "    id_col : str (default='unique_id')\n",
    "        Column that will identify each serie after aggregation.\n",
    "    time_col : str (default='ds')\n",
    "        Column that identifies each timestep, its values can be timestamps or integers.\n",
    "    id_time_col : str (default='temporal_id')\n",
    "        Column that will identify each timestep after aggregation.\n",
    "    target_cols : (default=['y'])\n",
    "        List of columns that contain the targets to aggregate.        \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Y_df : DataFrame\n",
    "        Temporally hierarchically structured series.\n",
    "    S_df : DataFrame\n",
    "        Temporal summing dataframe.\n",
    "    tags : dict\n",
    "        Temporal aggregation indices.\n",
    "    \"\"\"\n",
    "    # Generate order-preserving list of unique time features based on spec\n",
    "    fseen = set()\n",
    "    time_features = [col for cols in spec for col in cols if col not in fseen and not fseen.add(col)] # type: ignore[func-returns-value]\n",
    "    # Check if last level in spec is equivalent to time features\n",
    "    missing_time_features_in_bottom_spec = set(time_features) - set(spec[-1])\n",
    "    if len(missing_time_features_in_bottom_spec) > 0:\n",
    "        raise ValueError(f\"Check the last (bottom) level of spec, it has missing time features: {reprlib.repr(missing_time_features_in_bottom_spec)}\")\n",
    "    \n",
    "    # If target_cols is not in df, we add a placeholder column so that we can compute the aggregations\n",
    "    add_placeholder = False\n",
    "    if set(df.columns) == set([time_col, id_col]):\n",
    "        add_placeholder = True\n",
    "        df_nw = nw.from_native(df)\n",
    "        df_nw = df_nw.with_columns(nw.lit(0).alias(\"y\"))\n",
    "        df = df_nw.to_native()\n",
    "\n",
    "    # Compute time features\n",
    "    time_features.remove(time_col)\n",
    "    df, _ = ufe.time_features(df=df, \n",
    "                                freq=freq, \n",
    "                                h=0, \n",
    "                                features=time_features, \n",
    "                                time_col=time_col,\n",
    "                                id_col=id_col)\n",
    "   \n",
    "    # Create the aggregation\n",
    "    Y_df, S_df, tags = aggregate(df = df, \n",
    "                                spec = spec, \n",
    "                                exog_vars=exog_vars,\n",
    "                                sparse_s=sparse_s,\n",
    "                                id_col=id_time_col, \n",
    "                                target_cols=target_cols,\n",
    "                                time_col=id_col)   \n",
    "    Y_nw = nw.from_native(Y_df)\n",
    "\n",
    "    # Add unique_id_ds column to Y_df\n",
    "    # This is a bit dirty but Narwhals does not support string splitting\n",
    "    timestamps = pd.to_datetime(Y_nw[id_time_col].to_pandas().str.split(\"/\").str[-1], format=\"mixed\").values\n",
    "    Y_nw = Y_nw.with_columns(**{time_col: timestamps})\n",
    "\n",
    "    # Drop the placeholder column if it was added\n",
    "    if add_placeholder:\n",
    "        Y_nw = Y_nw.drop(\"y\")\n",
    "\n",
    "    Y_df = Y_nw.to_native()\n",
    "\n",
    "    return Y_df, S_df, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fc4aad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/hierarchicalforecast/blob/main/hierarchicalforecast/utils.py#L257){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### aggregate_temporal\n",
       "\n",
       ">      aggregate_temporal\n",
       ">                          (df:Union[ForwardRef('DataFrame[Any]'),ForwardRef('La\n",
       ">                          zyFrame[Any]')], spec:list[list[str]],\n",
       ">                          freq:Union[str,int], exog_vars:Optional[dict[str,Unio\n",
       ">                          n[str,list[str]]]]=None, sparse_s:bool=False,\n",
       ">                          id_col:str='unique_id', time_col:str='ds',\n",
       ">                          id_time_col:str='temporal_id',\n",
       ">                          target_cols:list[str]=['y'])\n",
       "\n",
       "*Utils Aggregation Function for Temporal aggregations.\n",
       "Aggregates bottom level timesteps contained in the DataFrame `df` according\n",
       "to temporal levels defined in the `spec` list.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| df | Union |  | Dataframe with columns `[time_col, target_cols]` and columns to aggregate. |\n",
       "| spec | list |  | List of temporal levels. May only contain string aliases of timestamp attributes. |\n",
       "| freq | Union |  | Frequency of the data. Must be a valid pandas or polars offset alias, or an integer. |\n",
       "| exog_vars | Optional | None |  |\n",
       "| sparse_s | bool | False | Return `S_df` as a sparse Pandas dataframe. |\n",
       "| id_col | str | unique_id | Column that will identify each serie after aggregation. |\n",
       "| time_col | str | ds | Column that identifies each timestep, its values can be timestamps or integers. |\n",
       "| id_time_col | str | temporal_id | Column that will identify each timestep after aggregation. |\n",
       "| target_cols | list | ['y'] | List of columns that contain the targets to aggregate.         |\n",
       "| **Returns** | **tuple** |  | **Temporally hierarchically structured series.** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/hierarchicalforecast/blob/main/hierarchicalforecast/utils.py#L257){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### aggregate_temporal\n",
       "\n",
       ">      aggregate_temporal\n",
       ">                          (df:Union[ForwardRef('DataFrame[Any]'),ForwardRef('La\n",
       ">                          zyFrame[Any]')], spec:list[list[str]],\n",
       ">                          freq:Union[str,int], exog_vars:Optional[dict[str,Unio\n",
       ">                          n[str,list[str]]]]=None, sparse_s:bool=False,\n",
       ">                          id_col:str='unique_id', time_col:str='ds',\n",
       ">                          id_time_col:str='temporal_id',\n",
       ">                          target_cols:list[str]=['y'])\n",
       "\n",
       "*Utils Aggregation Function for Temporal aggregations.\n",
       "Aggregates bottom level timesteps contained in the DataFrame `df` according\n",
       "to temporal levels defined in the `spec` list.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| df | Union |  | Dataframe with columns `[time_col, target_cols]` and columns to aggregate. |\n",
       "| spec | list |  | List of temporal levels. May only contain string aliases of timestamp attributes. |\n",
       "| freq | Union |  | Frequency of the data. Must be a valid pandas or polars offset alias, or an integer. |\n",
       "| exog_vars | Optional | None |  |\n",
       "| sparse_s | bool | False | Return `S_df` as a sparse Pandas dataframe. |\n",
       "| id_col | str | unique_id | Column that will identify each serie after aggregation. |\n",
       "| time_col | str | ds | Column that identifies each timestep, its values can be timestamps or integers. |\n",
       "| id_time_col | str | temporal_id | Column that will identify each timestep after aggregation. |\n",
       "| target_cols | list | ['y'] | List of columns that contain the targets to aggregate.         |\n",
       "| **Returns** | **tuple** |  | **Temporally hierarchically structured series.** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(aggregate_temporal, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16f49c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# simple case\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        'cat1': 8 * ['a'] + 8 * ['b'],\n",
    "        'y': np.linspace(10, 160, 16),\n",
    "        'ds': 2 * ['2020-01-01', '2020-04-01', '2020-07-01', '2020-10-01', '2021-01-01', '2021-04-01', '2021-07-01', '2021-10-01']\n",
    "    }\n",
    ")\n",
    "df['country'] = 'COUNTRY'\n",
    "df[\"unique_id\"] = df[\"country\"] + \"/\" + df[\"cat1\"]\n",
    "df[\"ds\"] = pd.to_datetime(df[\"ds\"])\n",
    "\n",
    "spec = [['year'], ['year', 'ds']]\n",
    "Y_df, S_df, tags = aggregate_temporal(df, spec, freq='QS')\n",
    "\n",
    "test_eq(\n",
    "    list(Y_df[\"unique_id\"]), \n",
    "    10 * ['COUNTRY/a', 'COUNTRY/b'] \n",
    ")\n",
    "test_eq(\n",
    "    list(Y_df[\"temporal_id\"]), \n",
    "    2 * ['2020'] + 2 * ['2021'] + 2 * ['2020/2020-01-01'] + 2 * ['2020/2020-04-01'] + 2 * ['2020/2020-07-01'] + 2 * ['2020/2020-10-01'] + 2 * ['2021/2021-01-01'] + 2 * ['2021/2021-04-01'] + 2 * ['2021/2021-07-01'] + 2 * ['2021/2021-10-01']\n",
    ")\n",
    "test_eq(Y_df.query(\"unique_id == 'COUNTRY/a' & temporal_id in ['2020', '2021']\")['y'].values, [100, 260])\n",
    "test_eq(\n",
    "    list(S_df[\"temporal_id\"]),\n",
    "    ['2020', '2021', '2020/2020-01-01', '2020/2020-04-01', '2020/2020-07-01', '2020/2020-10-01', '2021/2021-01-01', '2021/2021-04-01', '2021/2021-07-01', '2021/2021-10-01'],\n",
    ")\n",
    "test_eq(\n",
    "    S_df.columns,\n",
    "    ['temporal_id', '2020/2020-01-01', '2020/2020-04-01', '2020/2020-07-01', '2020/2020-10-01', '2021/2021-01-01', '2021/2021-04-01', '2021/2021-07-01', '2021/2021-10-01'],\n",
    ")\n",
    "expected_tags = {\n",
    "    'year': ['2020', '2021'],\n",
    "    'year/ds': ['2020/2020-01-01', '2020/2020-04-01', '2020/2020-07-01', '2020/2020-10-01', '2021/2021-01-01', '2021/2021-04-01', '2021/2021-07-01', '2021/2021-10-01'],\n",
    "}\n",
    "for k, actual in tags.items():\n",
    "    test_eq(actual, expected_tags[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43beaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# Testing equivalence of other dataframe libs to pandas results. \n",
    "# TODO: extend for other frameworks\n",
    "def test_eq_agg_dataframe(df, spec, frameworks=[\"polars\"]):\n",
    "    for framework in frameworks:\n",
    "        if framework == \"polars\":\n",
    "            df_f = pl.from_pandas(df)\n",
    "        else:\n",
    "            raise ValueError(f\"Framework {framework} not recognized\")\n",
    "        \n",
    "        Y_df, S_df, tags = aggregate_temporal(df=df, spec=spec, freq='QS')\n",
    "        Y_df_f, S_df_f, tags_f = aggregate_temporal(df=df_f, spec=spec, freq='QS')\n",
    "\n",
    "        pd.testing.assert_frame_equal(Y_df[[\"unique_id\", \"ds\", \"y\"]], Y_df_f[[\"unique_id\", \"ds\", \"y\"]].to_pandas())\n",
    "        np.testing.assert_array_equal(S_df.iloc[1:, 1:], S_df_f.to_pandas().iloc[1:, 1:])\n",
    "        # for tag in tags:\n",
    "        #     test_eq(tags[tag], tags_f[tag])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2263e4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# polars\n",
    "test_eq_agg_dataframe(df, spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83759ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def make_future_dataframe(\n",
    "    df: Frame,\n",
    "    freq: Union[str, int],\n",
    "    h: int,\n",
    "    id_col: str = \"unique_id\",\n",
    "    time_col: str = \"ds\",\n",
    ") -> FrameT:\n",
    "    \"\"\"Create future dataframe for forecasting.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas or polars DataFrame\n",
    "        Dataframe with ids, times and values for the exogenous regressors.\n",
    "    freq : str or int\n",
    "        Frequency of the data. Must be a valid pandas or polars offset alias, or an integer.\n",
    "    h : int\n",
    "        Forecast horizon.\n",
    "    id_col : str (default='unique_id')\n",
    "        Column that identifies each serie.\n",
    "    time_col : str (default='ds')\n",
    "        Column that identifies each timestep, its values can be timestamps or integers.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    future_df : pandas or polars DataFrame\n",
    "        DataFrame with future values\n",
    "    \"\"\"\n",
    "    times_by_id = ufp.group_by_agg(df, id_col, {time_col: \"max\"}, maintain_order=True)\n",
    "    times_by_id = ufp.sort(times_by_id, id_col)\n",
    "    future_df = ufp.make_future_dataframe(\n",
    "        uids=times_by_id[id_col],\n",
    "        last_times=times_by_id[time_col],\n",
    "        freq=freq,\n",
    "        h=h,\n",
    "        id_col=id_col,\n",
    "        time_col=time_col,\n",
    "    )\n",
    "    return future_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043cf5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(make_future_dataframe, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d218ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_cross_temporal_tags(df: Frame, \n",
    "                            tags_cs: dict[str, np.ndarray], \n",
    "                            tags_te: dict[str, np.ndarray], \n",
    "                            sep: str = \"//\", \n",
    "                            id_col: str = \"unique_id\", \n",
    "                            id_time_col: str = \"temporal_id\", \n",
    "                            cross_temporal_id_col: str = \"cross_temporal_id\") -> tuple[FrameT, dict[str, np.ndarray]]:\n",
    "    \"\"\"Get cross-temporal tags.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        DataFrame with temporal ids.\n",
    "    tags_cs : dict\n",
    "        Tags for the cross-sectional hierarchies\n",
    "    tags_te : dict\n",
    "        Tags for the temporal hierarchies\n",
    "    sep : str (default=\"//\")\n",
    "        Separator for the cross-temporal tags.\n",
    "    id_col : str (default='unique_id')\n",
    "        Column that identifies each serie.\n",
    "    id_time_col : str (default='temporal_id')\n",
    "        Column that identifies each (aggregated) timestep.\n",
    "    cross_temporal_id_col : str (default='cross_temporal_id')\n",
    "        Column that will identify each cross-temporal aggregation.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df: DataFrame\n",
    "        DataFrame with cross-temporal ids.\n",
    "    tags_ct : dict\n",
    "        Tags for the cross-temporal hierarchies\n",
    "    \"\"\"\n",
    "    tags_ct = {}\n",
    "    for key_cs, value_cs in tags_cs.items():\n",
    "        for key_te, value_te in tags_te.items():\n",
    "            key_ct = key_cs + \"//\" + key_te\n",
    "            value_ct = list(\"//\".join(s) for s in itertools.product(value_cs, value_te))\n",
    "            tags_ct[key_ct] = value_ct\n",
    "\n",
    "    df_nw = nw.from_native(df)\n",
    "    df_nw = df_nw.with_columns(**{cross_temporal_id_col: df_nw[id_col] + sep + df_nw[id_time_col]})\n",
    "    df = df_nw.to_native()\n",
    "\n",
    "    return df, tags_ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6227c5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(get_cross_temporal_tags, title_level=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22febc26-1901-4bef-a181-09ae2f52453b",
   "metadata": {},
   "source": [
    "# Hierarchical Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b125e256-c210-4776-ac55-9841acee583b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class HierarchicalPlot:\n",
    "    \"\"\" Hierarchical Plot\n",
    "\n",
    "    This class contains a collection of matplotlib visualization methods, suited for small\n",
    "    to medium sized hierarchical series.\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `S`: DataFrame with summing matrix of size `(base, bottom)`, see [aggregate function](https://nixtla.github.io/hierarchicalforecast/utils.html#aggregate).<br>\n",
    "    `tags`: np.ndarray, with hierarchical aggregation indexes, where \n",
    "        each key is a level and its value contains tags associated to that level.<br>\n",
    "    `S_id_col` : str='unique_id', column that identifies each aggregation.<br>\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 S: Frame,\n",
    "                 tags: dict[str, np.ndarray],\n",
    "                 S_id_col: str = \"unique_id\",\n",
    "                 ):\n",
    "\n",
    "        self.S = nw.from_native(S)\n",
    "        S_cols_ex_id_col = self.S.columns\n",
    "        S_cols_ex_id_col.remove(S_id_col)\n",
    "        self.S_cols_ex_id_col = S_cols_ex_id_col\n",
    "        self.tags = tags\n",
    "\n",
    "    def plot_summing_matrix(self):\n",
    "        \"\"\" Summation Constraints plot\n",
    "        \n",
    "        This method simply plots the hierarchical aggregation\n",
    "        constraints matrix $\\mathbf{S}$.\n",
    "        \"\"\"\n",
    "        plt.figure(num=1, figsize=(4, 6), dpi=80, facecolor='w')\n",
    "        plt.spy(self.S[self.S_cols_ex_id_col].to_numpy())\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "    def plot_series(self,\n",
    "                    series: str,\n",
    "                    Y_df: Frame,\n",
    "                    models: Optional[list[str]] = None,\n",
    "                    level: Optional[list[int]] = None,\n",
    "                    id_col: str = \"unique_id\",\n",
    "                    time_col: str = \"ds\",\n",
    "                    target_col: str = \"y\",\n",
    "                    ):\n",
    "        \"\"\" Single Series plot\n",
    "\n",
    "        **Parameters:**<br>\n",
    "        `series`: str, string identifying the `'unique_id'` any-level series to plot.<br>\n",
    "        `Y_df`: DataFrame, hierarchically structured series ($\\mathbf{y}_{[a,b]}$). \n",
    "                It contains columns `['unique_id', 'ds', 'y']`, it may have `'models'`.<br>\n",
    "        `models`: list[str], string identifying filtering model columns.<br>\n",
    "        `level`: float list 0-100, confidence levels for prediction intervals available in `Y_df`.<br>\n",
    "        `id_col` : str='unique_id', column that identifies each serie.<br>\n",
    "        `time_col` : str='ds', column that identifies each timestep, its values can be timestamps or integers.<br>\n",
    "        `target_col` : str='y', column that contains the target.<br>           \n",
    "\n",
    "        **Returns:**<br>\n",
    "        Single series plot with filtered models and prediction interval level.<br><br>\n",
    "        \"\"\"\n",
    "        Y_nw = nw.from_native(Y_df)\n",
    "\n",
    "        if series not in self.S[id_col]:\n",
    "            raise Exception(f'time series {series} not found')\n",
    "        fig, ax = plt.subplots(1, 1, figsize = (20, 7))\n",
    "        df_plot = Y_nw.filter(nw.col(id_col) == series)\n",
    "        cols = models if models is not None else [col for col in df_plot.columns if col not in [id_col, time_col]]\n",
    "        cols_wo_levels = [col for col in cols if ('-lo-' not in col and '-hi-' not in col)]\n",
    "        try:\n",
    "            cmap = plt.get_cmap(\"tab10\", 10)\n",
    "        except AttributeError:\n",
    "            cmap = plt.cm.get_cmap(\"tab10\", 10)\n",
    "        cmap = [cmap(i) for i in range(10)][:len(cols_wo_levels)]\n",
    "        cmap_dict = dict(zip(cols_wo_levels, cmap))\n",
    "        for col in cols_wo_levels:\n",
    "            ax.plot(df_plot[time_col].to_numpy(), df_plot[col].to_numpy(), linewidth=2, label=col, color=cmap_dict[col])\n",
    "            if level is not None and col != target_col:\n",
    "                for lv in level:\n",
    "                    if f'{col}-lo-{lv}' not in df_plot.columns:\n",
    "                        # if model\n",
    "                        # doesnt have levels\n",
    "                        continue\n",
    "                    ax.fill_between(\n",
    "                        df_plot.select(nw.col(time_col))[time_col].to_numpy(), \n",
    "                        df_plot.select(nw.col(f'{col}-lo-{lv}'))[f'{col}-lo-{lv}'].to_numpy(), \n",
    "                        df_plot.select(nw.col(f'{col}-hi-{lv}'))[f'{col}-hi-{lv}'].to_numpy(),\n",
    "                        alpha=-lv/100 + 1,\n",
    "                        color=cmap_dict[col],\n",
    "                        label=f'{col}_level_{lv}'\n",
    "                    )\n",
    "        ax.set_title(f'{series} Forecast', fontsize=22)\n",
    "        ax.set_xlabel('Timestamp [t]', fontsize=20)\n",
    "        ax.legend(prop={'size': 15})\n",
    "        ax.grid()\n",
    "        ax.xaxis.set_major_locator(\n",
    "            plt.MaxNLocator(min(max(len(df_plot) // 10, 1), 10))\n",
    "        )\n",
    "        for label in (ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "            label.set_fontsize(20)\n",
    "                    \n",
    "    def plot_hierarchically_linked_series(self,\n",
    "                                          bottom_series: str,\n",
    "                                          Y_df: Frame,\n",
    "                                          models: Optional[list[str]] = None,\n",
    "                                          level: Optional[list[int]] = None,\n",
    "                                          id_col: str = \"unique_id\",\n",
    "                                          time_col: str = \"ds\",\n",
    "                                          target_col: str = \"y\",                                                              \n",
    "                                          ):\n",
    "        \"\"\" Hierarchically Linked Series plot\n",
    "\n",
    "        **Parameters:**<br>\n",
    "        `bottom_series`: str, string identifying the `'unique_id'` bottom-level series to plot.<br>\n",
    "        `Y_df`: DataFrame, hierarchically structured series ($\\mathbf{y}_{[a,b]}$). \n",
    "                It contains columns ['unique_id', 'ds', 'y'] and models. <br>\n",
    "        `models`: list[str], string identifying filtering model columns.<br>\n",
    "        `level`: float list 0-100, confidence levels for prediction intervals available in `Y_df`.<br>\n",
    "        `id_col` : str='unique_id', column that identifies each serie.<br>\n",
    "        `time_col` : str='ds', column that identifies each timestep, its values can be timestamps or integers.<br>\n",
    "        `target_col` : str='y', column that contains the target.<br>          \n",
    "\n",
    "        **Returns:**<br>\n",
    "        Collection of hierarchilly linked series plots associated with the `bottom_series`\n",
    "        and filtered models and prediction interval level.<br><br>\n",
    "        \"\"\"\n",
    "        Y_nw = nw.from_native(Y_df)\n",
    "\n",
    "        if bottom_series not in self.S.columns:\n",
    "            raise Exception(f'bottom time series {bottom_series} not found')\n",
    "\n",
    "        linked_series = self.S[[id_col, bottom_series]].filter(nw.col(bottom_series) == 1)[id_col].to_numpy()\n",
    "        fig, axs = plt.subplots(len(linked_series), 1, figsize=(20, 2 * len(linked_series)))\n",
    "        cols = models if models is not None else [col for col in Y_nw.columns if col not in [id_col, time_col]]\n",
    "        cols_wo_levels = [col for col in cols if ('-lo-' not in col and '-hi-' not in col)]\n",
    "        cmap = plt.cm.get_cmap(\"tab10\", 10)\n",
    "        cmap = [cmap(i) for i in range(10)][:len(cols_wo_levels)]\n",
    "        cmap_dict = dict(zip(cols_wo_levels, cmap))\n",
    "        for idx, series in enumerate(linked_series):\n",
    "            df_plot = Y_nw.filter(nw.col(id_col) == series)\n",
    "            for col in cols_wo_levels:\n",
    "                axs[idx].plot(df_plot[time_col].to_numpy(), df_plot[col].to_numpy(), linewidth=2, label=col, color=cmap_dict[col])\n",
    "                if level is not None and col != target_col:\n",
    "                    for lv in level:\n",
    "                        if f'{col}-lo-{lv}' not in df_plot.columns:\n",
    "                            # if model\n",
    "                            # doesnt have levels\n",
    "                            continue\n",
    "                        axs[idx].fill_between(\n",
    "                            df_plot.select(nw.col(time_col))[time_col].to_numpy(), \n",
    "                            df_plot.select(nw.col(f'{col}-lo-{lv}'))[f'{col}-lo-{lv}'].to_numpy(), \n",
    "                            df_plot.select(nw.col(f'{col}-hi-{lv}'))[f'{col}-hi-{lv}'].to_numpy(),                          \n",
    "                            alpha=-lv/100 + 1,\n",
    "                            color=cmap_dict[col],\n",
    "                            label=f'{col}_level_{lv}'\n",
    "                        )\n",
    "            axs[idx].set_title(f'{series}', fontsize=10)\n",
    "            axs[idx].grid()\n",
    "            axs[idx].get_xaxis().label.set_visible(False)\n",
    "            axs[idx].legend().set_visible(False)\n",
    "            axs[idx].xaxis.set_major_locator(\n",
    "                plt.MaxNLocator(min(max(len(df_plot) // 10, 1), 10))\n",
    "            )\n",
    "            for label in (axs[idx].get_xticklabels() + axs[idx].get_yticklabels()):\n",
    "                label.set_fontsize(10)\n",
    "        plt.subplots_adjust(hspace=0.4)\n",
    "        handles, labels = axs[0].get_legend_handles_labels()\n",
    "        kwargs = dict(loc='lower center', \n",
    "                      prop={'size': 10}, \n",
    "                      bbox_to_anchor=(0, 0.05, 1, 1))\n",
    "        if sys.version_info.minor > 7:\n",
    "            kwargs['ncols'] = np.max([2, np.ceil(len(labels) / 2)])\n",
    "        fig.legend(handles, labels, **kwargs)\n",
    "\n",
    "    def plot_hierarchical_predictions_gap(self,\n",
    "                                          Y_df: Frame,\n",
    "                                          models: Optional[list[str]] = None,\n",
    "                                          xlabel: Optional[str] = None,\n",
    "                                          ylabel: Optional[str] = None,\n",
    "                                          id_col: str = \"unique_id\",\n",
    "                                          time_col: str = \"ds\",\n",
    "                                          target_col: str = \"y\",                                             \n",
    "                                          ):\n",
    "        \"\"\" Hierarchically Predictions Gap plot\n",
    "\n",
    "        **Parameters:**<br>\n",
    "        `Y_df`: DataFrame, hierarchically structured series ($\\mathbf{y}_{[a,b]}$). \n",
    "                It contains columns ['unique_id', 'ds', 'y'] and models. <br>\n",
    "        `models`: list[str], string identifying filtering model columns. <br>\n",
    "        `xlabel`: str, string for the plot's x axis label.<br>\n",
    "        `ylabel`: str, string for the plot's y axis label.<br>\n",
    "        `id_col` : str='unique_id', column that identifies each serie.<br>\n",
    "        `time_col` : str='ds', column that identifies each timestep, its values can be timestamps or integers.<br>\n",
    "        `target_col` : str='y', column that contains the target.<br>              \n",
    "\n",
    "        **Returns:**<br>\n",
    "        Plots of aggregated predictions at different levels of the hierarchical structure.\n",
    "        The aggregation is performed according to the tag levels see \n",
    "        [aggregate function](https://nixtla.github.io/hierarchicalforecast/utils.html).<br><br>\n",
    "        \"\"\"\n",
    "        Y_nw = nw.from_native(Y_df)\n",
    "        \n",
    "        # Parse predictions dataframe\n",
    "        horizon_dates = Y_nw['ds'].unique().to_numpy()\n",
    "        cols = models if models is not None else [col for col in Y_nw.columns if col not in [id_col, time_col]]        \n",
    "        \n",
    "        # Plot predictions across tag levels\n",
    "        fig, ax = plt.subplots(figsize=(8, 5))\n",
    "        \n",
    "        if target_col in Y_nw.columns:\n",
    "            idx_top = self.S.with_columns(sum_cols = nw.sum_horizontal(cols)).sort(by=\"sum_cols\", descending=True)[0][id_col].to_numpy()\n",
    "            y_plot = Y_nw.filter(nw.col(id_col) == idx_top)[target_col].to_numpy()\n",
    "            plt.plot(horizon_dates, y_plot, label='True')\n",
    "\n",
    "        ys = []\n",
    "        for tag in self.tags:\n",
    "            y_plot = sum([Y_nw.filter(nw.col(id_col) == idx)[cols].to_numpy() \\\n",
    "                          for idx in self.tags[tag]])\n",
    "            plt.plot(horizon_dates, y_plot, label=f'Level: {tag}')\n",
    "            \n",
    "            ys.append(y_plot[:,None])\n",
    "\n",
    "        plt.title('Predictions Accumulated Difference')\n",
    "        if ylabel is not None:\n",
    "            plt.ylabel(ylabel)\n",
    "        if xlabel is not None:\n",
    "            plt.xlabel(xlabel)\n",
    "\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27c4cff-ebbe-41f9-920b-7bbc997d0adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(HierarchicalPlot, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d1872c-7979-44f7-972b-2031729b04b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(HierarchicalPlot.plot_summing_matrix, \n",
    "         name='plot_summing_matrix', title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920de36f-e7fe-4ea4-81bb-d0f897e1f469",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(HierarchicalPlot.plot_series, \n",
    "         name='plot_series', title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa265cd-0c05-4617-a40a-f2c62513f7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(HierarchicalPlot.plot_hierarchically_linked_series, \n",
    "         name='plot_hierarchically_linked_series', title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8621cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(HierarchicalPlot.plot_hierarchical_predictions_gap,\n",
    "         name='plot_hierarchical_predictions_gap', title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662bdbce-cb13-4ba2-a794-1cd1bc3b96a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "hplots = HierarchicalPlot(S=S_df, tags=tags)\n",
    "hplots.plot_summing_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64215ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# polars\n",
    "S_df_f = pl.from_pandas(S_df.reset_index())\n",
    "hplots_f = HierarchicalPlot(S=S_df_f, tags=tags)\n",
    "hplots_f.plot_summing_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdaa2909-6344-4b6c-a4d9-6d4f903ce408",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "hier_df['Model'] = hier_df['y'] * 1.1\n",
    "hier_df['Model-lo-80'] = hier_df['Model'] - 0.1 * hier_df['Model']\n",
    "hier_df['Model-hi-80'] = hier_df['Model'] + 0.1 * hier_df['Model']\n",
    "hier_df['Model-lo-90'] = hier_df['Model'] - 0.2 * hier_df['Model']\n",
    "hier_df['Model-hi-90'] = hier_df['Model'] + 0.2 * hier_df['Model']\n",
    "hplots.plot_series(\n",
    "    series='Australia', \n",
    "    Y_df=hier_df,\n",
    "    level=[80, 90]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f89faa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# polars\n",
    "hier_df_f = pl.from_pandas(hier_df)\n",
    "\n",
    "hplots_f.plot_series(\n",
    "    series='Australia', \n",
    "    Y_df=hier_df_f,\n",
    "    level=[80, 90]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88068d1a-b670-410a-975e-a92e22ea9948",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "hplots.plot_series(series='Australia', \n",
    "                   Y_df=hier_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b90ed3-e1c3-47da-850b-ccda3319f630",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "hplots.plot_hierarchically_linked_series(\n",
    "    bottom_series='Australia/Western Australia/Experience Perth/Visiting', \n",
    "    Y_df=hier_df,\n",
    "    level=[80, 90]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e9be6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# polars\n",
    "hplots.plot_hierarchically_linked_series(\n",
    "    bottom_series='Australia/Western Australia/Experience Perth/Visiting', \n",
    "    Y_df=hier_df_f,\n",
    "    level=[80, 90]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45855eb-e800-40db-a00b-5ddb956ae348",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "hplots.plot_hierarchically_linked_series(\n",
    "    bottom_series='Australia/Western Australia/Experience Perth/Visiting', \n",
    "    Y_df=hier_df,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c9263e-6d07-4527-88ea-40153435f44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test series with just one value\n",
    "hplots.plot_hierarchically_linked_series(\n",
    "    bottom_series='Australia/Western Australia/Experience Perth/Visiting', \n",
    "    Y_df=hier_df.groupby('unique_id').tail(1),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69df1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# polars\n",
    "hplots_f.plot_hierarchically_linked_series(\n",
    "    bottom_series='Australia/Western Australia/Experience Perth/Visiting', \n",
    "    Y_df=pl.from_pandas(hier_df.groupby('unique_id').tail(1)),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7cc841-73db-4e00-99e4-b123b9d09db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "hplots.plot_hierarchical_predictions_gap(Y_df=hier_df.drop(columns='y'), models=['Model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f670a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# polars\n",
    "hplots_f.plot_hierarchical_predictions_gap(Y_df=pl.from_pandas(hier_df.drop(columns='y')), models=['Model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c2b84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "from statsforecast.core import StatsForecast\n",
    "from statsforecast.models import AutoETS\n",
    "from datasetsforecast.hierarchical import HierarchicalData\n",
    "\n",
    "Y_df, S, tags = HierarchicalData.load('./data', 'Labour')\n",
    "Y_df['ds'] = pd.to_datetime(Y_df['ds'])\n",
    "S = S.reset_index(names=\"unique_id\")\n",
    "\n",
    "Y_test_df  = Y_df.groupby('unique_id').tail(24)\n",
    "Y_train_df = Y_df.drop(Y_test_df.index)\n",
    "\n",
    "fcst = StatsForecast( \n",
    "    models=[AutoETS(season_length=12, model='AAZ')],\n",
    "    freq='MS', \n",
    "    n_jobs=-1\n",
    ")\n",
    "Y_hat_df = fcst.forecast(df=Y_train_df, h=24).reset_index()\n",
    "\n",
    "# Plot prediction difference of different aggregation\n",
    "# Levels Country, Country/Region, Country/Gender/Region ...\n",
    "hplots = HierarchicalPlot(S=S, tags=tags)\n",
    "\n",
    "hplots.plot_hierarchical_predictions_gap(\n",
    "    Y_df=Y_hat_df, models='AutoETS',\n",
    "    xlabel='Month', ylabel='Predictions',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83be7f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "# polars\n",
    "from statsforecast.core import StatsForecast\n",
    "from statsforecast.models import AutoETS\n",
    "from datasetsforecast.hierarchical import HierarchicalData\n",
    "\n",
    "Y_df, S, tags = HierarchicalData.load('./data', 'Labour')\n",
    "Y_df['ds'] = pd.to_datetime(Y_df['ds'])\n",
    "S = S.reset_index(names=\"unique_id\")\n",
    "\n",
    "Y_test_df  = Y_df.groupby('unique_id').tail(24)\n",
    "Y_train_df = Y_df.drop(Y_test_df.index)\n",
    "Y_test_df_pl  = pl.from_pandas(Y_test_df)\n",
    "Y_train_df_pl = pl.from_pandas(Y_train_df)\n",
    "\n",
    "fcst = StatsForecast(\n",
    "    models=[AutoETS(season_length=12, model='AAZ')],\n",
    "    freq='1m', \n",
    "    n_jobs=-1\n",
    ")\n",
    "Y_hat_df = fcst.forecast(df=Y_train_df_pl, h=24)\n",
    "\n",
    "# Plot prediction difference of different aggregation\n",
    "# Levels Country, Country/Region, Country/Gender/Region ...\n",
    "hplots = HierarchicalPlot(S=S, tags=tags)\n",
    "\n",
    "hplots.plot_hierarchical_predictions_gap(\n",
    "    Y_df=Y_hat_df, models='AutoETS',\n",
    "    xlabel='Month', ylabel='Predictions',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fcfbc2",
   "metadata": {},
   "source": [
    "# External Forecast Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c629ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "# convert levels to output quantile names\n",
    "def level_to_outputs(level: list[int]) -> tuple[list[float], list[str]]:\n",
    "    \"\"\" Converts list of levels into output names matching StatsForecast and NeuralForecast methods.\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `level`: int list [0,100]. Probability levels for prediction intervals.<br>\n",
    "\n",
    "    **Returns:**<br>\n",
    "    `output_names`: str list. String list with output column names.\n",
    "    \"\"\"\n",
    "    qs = sum([[50-l/2, 50+l/2] for l in level], [])\n",
    "    output_names = sum([[f'-lo-{l}', f'-hi-{l}'] for l in level], [])\n",
    "\n",
    "    sort_idx = np.argsort(qs)\n",
    "    quantiles = np.array(qs)[sort_idx]\n",
    "\n",
    "    # Add default median\n",
    "    quantiles = np.concatenate([np.array([50]), quantiles]) / 100\n",
    "    output_names = list(np.array(output_names)[sort_idx])\n",
    "    output_names.insert(0, '-median')\n",
    "    \n",
    "    return quantiles, output_names\n",
    "\n",
    "# convert quantiles to output quantile names\n",
    "def quantiles_to_outputs(quantiles: list[float]) -> tuple[list[float], list[str]]:\n",
    "    \"\"\"Converts list of quantiles into output names matching StatsForecast and NeuralForecast methods.\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `quantiles`: float list [0., 1.]. Alternative to level, quantiles to estimate from y distribution.<br>\n",
    "\n",
    "    **Returns:**<br>\n",
    "    `output_names`: str list. String list with output column names.\n",
    "    \"\"\"\n",
    "    output_names = []\n",
    "    for q in quantiles:\n",
    "        if q<.50:\n",
    "            output_names.append(f'-lo-{np.round(100-200*q,2)}')\n",
    "        elif q>.50:\n",
    "            output_names.append(f'-hi-{np.round(100-200*(1-q),2)}')\n",
    "        else:\n",
    "            output_names.append('-median')\n",
    "    return quantiles, output_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b52fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "# given input array of sample forecasts and inptut quantiles/levels, \n",
    "# output a Pandas Dataframe with columns of quantile predictions\n",
    "def samples_to_quantiles_df(samples: np.ndarray, \n",
    "                            unique_ids: Sequence[str], \n",
    "                            dates: list[str], \n",
    "                            quantiles: Optional[list[float]] = None,\n",
    "                            level: Optional[list[int]] = None, \n",
    "                            model_name: str = \"model\",\n",
    "                            id_col: str = 'unique_id',\n",
    "                            time_col: str = 'ds',\n",
    "                            backend: str = 'pandas',\n",
    "                            ) -> tuple[list[float], FrameT]:\n",
    "    \"\"\" Transform Random Samples into HierarchicalForecast input.\n",
    "    Auxiliary function to create compatible HierarchicalForecast input `Y_hat_df` dataframe.\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `samples`: numpy array. Samples from forecast distribution of shape [n_series, n_samples, horizon].<br>\n",
    "    `unique_ids`: string list. Unique identifiers for each time series.<br>\n",
    "    `dates`: datetime list. list of forecast dates.<br>\n",
    "    `quantiles`: float list in [0., 1.]. Alternative to level, quantiles to estimate from y distribution.<br>\n",
    "    `level`: int list in [0,100]. Probability levels for prediction intervals.<br>\n",
    "    `model_name`: string. Name of forecasting model.<br>\n",
    "    `id_col` : str='unique_id', column that identifies each serie.<br>\n",
    "    `time_col` : str='ds', column that identifies each timestep, its values can be timestamps or integers.<br>\n",
    "    `backend` : str='pandas', backend to use for the output dataframe, either 'pandas' or 'polars'.<br>\n",
    "\n",
    "    **Returns:**<br>\n",
    "    `quantiles`: float list in [0., 1.]. quantiles to estimate from y distribution .<br>\n",
    "    `Y_hat_df`: DataFrame. With base quantile forecasts with columns ds and models to reconcile indexed by unique_id.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the shape of the array\n",
    "    n_series, n_samples, horizon = samples.shape\n",
    "\n",
    "    if n_series != len(unique_ids):\n",
    "        raise ValueError(\n",
    "            f\"Number of unique_ids ({len(unique_ids)}) must match the number of series ({n_series}).\"\n",
    "        )\n",
    "    if horizon != len(dates):\n",
    "        raise ValueError(\n",
    "            f\"Number of dates ({len(dates)}) must match third dimension of samples array ({horizon}).\"\n",
    "        )\n",
    "    if not ((quantiles is None) ^ (level is None)):\n",
    "        raise ValueError(\"Either quantiles or level must be provided, but not both.\")\n",
    "\n",
    "    namespace = sys.modules.get(backend, None)\n",
    "    if namespace is None:\n",
    "        raise ValueError(f\"DataFrame backend {backend} not installed.\")\n",
    "\n",
    "    #create initial dictionary\n",
    "    forecasts_mean = np.mean(samples, axis=1).flatten()\n",
    "    unique_ids = np.repeat(unique_ids, horizon)\n",
    "    ds = np.tile(dates, n_series)\n",
    "\n",
    "    #create quantiles and quantile names\n",
    "    if level is not None:\n",
    "        _quantiles, quantile_names = level_to_outputs(level)\n",
    "    elif quantiles is not None:\n",
    "        _quantiles, quantile_names = quantiles_to_outputs(quantiles)\n",
    "\n",
    "    percentiles = [quantile * 100 for quantile in _quantiles]\n",
    "    col_names = np.array([model_name + quantile_name for quantile_name in quantile_names])\n",
    "    \n",
    "    #add quantiles to dataframe\n",
    "    forecasts_quantiles = np.percentile(samples, percentiles, axis=1)\n",
    "\n",
    "    forecasts_quantiles = np.transpose(forecasts_quantiles, (1,2,0)) # [Q,H,N] -> [N,H,Q]\n",
    "    forecasts_quantiles = forecasts_quantiles.reshape(-1,len(_quantiles))\n",
    "    \n",
    "    df_nw = nw.from_dict(\n",
    "        {\n",
    "            **{id_col: unique_ids, time_col: ds, model_name: forecasts_mean},\n",
    "            **dict(zip(col_names, forecasts_quantiles.T)),\n",
    "        },\n",
    "        native_namespace=namespace,\n",
    "    )\n",
    "\n",
    "    return _quantiles, df_nw.to_native()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e4fcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(samples_to_quantiles_df, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500ad055",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "#level_to_outputs unit tests\n",
    "test_eq(\n",
    "    level_to_outputs([80, 90]),\n",
    "    ([0.5 , 0.05, 0.1 , 0.9 , 0.95], ['-median', '-lo-90', '-lo-80', '-hi-80', '-hi-90'])\n",
    ")\n",
    "test_eq(\n",
    "    level_to_outputs([30]),\n",
    "    ([0.5 , 0.35, 0.65], ['-median', '-lo-30', '-hi-30'])\n",
    ")\n",
    "#quantiles_to_outputs unit tests\n",
    "test_eq(\n",
    "    quantiles_to_outputs([0.2, 0.4, 0.6, 0.8]),\n",
    "    ([0.2,0.4,0.6, 0.8], ['-lo-60.0', '-lo-20.0', '-hi-20.0', '-hi-60.0'])\n",
    ")\n",
    "test_eq(\n",
    "    quantiles_to_outputs([0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]),\n",
    "    ([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9], \n",
    "     ['-lo-80.0', '-lo-60.0', '-lo-40.0', '-lo-20.0', '-median', '-hi-20.0', '-hi-40.0', '-hi-60.0', '-hi-80.0'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f4f2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "#samples_to_quantiles_df unit tests\n",
    "start_date = pd.Timestamp(\"2023-06-01\")\n",
    "end_date = pd.Timestamp(\"2023-06-10\")\n",
    "frequency = \"D\"  # Daily frequency\n",
    "dates = pd.date_range(start=start_date, end=end_date, freq=frequency).tolist()\n",
    "samples = np.random.rand(3, 200, 10)\n",
    "unique_ids = ['id1', 'id2', 'id3']\n",
    "level = np.array([10, 50, 90])\n",
    "quantiles=np.array([0.5, 0.05, 0.25, 0.45, 0.55, 0.75, 0.95])\n",
    "\n",
    "ret_quantiles_1, ret_df_1 = samples_to_quantiles_df(samples, unique_ids, dates, level=level)\n",
    "ret_quantiles_2, ret_df_2 = samples_to_quantiles_df(samples, unique_ids, dates, quantiles=quantiles)\n",
    "\n",
    "test_eq(\n",
    "    ret_quantiles_1,\n",
    "    quantiles\n",
    ")\n",
    "test_eq(\n",
    "    ret_df_1.columns,\n",
    "    ['unique_id', 'ds', 'model', 'model-median', 'model-lo-90', 'model-lo-50', 'model-lo-10', 'model-hi-10', 'model-hi-50', 'model-hi-90']\n",
    ")\n",
    "test_eq(\n",
    "    ret_df_1[\"unique_id\"].values,\n",
    "    ['id1', 'id1', 'id1', 'id1', 'id1', 'id1', 'id1', 'id1', 'id1', 'id1',\n",
    "       'id2', 'id2', 'id2', 'id2', 'id2', 'id2', 'id2', 'id2', 'id2', 'id2',\n",
    "       'id3', 'id3', 'id3', 'id3', 'id3', 'id3', 'id3', 'id3', 'id3', 'id3']\n",
    ")\n",
    "test_eq(\n",
    "    ret_quantiles_1, ret_quantiles_2\n",
    ")\n",
    "test_eq(\n",
    "    ret_df_1[\"unique_id\"], ret_df_2[\"unique_id\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6eeb27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# polars\n",
    "\n",
    "ret_quantiles_1, ret_df_1 = samples_to_quantiles_df(samples, unique_ids, dates, level=level, backend='polars')\n",
    "ret_quantiles_2, ret_df_2 = samples_to_quantiles_df(samples, unique_ids, dates, quantiles=quantiles, backend='polars')\n",
    "\n",
    "test_eq(\n",
    "    ret_quantiles_1,\n",
    "    quantiles\n",
    ")\n",
    "test_eq(\n",
    "    ret_df_1.columns,\n",
    "    ['unique_id', 'ds', 'model', 'model-median', 'model-lo-90', 'model-lo-50', 'model-lo-10', 'model-hi-10', 'model-hi-50', 'model-hi-90']\n",
    ")\n",
    "test_eq(\n",
    "    list(ret_df_1[\"unique_id\"]),\n",
    "    ['id1', 'id1', 'id1', 'id1', 'id1', 'id1', 'id1', 'id1', 'id1', 'id1',\n",
    "       'id2', 'id2', 'id2', 'id2', 'id2', 'id2', 'id2', 'id2', 'id2', 'id2',\n",
    "       'id3', 'id3', 'id3', 'id3', 'id3', 'id3', 'id3', 'id3', 'id3', 'id3']\n",
    ")\n",
    "test_eq(\n",
    "    ret_quantiles_1, ret_quantiles_2\n",
    ")\n",
    "test_eq(\n",
    "    ret_df_1[\"unique_id\"], ret_df_2[\"unique_id\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd3fec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "#Unit test for the aggregate function accounting for exog_vars\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        'cat1': ['a', 'a', 'c'],\n",
    "        'cat2': ['1', '2', '3'],\n",
    "        'exog1': [4, 5, 6],\n",
    "        'exog2': [7, 6, 5],\n",
    "        'y': [10, 20, 30],\n",
    "        'ds': ['2020-01-01', '2020-02-01', '2020-03-01']\n",
    "    }\n",
    ")\n",
    "spec = [[\"cat1\"],[\"cat1\",\"cat2\"]]\n",
    "\n",
    "\n",
    "Y_df_check = pd.DataFrame(\n",
    "    data={\n",
    "        'unique_id': ['a', 'a', 'c', 'a/1', 'a/2', 'c/3'],\n",
    "        'ds': ['2020-01-01','2020-02-01','2020-03-01','2020-01-01','2020-02-01','2020-03-01'],\n",
    "        'y': [10, 20, 30, 10, 20, 30],\n",
    "    },\n",
    ")\n",
    "\n",
    "S_df_check = pd.DataFrame(\n",
    "    data={\n",
    "        'unique_id': ['a','c','a/1','a/2','c/3'],\n",
    "        'a/1': np.array([1.0, 0.0, 1.0, 0.0, 0.0], dtype=np.float64),\n",
    "        'a/2': np.array([1.0, 0.0, 0.0, 1.0, 0.0], dtype=np.float64),\n",
    "        'c/3': np.array([0.0, 1.0, 0.0, 0.0, 1.0], dtype=np.float64)\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "Y_df_check_exog = pd.DataFrame(\n",
    "    data = {\n",
    "        'unique_id': ['a', 'a', 'c', 'a/1', 'a/2', 'c/3'],\n",
    "        'ds': ['2020-01-01', '2020-02-01', '2020-03-01', '2020-01-01', '2020-02-01', '2020-03-01'],\n",
    "        'y': [10, 20, 30, 10, 20, 30],\n",
    "        'exog1_mean': [4.0, 5.0, 6.0, 4.0, 5.0, 6.0],\n",
    "        'exog2_sum': [7, 6, 5, 7, 6, 5]\n",
    "    },\n",
    ")\n",
    "\n",
    "Y_df, S_df, tags = aggregate(\n",
    "    df = df,\n",
    "    spec = spec,\n",
    "    exog_vars = None,\n",
    ")\n",
    "\n",
    "Y_df_exog, S_df_exog, tags = aggregate(\n",
    "    df = df,\n",
    "    spec = spec,\n",
    "    exog_vars = {'exog1':'mean','exog2':'sum'},\n",
    ")\n",
    "\n",
    "test_eq(Y_df, \n",
    "        Y_df_check)\n",
    "\n",
    "test_eq(S_df, \n",
    "        S_df_check)\n",
    "\n",
    "test_eq(Y_df_exog,\n",
    "        Y_df_check_exog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4268070d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# polars\n",
    "\n",
    "df_f = pl.from_pandas(df)\n",
    "Y_df_check_f = pl.from_pandas(Y_df_check)\n",
    "S_df_check_f = pl.from_pandas(S_df_check)\n",
    "Y_df_check_exog_f = pl.from_pandas(Y_df_check_exog)\n",
    "\n",
    "Y_df, S_df, tags = aggregate(\n",
    "    df = df_f,\n",
    "    spec = spec,\n",
    "    exog_vars = None,\n",
    ")\n",
    "\n",
    "Y_df_exog, S_df_exog, tags = aggregate(\n",
    "    df = df_f,\n",
    "    spec = spec,\n",
    "    exog_vars = {'exog1':'mean','exog2':'sum'},\n",
    ")\n",
    "\n",
    "test_eq(Y_df, \n",
    "        Y_df_check_f)\n",
    "\n",
    "test_eq(S_df, \n",
    "        S_df_check_f)\n",
    "\n",
    "test_eq(Y_df_exog,\n",
    "        Y_df_check_exog_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec9c7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "# Masked empirical covariance matrix\n",
    "@njit(\"Array(float64, 2, 'F')(Array(float64, 2, 'C'), Array(bool_, 2, 'C'))\", nogil=NUMBA_NOGIL, cache=NUMBA_CACHE, parallel=NUMBA_PARALLEL, fastmath=NUMBA_FASTMATH, error_model=\"numpy\")\n",
    "def _ma_cov(residuals: np.ndarray, not_nan_mask: np.ndarray):\n",
    "    \"\"\"Masked empirical covariance matrix.\n",
    "\n",
    "    :meta private:\n",
    "    \"\"\"\n",
    "    n_timeseries = residuals.shape[0]\n",
    "    W = np.zeros((n_timeseries, n_timeseries), dtype=np.float64).T\n",
    "    for i in prange(n_timeseries):\n",
    "        not_nan_mask_i = not_nan_mask[i]\n",
    "        for j in range(i + 1):\n",
    "            not_nan_mask_j = not_nan_mask[j]\n",
    "            not_nan_mask_ij = not_nan_mask_i & not_nan_mask_j   \n",
    "            n_samples = np.sum(not_nan_mask_ij)\n",
    "            # Only compute if we have enough non-nan samples in the time series pair\n",
    "            if n_samples > 1:\n",
    "                # Masked residuals\n",
    "                residuals_i = residuals[i][not_nan_mask_ij]\n",
    "                residuals_j = residuals[j][not_nan_mask_ij]\n",
    "                residuals_i_mean = np.mean(residuals_i)\n",
    "                residuals_j_mean = np.mean(residuals_j)\n",
    "                X_i = (residuals_i - residuals_i_mean)\n",
    "                X_j = (residuals_j - residuals_j_mean)\n",
    "                # Empirical covariance\n",
    "                factor_emp_cov = np.float64(1 / (n_samples - 1))\n",
    "                W[i, j] = W[j, i] = factor_emp_cov * np.sum(X_i * X_j)\n",
    "\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72493f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "# Shrunk covariance matrix using the Schafer-Strimmer method\n",
    "\n",
    "@njit(\"Array(float64, 2, 'F')(Array(float64, 2, 'C'), float64)\", nogil=NUMBA_NOGIL, cache=NUMBA_CACHE, parallel=NUMBA_PARALLEL, fastmath=NUMBA_FASTMATH, error_model=\"numpy\")\n",
    "def _shrunk_covariance_schaferstrimmer_no_nans(residuals: np.ndarray, mint_shr_ridge: float):\n",
    "    \"\"\"Shrink empirical covariance according to the following method:\n",
    "        Schfer, Juliane, and Korbinian Strimmer. \n",
    "        A Shrinkage Approach to Large-Scale Covariance Matrix Estimation and \n",
    "        Implications for Functional Genomics. Statistical Applications in \n",
    "        Genetics and Molecular Biology 4, no. 1 (14 January 2005). \n",
    "        https://doi.org/10.2202/1544-6115.1175.\n",
    "\n",
    "    :meta private:\n",
    "    \"\"\"\n",
    "    n_timeseries = residuals.shape[0]\n",
    "    n_samples = residuals.shape[1]\n",
    "    \n",
    "    # We need the empirical covariance, the off-diagonal sum of the variance of \n",
    "    # the empirical correlation matrix and the off-diagonal sum of the squared \n",
    "    # empirical correlation matrix.\n",
    "    W = np.zeros((n_timeseries, n_timeseries), dtype=np.float64).T\n",
    "    sum_var_emp_corr = np.float64(0.0)\n",
    "    sum_sq_emp_corr = np.float64(0.0)\n",
    "    factor_emp_cov = np.float64(1 / (n_samples - 1))\n",
    "    factor_shrinkage = np.float64(1 / (n_samples * (n_samples - 1)))\n",
    "    epsilon = np.float64(2e-8)\n",
    "    for i in prange(n_timeseries):\n",
    "        # Mean of the standardized residuals\n",
    "        X_i = residuals[i] - np.mean(residuals[i])\n",
    "        Xs_i = X_i / (np.std(residuals[i]) + epsilon)\n",
    "        Xs_i_mean = np.mean(Xs_i)\n",
    "        for j in range(i + 1):\n",
    "            # Empirical covariance\n",
    "            X_j = residuals[j] - np.mean(residuals[j])\n",
    "            W[i, j] = factor_emp_cov * np.sum(X_i * X_j)\n",
    "            # Off-diagonal sums\n",
    "            if i != j:\n",
    "                Xs_j = X_j / (np.std(residuals[j]) + epsilon)\n",
    "                Xs_j_mean = np.mean(Xs_j)\n",
    "                # Sum off-diagonal variance of empirical correlation\n",
    "                w = (Xs_i - Xs_i_mean) * (Xs_j - Xs_j_mean)\n",
    "                w_mean = np.mean(w)\n",
    "                sum_var_emp_corr += np.sum(np.square(w - w_mean))\n",
    "                # Sum squared empirical correlation\n",
    "                sum_sq_emp_corr += w_mean**2\n",
    "\n",
    "    # Calculate shrinkage intensity \n",
    "    shrinkage = 1.0 - max(min((factor_shrinkage * sum_var_emp_corr) / (sum_sq_emp_corr + epsilon), 1.0), 0.0)\n",
    "    # Shrink the empirical covariance\n",
    "    for i in prange(n_timeseries):\n",
    "        for j in range(i + 1):\n",
    "            if i != j:    \n",
    "                W[i, j] = W[j, i] = shrinkage * W[i, j]\n",
    "            else:\n",
    "                W[i, j] = W[j, i] = max(W[i, j], mint_shr_ridge)\n",
    "    return W\n",
    "\n",
    "@njit(\"Array(float64, 2, 'F')(Array(float64, 2, 'C'), Array(bool_, 2, 'C'), float64)\", nogil=NUMBA_NOGIL, cache=NUMBA_CACHE, parallel=NUMBA_PARALLEL, fastmath=NUMBA_FASTMATH, error_model=\"numpy\")\n",
    "def _shrunk_covariance_schaferstrimmer_with_nans(residuals: np.ndarray, not_nan_mask: np.ndarray, mint_shr_ridge: float):\n",
    "    \"\"\"Shrink empirical covariance according to the following method:\n",
    "        Schfer, Juliane, and Korbinian Strimmer. \n",
    "        A Shrinkage Approach to Large-Scale Covariance Matrix Estimation and \n",
    "        Implications for Functional Genomics. Statistical Applications in \n",
    "        Genetics and Molecular Biology 4, no. 1 (14 January 2005). \n",
    "        https://doi.org/10.2202/1544-6115.1175.\n",
    "\n",
    "    :meta private:\n",
    "    \"\"\"\n",
    "    n_timeseries = residuals.shape[0]\n",
    "    \n",
    "    # We need the empirical covariance, the off-diagonal sum of the variance of \n",
    "    # the empirical correlation matrix and the off-diagonal sum of the squared \n",
    "    # empirical correlation matrix.\n",
    "    W = np.zeros((n_timeseries, n_timeseries), dtype=np.float64).T\n",
    "    sum_var_emp_corr = np.float64(0.0)\n",
    "    sum_sq_emp_corr = np.float64(0.0)\n",
    "    epsilon = np.float64(2e-8)\n",
    "    for i in prange(n_timeseries):\n",
    "        not_nan_mask_i = not_nan_mask[i]\n",
    "        for j in range(i + 1):\n",
    "            not_nan_mask_j = not_nan_mask[j]\n",
    "            not_nan_mask_ij = not_nan_mask_i & not_nan_mask_j   \n",
    "            n_samples = np.sum(not_nan_mask_ij)\n",
    "            # Only compute if we have enough non-nan samples in the time series pair\n",
    "            if n_samples > 1:\n",
    "                # Masked residuals\n",
    "                residuals_i = residuals[i][not_nan_mask_ij]\n",
    "                residuals_j = residuals[j][not_nan_mask_ij]\n",
    "                residuals_i_mean = np.mean(residuals_i)\n",
    "                residuals_j_mean = np.mean(residuals_j)\n",
    "                X_i = (residuals_i - residuals_i_mean)\n",
    "                X_j = (residuals_j - residuals_j_mean)\n",
    "                # Empirical covariance\n",
    "                factor_emp_cov = np.float64(1 / (n_samples - 1))\n",
    "                W[i, j] = factor_emp_cov * np.sum(X_i * X_j)\n",
    "                # Off-diagonal sums\n",
    "                if i != j:\n",
    "                    factor_var_emp_cor = np.float64(n_samples / (n_samples - 1)**3)\n",
    "                    residuals_i_std = np.std(residuals_i) + epsilon\n",
    "                    residuals_j_std = np.std(residuals_j) + epsilon\n",
    "                    Xs_i = X_i / (residuals_i_std + epsilon)\n",
    "                    Xs_j = X_j / (residuals_j_std + epsilon)\n",
    "                    Xs_im_mean = np.mean(Xs_i)\n",
    "                    Xs_jm_mean = np.mean(Xs_j)\n",
    "                    # Sum off-diagonal variance of empirical correlation\n",
    "                    w = (Xs_i - Xs_im_mean) * (Xs_j - Xs_jm_mean)\n",
    "                    w_mean = np.mean(w)\n",
    "                    sum_var_emp_corr += factor_var_emp_cor * np.sum(np.square(w - w_mean))\n",
    "                    # Sum squared empirical correlation\n",
    "                    sum_sq_emp_corr += np.square(factor_emp_cov * n_samples * w_mean)\n",
    "\n",
    "    # Calculate shrinkage intensity \n",
    "    shrinkage = 1.0 - max(min((sum_var_emp_corr) / (sum_sq_emp_corr + epsilon), 1.0), 0.0)\n",
    "\n",
    "    # Shrink the empirical covariance\n",
    "    for i in prange(n_timeseries):\n",
    "        for j in range(i + 1):\n",
    "            if i != j:    \n",
    "                W[i, j] = W[j, i] = shrinkage * W[i, j]\n",
    "            else:\n",
    "                W[i, j] = W[j, i] = max(W[i, j], mint_shr_ridge)\n",
    "\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce27bfe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test covariance equivalence\n",
    "n_samples = 100\n",
    "n_hiers = 10\n",
    "y_insample = np.random.rand(n_samples, n_hiers)\n",
    "y_hat_insample = np.random.rand(n_samples, n_hiers)\n",
    "residuals = (y_insample - y_hat_insample)\n",
    "nan_mask = np.isnan(residuals)\n",
    "\n",
    "# Check equivalence of covariance functions in case of no nans\n",
    "W_nb = _ma_cov(residuals, ~nan_mask)\n",
    "W_np = np.cov(residuals)\n",
    "np.testing.assert_allclose(W_nb, W_np, atol=1e-6)\n",
    "\n",
    "# Check equivalence of shrunk covariance functions in case of no nans\n",
    "W_ss_nonan = _shrunk_covariance_schaferstrimmer_no_nans(residuals, 2e-8)\n",
    "W_ss_nan = _shrunk_covariance_schaferstrimmer_with_nans(residuals, ~nan_mask, 2e-8)\n",
    "np.testing.assert_allclose(W_ss_nan, W_ss_nonan, atol=1e-6)\n",
    "\n",
    "# Check equivalence of diagonal elements of shrunk W to non-shrunk W in case of no nans\n",
    "np.testing.assert_allclose(np.diag(W_np), np.diag(W_ss_nan), atol=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9859f019",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "# Lasso cyclic coordinate descent\n",
    "@njit(\"Array(float64, 1, 'C')(Array(float64, 2, 'C'), Array(float64, 1, 'C'), float64, int64, float64)\", nogil=NUMBA_NOGIL, cache=NUMBA_CACHE, fastmath=NUMBA_FASTMATH, error_model=\"numpy\")\n",
    "def _lasso(X: np.ndarray, y: np.ndarray, \n",
    "          lambda_reg: float, max_iters: int = 1_000,\n",
    "          tol: float = 1e-4):\n",
    "    # lasso cyclic coordinate descent\n",
    "    n, feats = X.shape\n",
    "    norms = np.sum(X ** 2, axis=0)\n",
    "    beta = np.zeros(feats, dtype=np.float64)\n",
    "    beta_changes = np.zeros(feats, dtype=np.float64)\n",
    "    residuals = y.copy()\n",
    "\n",
    "    for it in range(max_iters):\n",
    "        for i in range(feats):            \n",
    "            norms_i = norms[i]\n",
    "            # is feature is close to zero, we \n",
    "            # continue to the next.\n",
    "            # in this case is optimal betai= 0\n",
    "            if abs(norms_i) < 1e-8:\n",
    "                continue\n",
    "            beta_i = beta[i]\n",
    "\n",
    "            #we calculate the normalized derivative\n",
    "            rho = beta_i\n",
    "            for j in range(n):\n",
    "                rho += X[j, i] * residuals[j] / norms_i\n",
    "\n",
    "            #soft threshold\n",
    "            beta_i_next = np.sign(rho) * max(np.abs(rho) - lambda_reg * n / norms_i, 0.)#(norms[i] + 1e-3), 0.)\n",
    "            beta_delta = beta_i - beta_i_next\n",
    "            beta_changes[i] = np.abs(beta_delta)\n",
    "            if beta_delta != 0.0:\n",
    "                for j in range(n):\n",
    "                    residuals[j] += beta_delta * X[j, i]\n",
    "\n",
    "                beta[i] = beta_i_next\n",
    "        \n",
    "        if max(beta_changes) < tol:\n",
    "            break\n",
    "\n",
    "    return beta"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
