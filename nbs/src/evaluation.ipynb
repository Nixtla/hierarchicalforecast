{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To assist the evaluation of hierarchical forecasting systems, we make available accuracy metrics along with the `HierarchicalEvaluation` module that facilitates the measurement of prediction's accuracy through the hierarchy levels. \n",
    "\n",
    "The available metrics include point and probabilistic multivariate scoring rules that were used in previous hierarchical forecasting studies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import narwhals as nw\n",
    "import numpy as np\n",
    "\n",
    "from hierarchicalforecast.utils import _to_narwhals_maybe_warn_and_reset_idx, _to_native_maybe_set_index\n",
    "from inspect import signature\n",
    "from narwhals.typing import Frame\n",
    "from scipy.stats import multivariate_normal\n",
    "from typing import Callable, Dict, List, Optional, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from fastcore.test import test_close, test_fail\n",
    "from nbdev.showdoc import add_docs, show_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "os.environ['NIXTLA_ID_AS_COL'] = '1'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy Measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _metric_protections(y: np.ndarray, y_hat: np.ndarray, \n",
    "                        weights: Optional[np.ndarray]) -> None:\n",
    "    if not ((weights is None) or (np.sum(weights) > 0)):\n",
    "        raise Exception('Sum of `weights` cannot be 0')\n",
    "    if not ((weights is None) or (weights.shape == y.shape)):\n",
    "        raise Exception(\n",
    "        f'Wrong weight dimension weights.shape {weights.shape}, y.shape {y.shape}')\n",
    "\n",
    "def mse(y: np.ndarray, y_hat: np.ndarray, \n",
    "        weights: Optional[np.ndarray] = None,\n",
    "        axis: Optional[int] = None) -> Union[float, np.ndarray]:\n",
    "    \"\"\"Mean Squared Error\n",
    "\n",
    "    Calculates Mean Squared Error between\n",
    "    `y` and `y_hat`. MSE measures the relative prediction\n",
    "    accuracy of a forecasting method by calculating the \n",
    "    squared deviation of the prediction and the true\n",
    "    value at a given time, and averages these devations\n",
    "    over the length of the series.\n",
    "\n",
    "    $$ \\mathrm{MSE}(\\\\mathbf{y}_{\\\\tau}, \\\\mathbf{\\hat{y}}_{\\\\tau}) = \\\\frac{1}{H} \\\\sum^{t+H}_{\\\\tau=t+1} (y_{\\\\tau} - \\hat{y}_{\\\\tau})^{2} $$\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `y`: numpy array, Actual values.<br>\n",
    "    `y_hat`: numpy array, Predicted values.<br>\n",
    "    `mask`: numpy array, Specifies date stamps per serie to consider in loss.<br>\n",
    "\n",
    "    **Returns:**<br>\n",
    "    `mse`: numpy array, (single value).\n",
    "    \"\"\"\n",
    "    _metric_protections(y, y_hat, weights)\n",
    "\n",
    "    delta_y = np.square(y - y_hat)\n",
    "    if weights is not None:\n",
    "        mse = np.average(delta_y[~np.isnan(delta_y)],\n",
    "                         weights=weights[~np.isnan(delta_y)],\n",
    "                         axis=axis)\n",
    "    else:\n",
    "        mse = np.nanmean(delta_y, axis=axis)\n",
    "    return mse\n",
    "\n",
    "def mqloss(y: np.ndarray, y_hat: np.ndarray, \n",
    "           quantiles: np.ndarray, \n",
    "           weights: Optional[np.ndarray] = None,\n",
    "           axis: Optional[int] = None) -> Union[float, np.ndarray]:\n",
    "    \"\"\"Multi-Quantile Loss\n",
    "\n",
    "    Calculates the Multi-Quantile loss (MQL) between `y` and `y_hat`.\n",
    "    MQL calculates the average multi-quantile Loss for\n",
    "    a given set of quantiles, based on the absolute \n",
    "    difference between predicted quantiles and observed values.\n",
    "\n",
    "    $$ \\mathrm{MQL}(\\\\mathbf{y}_{\\\\tau},[\\\\mathbf{\\hat{y}}^{(q_{1})}_{\\\\tau}, ... ,\\hat{y}^{(q_{n})}_{\\\\tau}]) = \\\\frac{1}{n} \\\\sum_{q_{i}} \\mathrm{QL}(\\\\mathbf{y}_{\\\\tau}, \\\\mathbf{\\hat{y}}^{(q_{i})}_{\\\\tau}) $$\n",
    "\n",
    "    The limit behavior of MQL allows to measure the accuracy \n",
    "    of a full predictive distribution $\\mathbf{\\hat{F}}_{\\\\tau}$ with \n",
    "    the continuous ranked probability score (CRPS). This can be achieved \n",
    "    through a numerical integration technique, that discretizes the quantiles \n",
    "    and treats the CRPS integral with a left Riemann approximation, averaging over \n",
    "    uniformly distanced quantiles.    \n",
    "\n",
    "    $$ \\mathrm{CRPS}(y_{\\\\tau}, \\mathbf{\\hat{F}}_{\\\\tau}) = \\int^{1}_{0} \\mathrm{QL}(y_{\\\\tau}, \\hat{y}^{(q)}_{\\\\tau}) dq $$\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `y`: numpy array, Actual values.<br>\n",
    "    `y_hat`: numpy array, Predicted values.<br>\n",
    "    `quantiles`: numpy array. Quantiles between 0 and 1, to perform evaluation upon size (n_quantiles).<br>\n",
    "    `mask`: numpy array, Specifies date stamps per serie to consider in loss.<br>\n",
    " \n",
    "    **Returns:**<br>\n",
    "    `mqloss`: numpy array, (single value).\n",
    "\n",
    "    **References:**<br>\n",
    "    [Roger Koenker and Gilbert Bassett, Jr., \"Regression Quantiles\".](https://www.jstor.org/stable/1913643)<br>\n",
    "    [James E. Matheson and Robert L. Winkler, \"Scoring Rules for Continuous Probability Distributions\".](https://www.jstor.org/stable/2629907)\n",
    "    \"\"\"\n",
    "    if weights is None: weights = np.ones(y.shape)\n",
    "    if (np.sum(quantiles>1)>0 or np.sum(quantiles<0)>0):\n",
    "        raise Exception('`quantiles` need to be between 0 and 1')\n",
    "        \n",
    "    _metric_protections(y, y_hat, weights)\n",
    "    n_q = len(quantiles)\n",
    "    \n",
    "    y_rep  = np.expand_dims(y, axis=-1)\n",
    "    error  = y_hat - y_rep\n",
    "    sq     = np.maximum(-error, np.zeros_like(error))\n",
    "    s1_q   = np.maximum(error, np.zeros_like(error))\n",
    "    mqloss = (quantiles * sq + (1 - quantiles) * s1_q)\n",
    "    \n",
    "    # Match y/weights dimensions and compute weighted average\n",
    "    weights = np.repeat(np.expand_dims(weights, axis=-1), repeats=n_q, axis=-1)\n",
    "    mqloss  = np.average(mqloss, weights=weights, axis=axis)\n",
    "\n",
    "    return mqloss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relative Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def rel_mse(y, y_hat, y_train, mask=None):\n",
    "    \"\"\"Relative Mean Squared Error\n",
    "\n",
    "    Computes Relative mean squared error (RelMSE), as proposed by Hyndman & Koehler (2006)\n",
    "    as an alternative to percentage errors, to avoid measure unstability.\n",
    "\n",
    "    $$\n",
    "    \\mathrm{RelMSE}(\\\\mathbf{y}, \\\\mathbf{\\hat{y}}, \\\\mathbf{\\hat{y}}^{naive1}) =\n",
    "    \\\\frac{\\mathrm{MSE}(\\\\mathbf{y}, \\\\mathbf{\\hat{y}})}{\\mathrm{MSE}(\\\\mathbf{y}, \\\\mathbf{\\hat{y}}^{naive1})}\n",
    "    $$\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `y`: numpy array, Actual values of size (`n_series`, `horizon`).<br>\n",
    "    `y_hat`: numpy array, Predicted values (`n_series`, `horizon`).<br>\n",
    "    `mask`: numpy array, Specifies date stamps per serie to consider in loss.<br>\n",
    "\n",
    "    **Returns:**<br>\n",
    "    `loss`: float.    \n",
    "\n",
    "    **References:**<br>\n",
    "    - [Hyndman, R. J and Koehler, A. B. (2006).\n",
    "       \"Another look at measures of forecast accuracy\",\n",
    "       International Journal of Forecasting, Volume 22, Issue 4.](https://www.sciencedirect.com/science/article/pii/S0169207006000239)<br>\n",
    "    - [Kin G. Olivares, O. Nganba Meetei, Ruijun Ma, Rohan Reddy, Mengfei Cao, Lee Dicker. \n",
    "       \"Probabilistic Hierarchical Forecasting with Deep Poisson Mixtures. \n",
    "       Submitted to the International Journal Forecasting, Working paper available at arxiv.](https://arxiv.org/pdf/2110.13179.pdf)\n",
    "    \"\"\"\n",
    "    if mask is None: \n",
    "       mask = np.ones_like(y)\n",
    "    n_series, horizon = y.shape\n",
    "\n",
    "    eps = np.finfo(float).eps\n",
    "    y_naive = np.repeat(y_train[:,[-1]], horizon, axis=1)\n",
    "    norm = mse(y=y, y_hat=y_naive)\n",
    "    loss = mse(y=y, y_hat=y_hat, weights=mask)\n",
    "    loss = loss / (norm + eps)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/hierarchicalforecast/blob/main/hierarchicalforecast/evaluation.py#L127){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### rel_mse\n",
       "\n",
       ">      rel_mse (y, y_hat, y_train, mask=None)\n",
       "\n",
       "*Relative Mean Squared Error\n",
       "\n",
       "Computes Relative mean squared error (RelMSE), as proposed by Hyndman & Koehler (2006)\n",
       "as an alternative to percentage errors, to avoid measure unstability.\n",
       "\n",
       "$$\n",
       "\\mathrm{RelMSE}(\\mathbf{y}, \\mathbf{\\hat{y}}, \\mathbf{\\hat{y}}^{naive1}) =\n",
       "\\frac{\\mathrm{MSE}(\\mathbf{y}, \\mathbf{\\hat{y}})}{\\mathrm{MSE}(\\mathbf{y}, \\mathbf{\\hat{y}}^{naive1})}\n",
       "$$\n",
       "\n",
       "**Parameters:**<br>\n",
       "`y`: numpy array, Actual values of size (`n_series`, `horizon`).<br>\n",
       "`y_hat`: numpy array, Predicted values (`n_series`, `horizon`).<br>\n",
       "`mask`: numpy array, Specifies date stamps per serie to consider in loss.<br>\n",
       "\n",
       "**Returns:**<br>\n",
       "`loss`: float.    \n",
       "\n",
       "**References:**<br>\n",
       "- [Hyndman, R. J and Koehler, A. B. (2006).\n",
       "   \"Another look at measures of forecast accuracy\",\n",
       "   International Journal of Forecasting, Volume 22, Issue 4.](https://www.sciencedirect.com/science/article/pii/S0169207006000239)<br>\n",
       "- [Kin G. Olivares, O. Nganba Meetei, Ruijun Ma, Rohan Reddy, Mengfei Cao, Lee Dicker. \n",
       "   \"Probabilistic Hierarchical Forecasting with Deep Poisson Mixtures. \n",
       "   Submitted to the International Journal Forecasting, Working paper available at arxiv.](https://arxiv.org/pdf/2110.13179.pdf)*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/hierarchicalforecast/blob/main/hierarchicalforecast/evaluation.py#L127){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### rel_mse\n",
       "\n",
       ">      rel_mse (y, y_hat, y_train, mask=None)\n",
       "\n",
       "*Relative Mean Squared Error\n",
       "\n",
       "Computes Relative mean squared error (RelMSE), as proposed by Hyndman & Koehler (2006)\n",
       "as an alternative to percentage errors, to avoid measure unstability.\n",
       "\n",
       "$$\n",
       "\\mathrm{RelMSE}(\\mathbf{y}, \\mathbf{\\hat{y}}, \\mathbf{\\hat{y}}^{naive1}) =\n",
       "\\frac{\\mathrm{MSE}(\\mathbf{y}, \\mathbf{\\hat{y}})}{\\mathrm{MSE}(\\mathbf{y}, \\mathbf{\\hat{y}}^{naive1})}\n",
       "$$\n",
       "\n",
       "**Parameters:**<br>\n",
       "`y`: numpy array, Actual values of size (`n_series`, `horizon`).<br>\n",
       "`y_hat`: numpy array, Predicted values (`n_series`, `horizon`).<br>\n",
       "`mask`: numpy array, Specifies date stamps per serie to consider in loss.<br>\n",
       "\n",
       "**Returns:**<br>\n",
       "`loss`: float.    \n",
       "\n",
       "**References:**<br>\n",
       "- [Hyndman, R. J and Koehler, A. B. (2006).\n",
       "   \"Another look at measures of forecast accuracy\",\n",
       "   International Journal of Forecasting, Volume 22, Issue 4.](https://www.sciencedirect.com/science/article/pii/S0169207006000239)<br>\n",
       "- [Kin G. Olivares, O. Nganba Meetei, Ruijun Ma, Rohan Reddy, Mengfei Cao, Lee Dicker. \n",
       "   \"Probabilistic Hierarchical Forecasting with Deep Poisson Mixtures. \n",
       "   Submitted to the International Journal Forecasting, Working paper available at arxiv.](https://arxiv.org/pdf/2110.13179.pdf)*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(rel_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Squared Scaled Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def msse(y, y_hat, y_train, mask=None):\n",
    "    \"\"\"Mean Squared Scaled Error\n",
    "\n",
    "    Computes Mean squared scaled error (MSSE), as proposed by Hyndman & Koehler (2006)\n",
    "    as an alternative to percentage errors, to avoid measure unstability.\n",
    "\n",
    "    $$\n",
    "    \\\\mathrm{MSSE}(\\\\mathbf{y}, \\\\mathbf{\\\\hat{y}}, \\\\mathbf{y}^{in-sample}) =\n",
    "    \\\\frac{\\\\frac{1}{h} \\\\sum^{t+h}_{\\\\tau=t+1} (y_{\\\\tau} - \\\\hat{y}_{\\\\tau})^2}{\\\\frac{1}{t-1} \\\\sum^{t}_{\\\\tau=2} (y_{\\\\tau} - y_{\\\\tau-1})^2}\n",
    "    $$\n",
    "\n",
    "    where $n$ ($n=$`n`) is the size of the training data, and $h$ is the forecasting horizon ($h=$`horizon`).\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `y`: numpy array, Actual values of size (`n_series`, `horizon`).<br>\n",
    "    `y_hat`: numpy array, Predicted values (`n_series`, `horizon`).<br>\n",
    "    `y_train`: numpy array, Predicted values (`n_series`, `n`).<br>\n",
    "    `mask`: numpy array, Specifies date stamps per serie to consider in loss.<br>\n",
    "\n",
    "    **Returns:**<br>\n",
    "    `loss`: float.    \n",
    "\n",
    "    **References:**<br>\n",
    "    - [Hyndman, R. J and Koehler, A. B. (2006).\n",
    "       \"Another look at measures of forecast accuracy\",\n",
    "       International Journal of Forecasting, Volume 22, Issue 4.](https://www.sciencedirect.com/science/article/pii/S0169207006000239)<br>\n",
    "    \"\"\"\n",
    "    if mask is None: \n",
    "       mask = np.ones_like(y)\n",
    "    n_series, horizon = y.shape\n",
    "\n",
    "    eps = np.finfo(float).eps\n",
    "    y_in_sample_naive = y_train[:, :-1]\n",
    "    y_in_sample_true = y_train[:, 1:]\n",
    "    norm = mse(y=y_in_sample_true, y_hat=y_in_sample_naive)\n",
    "    loss = mse(y=y, y_hat=y_hat, weights=mask)\n",
    "    loss = loss / (norm + eps)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/hierarchicalforecast/blob/main/hierarchicalforecast/evaluation.py#L166){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### msse\n",
       "\n",
       ">      msse (y, y_hat, y_train, mask=None)\n",
       "\n",
       "*Mean Squared Scaled Error\n",
       "\n",
       "Computes Mean squared scaled error (MSSE), as proposed by Hyndman & Koehler (2006)\n",
       "as an alternative to percentage errors, to avoid measure unstability.\n",
       "\n",
       "$$\n",
       "\\mathrm{MSSE}(\\mathbf{y}, \\mathbf{\\hat{y}}, \\mathbf{y}^{in-sample}) =\n",
       "\\frac{\\frac{1}{h} \\sum^{t+h}_{\\tau=t+1} (y_{\\tau} - \\hat{y}_{\\tau})^2}{\\frac{1}{t-1} \\sum^{t}_{\\tau=2} (y_{\\tau} - y_{\\tau-1})^2}\n",
       "$$\n",
       "\n",
       "where $n$ ($n=$`n`) is the size of the training data, and $h$ is the forecasting horizon ($h=$`horizon`).\n",
       "\n",
       "**Parameters:**<br>\n",
       "`y`: numpy array, Actual values of size (`n_series`, `horizon`).<br>\n",
       "`y_hat`: numpy array, Predicted values (`n_series`, `horizon`).<br>\n",
       "`y_train`: numpy array, Predicted values (`n_series`, `n`).<br>\n",
       "`mask`: numpy array, Specifies date stamps per serie to consider in loss.<br>\n",
       "\n",
       "**Returns:**<br>\n",
       "`loss`: float.    \n",
       "\n",
       "**References:**<br>\n",
       "- [Hyndman, R. J and Koehler, A. B. (2006).\n",
       "   \"Another look at measures of forecast accuracy\",\n",
       "   International Journal of Forecasting, Volume 22, Issue 4.](https://www.sciencedirect.com/science/article/pii/S0169207006000239)<br>*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/hierarchicalforecast/blob/main/hierarchicalforecast/evaluation.py#L166){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### msse\n",
       "\n",
       ">      msse (y, y_hat, y_train, mask=None)\n",
       "\n",
       "*Mean Squared Scaled Error\n",
       "\n",
       "Computes Mean squared scaled error (MSSE), as proposed by Hyndman & Koehler (2006)\n",
       "as an alternative to percentage errors, to avoid measure unstability.\n",
       "\n",
       "$$\n",
       "\\mathrm{MSSE}(\\mathbf{y}, \\mathbf{\\hat{y}}, \\mathbf{y}^{in-sample}) =\n",
       "\\frac{\\frac{1}{h} \\sum^{t+h}_{\\tau=t+1} (y_{\\tau} - \\hat{y}_{\\tau})^2}{\\frac{1}{t-1} \\sum^{t}_{\\tau=2} (y_{\\tau} - y_{\\tau-1})^2}\n",
       "$$\n",
       "\n",
       "where $n$ ($n=$`n`) is the size of the training data, and $h$ is the forecasting horizon ($h=$`horizon`).\n",
       "\n",
       "**Parameters:**<br>\n",
       "`y`: numpy array, Actual values of size (`n_series`, `horizon`).<br>\n",
       "`y_hat`: numpy array, Predicted values (`n_series`, `horizon`).<br>\n",
       "`y_train`: numpy array, Predicted values (`n_series`, `n`).<br>\n",
       "`mask`: numpy array, Specifies date stamps per serie to consider in loss.<br>\n",
       "\n",
       "**Returns:**<br>\n",
       "`loss`: float.    \n",
       "\n",
       "**References:**<br>\n",
       "- [Hyndman, R. J and Koehler, A. B. (2006).\n",
       "   \"Another look at measures of forecast accuracy\",\n",
       "   International Journal of Forecasting, Volume 22, Issue 4.](https://www.sciencedirect.com/science/article/pii/S0169207006000239)<br>*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(msse)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaled CRPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def scaled_crps(y, y_hat, quantiles):\n",
    "    \"\"\"Scaled Continues Ranked Probability Score\n",
    "\n",
    "    Calculates a scaled variation of the CRPS, as proposed by Rangapuram (2021),\n",
    "    to measure the accuracy of predicted quantiles `y_hat` compared to the observation `y`.\n",
    "\n",
    "    This metric averages percentual weighted absolute deviations as \n",
    "    defined by the quantile losses.\n",
    "\n",
    "    $$\n",
    "    \\mathrm{sCRPS}(\\hat{F}_{\\\\tau}, \\mathbf{y}_{\\\\tau}) = \\\\frac{2}{N} \\sum_{i}\n",
    "    \\int^{1}_{0}\n",
    "    \\\\frac{\\mathrm{QL}(\\hat{F}_{i,\\\\tau}, y_{i,\\\\tau})_{q}}{\\sum_{i} | y_{i,\\\\tau} |} dq\n",
    "    $$\n",
    "\n",
    "    where $\\hat{F}_{\\\\tau}$ is the an estimated multivariate distribution, and $y_{i,\\\\tau}$\n",
    "    are its realizations.\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `y`: numpy array, Actual values of size (`n_series`, `horizon`).<br>\n",
    "    `y_hat`: numpy array, Predicted quantiles of size (`n_series`, `horizon`, `n_quantiles`).<br>\n",
    "    `quantiles`: numpy array,(`n_quantiles`). Quantiles to estimate from the distribution of y.<br>\n",
    "\n",
    "    **Returns:**<br>\n",
    "    `loss`: float.\n",
    "\n",
    "    **References:**<br>\n",
    "    - [Gneiting, Tilmann. (2011). \\\"Quantiles as optimal point forecasts\\\". \n",
    "    International Journal of Forecasting.](https://www.sciencedirect.com/science/article/pii/S0169207010000063)<br>\n",
    "    - [Spyros Makridakis, Evangelos Spiliotis, Vassilios Assimakopoulos, Zhi Chen, Anil Gaba, Ilia Tsetlin, Robert L. Winkler. (2022). \n",
    "    \\\"The M5 uncertainty competition: Results, findings and conclusions\\\". \n",
    "    International Journal of Forecasting.](https://www.sciencedirect.com/science/article/pii/S0169207021001722)<br>\n",
    "    - [Syama Sundar Rangapuram, Lucien D Werner, Konstantinos Benidis, Pedro Mercado, Jan Gasthaus, Tim Januschowski. (2021). \n",
    "    \\\"End-to-End Learning of Coherent Probabilistic Forecasts for Hierarchical Time Series\\\". \n",
    "    Proceedings of the 38th International Conference on Machine Learning (ICML).](https://proceedings.mlr.press/v139/rangapuram21a.html)\n",
    "    \"\"\"\n",
    "    eps = np.finfo(float).eps\n",
    "    norm  = np.sum(np.abs(y))\n",
    "    loss  = mqloss(y=y, y_hat=y_hat, quantiles=quantiles)\n",
    "    loss  = 2 * loss * np.sum(np.ones(y.shape)) / (norm + eps)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/hierarchicalforecast/blob/main/hierarchicalforecast/evaluation.py#L206){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### scaled_crps\n",
       "\n",
       ">      scaled_crps (y, y_hat, quantiles)\n",
       "\n",
       "*Scaled Continues Ranked Probability Score\n",
       "\n",
       "Calculates a scaled variation of the CRPS, as proposed by Rangapuram (2021),\n",
       "to measure the accuracy of predicted quantiles `y_hat` compared to the observation `y`.\n",
       "\n",
       "This metric averages percentual weighted absolute deviations as \n",
       "defined by the quantile losses.\n",
       "\n",
       "$$\n",
       "\\mathrm{sCRPS}(\\hat{F}_{\\tau}, \\mathbf{y}_{\\tau}) = \\frac{2}{N} \\sum_{i}\n",
       "\\int^{1}_{0}\n",
       "\\frac{\\mathrm{QL}(\\hat{F}_{i,\\tau}, y_{i,\\tau})_{q}}{\\sum_{i} | y_{i,\\tau} |} dq\n",
       "$$\n",
       "\n",
       "where $\\hat{F}_{\\tau}$ is the an estimated multivariate distribution, and $y_{i,\\tau}$\n",
       "are its realizations.\n",
       "\n",
       "**Parameters:**<br>\n",
       "`y`: numpy array, Actual values of size (`n_series`, `horizon`).<br>\n",
       "`y_hat`: numpy array, Predicted quantiles of size (`n_series`, `horizon`, `n_quantiles`).<br>\n",
       "`quantiles`: numpy array,(`n_quantiles`). Quantiles to estimate from the distribution of y.<br>\n",
       "\n",
       "**Returns:**<br>\n",
       "`loss`: float.\n",
       "\n",
       "**References:**<br>\n",
       "- [Gneiting, Tilmann. (2011). \"Quantiles as optimal point forecasts\". \n",
       "International Journal of Forecasting.](https://www.sciencedirect.com/science/article/pii/S0169207010000063)<br>\n",
       "- [Spyros Makridakis, Evangelos Spiliotis, Vassilios Assimakopoulos, Zhi Chen, Anil Gaba, Ilia Tsetlin, Robert L. Winkler. (2022). \n",
       "\"The M5 uncertainty competition: Results, findings and conclusions\". \n",
       "International Journal of Forecasting.](https://www.sciencedirect.com/science/article/pii/S0169207021001722)<br>\n",
       "- [Syama Sundar Rangapuram, Lucien D Werner, Konstantinos Benidis, Pedro Mercado, Jan Gasthaus, Tim Januschowski. (2021). \n",
       "\"End-to-End Learning of Coherent Probabilistic Forecasts for Hierarchical Time Series\". \n",
       "Proceedings of the 38th International Conference on Machine Learning (ICML).](https://proceedings.mlr.press/v139/rangapuram21a.html)*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/hierarchicalforecast/blob/main/hierarchicalforecast/evaluation.py#L206){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### scaled_crps\n",
       "\n",
       ">      scaled_crps (y, y_hat, quantiles)\n",
       "\n",
       "*Scaled Continues Ranked Probability Score\n",
       "\n",
       "Calculates a scaled variation of the CRPS, as proposed by Rangapuram (2021),\n",
       "to measure the accuracy of predicted quantiles `y_hat` compared to the observation `y`.\n",
       "\n",
       "This metric averages percentual weighted absolute deviations as \n",
       "defined by the quantile losses.\n",
       "\n",
       "$$\n",
       "\\mathrm{sCRPS}(\\hat{F}_{\\tau}, \\mathbf{y}_{\\tau}) = \\frac{2}{N} \\sum_{i}\n",
       "\\int^{1}_{0}\n",
       "\\frac{\\mathrm{QL}(\\hat{F}_{i,\\tau}, y_{i,\\tau})_{q}}{\\sum_{i} | y_{i,\\tau} |} dq\n",
       "$$\n",
       "\n",
       "where $\\hat{F}_{\\tau}$ is the an estimated multivariate distribution, and $y_{i,\\tau}$\n",
       "are its realizations.\n",
       "\n",
       "**Parameters:**<br>\n",
       "`y`: numpy array, Actual values of size (`n_series`, `horizon`).<br>\n",
       "`y_hat`: numpy array, Predicted quantiles of size (`n_series`, `horizon`, `n_quantiles`).<br>\n",
       "`quantiles`: numpy array,(`n_quantiles`). Quantiles to estimate from the distribution of y.<br>\n",
       "\n",
       "**Returns:**<br>\n",
       "`loss`: float.\n",
       "\n",
       "**References:**<br>\n",
       "- [Gneiting, Tilmann. (2011). \"Quantiles as optimal point forecasts\". \n",
       "International Journal of Forecasting.](https://www.sciencedirect.com/science/article/pii/S0169207010000063)<br>\n",
       "- [Spyros Makridakis, Evangelos Spiliotis, Vassilios Assimakopoulos, Zhi Chen, Anil Gaba, Ilia Tsetlin, Robert L. Winkler. (2022). \n",
       "\"The M5 uncertainty competition: Results, findings and conclusions\". \n",
       "International Journal of Forecasting.](https://www.sciencedirect.com/science/article/pii/S0169207021001722)<br>\n",
       "- [Syama Sundar Rangapuram, Lucien D Werner, Konstantinos Benidis, Pedro Mercado, Jan Gasthaus, Tim Januschowski. (2021). \n",
       "\"End-to-End Learning of Coherent Probabilistic Forecasts for Hierarchical Time Series\". \n",
       "Proceedings of the 38th International Conference on Machine Learning (ICML).](https://proceedings.mlr.press/v139/rangapuram21a.html)*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(scaled_crps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Energy Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def energy_score(y, y_sample1, y_sample2, beta=2):\n",
    "    \"\"\"Energy Score\n",
    "\n",
    "    Calculates Gneiting's Energy Score sample approximation for\n",
    "    `y` and independent multivariate samples `y_sample1` and `y_sample2`.\n",
    "    The Energy Score generalizes the CRPS (`beta`=1) in the multivariate setting.\n",
    "\n",
    "    $$\n",
    "    \\mathrm{ES}(\\\\mathbf{y}_{\\\\tau}, \\\\mathbf{\\hat{y}}_{\\\\tau}, \\\\mathbf{\\hat{y}}_{\\\\tau}') \n",
    "    = \\\\frac{1}{2} \\mathbb{E}_{\\hat{P}} \\\\left[ ||\\\\mathbf{\\hat{y}}_{\\\\tau} - \\\\mathbf{\\hat{y}}_{\\\\tau}'||^{\\\\beta} \\\\right]\n",
    "    -  \\mathbb{E}_{\\hat{P}} \\\\left[ ||\\\\mathbf{y}_{\\\\tau} - \\\\mathbf{\\hat{y}}_{\\\\tau}||^{\\\\beta} \\\\right] \n",
    "    \\quad \\\\beta \\in (0,2]\n",
    "    $$\n",
    "\n",
    "    where $\\\\mathbf{\\hat{y}}_{\\\\tau}, \\\\mathbf{\\hat{y}}_{\\\\tau}'$ are independent samples drawn from $\\hat{P}$.\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `y`: numpy array, Actual values of size (`n_series`, `horizon`).<br>\n",
    "    `y_sample1`: numpy array, predictive distribution sample of size (`n_series`, `horizon`, `n_samples`).<br>\n",
    "    `y_sample2`: numpy array, predictive distribution sample of size (`n_series`, `horizon`, `n_samples`).<br>\n",
    "    `beta`: float in (0,2], defines the energy score's power for the euclidean metric.<br>\n",
    "\n",
    "    **Returns:**<br>\n",
    "    `score`: float.\n",
    "\n",
    "    **References:**<br>\n",
    "    - [Gneiting, Tilmann, and Adrian E. Raftery. (2007). \n",
    "    \\\"Strictly proper scoring rules, prediction and estimation\\\". \n",
    "    Journal of the American Statistical Association.](https://sites.stat.washington.edu/raftery/Research/PDF/Gneiting2007jasa.pdf)<br>\n",
    "    - [Anastasios Panagiotelis, Puwasala Gamakumara, George Athanasopoulos, Rob J. Hyndman. (2022). \n",
    "    \\\"Probabilistic forecast reconciliation: Properties, evaluation and score optimisation\\\". \n",
    "    European Journal of Operational Research.](https://www.sciencedirect.com/science/article/pii/S0377221722006087)    \n",
    "    \"\"\"\n",
    "    if beta>2 or beta<0:\n",
    "        raise Exception(\"beta needs to be between 0 and 2.\")\n",
    "\n",
    "    dif1 = (y_sample1 - y_sample2)\n",
    "    dif2 = (y[:,:,None] - y_sample1)\n",
    "\n",
    "    term1 = np.linalg.norm(dif1, axis=0) ** beta\n",
    "    term2 = np.linalg.norm(dif2, axis=0) ** beta\n",
    "\n",
    "    score = np.mean(term2 - 0.5 * term1)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/hierarchicalforecast/blob/main/hierarchicalforecast/evaluation.py#L249){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### energy_score\n",
       "\n",
       ">      energy_score (y, y_sample1, y_sample2, beta=2)\n",
       "\n",
       "*Energy Score\n",
       "\n",
       "Calculates Gneiting's Energy Score sample approximation for\n",
       "`y` and independent multivariate samples `y_sample1` and `y_sample2`.\n",
       "The Energy Score generalizes the CRPS (`beta`=1) in the multivariate setting.\n",
       "\n",
       "$$\n",
       "\\mathrm{ES}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}_{\\tau}, \\mathbf{\\hat{y}}_{\\tau}') \n",
       "= \\frac{1}{2} \\mathbb{E}_{\\hat{P}} \\left[ ||\\mathbf{\\hat{y}}_{\\tau} - \\mathbf{\\hat{y}}_{\\tau}'||^{\\beta} \\right]\n",
       "-  \\mathbb{E}_{\\hat{P}} \\left[ ||\\mathbf{y}_{\\tau} - \\mathbf{\\hat{y}}_{\\tau}||^{\\beta} \\right] \n",
       "\\quad \\beta \\in (0,2]\n",
       "$$\n",
       "\n",
       "where $\\mathbf{\\hat{y}}_{\\tau}, \\mathbf{\\hat{y}}_{\\tau}'$ are independent samples drawn from $\\hat{P}$.\n",
       "\n",
       "**Parameters:**<br>\n",
       "`y`: numpy array, Actual values of size (`n_series`, `horizon`).<br>\n",
       "`y_sample1`: numpy array, predictive distribution sample of size (`n_series`, `horizon`, `n_samples`).<br>\n",
       "`y_sample2`: numpy array, predictive distribution sample of size (`n_series`, `horizon`, `n_samples`).<br>\n",
       "`beta`: float in (0,2], defines the energy score's power for the euclidean metric.<br>\n",
       "\n",
       "**Returns:**<br>\n",
       "`score`: float.\n",
       "\n",
       "**References:**<br>\n",
       "- [Gneiting, Tilmann, and Adrian E. Raftery. (2007). \n",
       "\"Strictly proper scoring rules, prediction and estimation\". \n",
       "Journal of the American Statistical Association.](https://sites.stat.washington.edu/raftery/Research/PDF/Gneiting2007jasa.pdf)<br>\n",
       "- [Anastasios Panagiotelis, Puwasala Gamakumara, George Athanasopoulos, Rob J. Hyndman. (2022). \n",
       "\"Probabilistic forecast reconciliation: Properties, evaluation and score optimisation\". \n",
       "European Journal of Operational Research.](https://www.sciencedirect.com/science/article/pii/S0377221722006087)*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/hierarchicalforecast/blob/main/hierarchicalforecast/evaluation.py#L249){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### energy_score\n",
       "\n",
       ">      energy_score (y, y_sample1, y_sample2, beta=2)\n",
       "\n",
       "*Energy Score\n",
       "\n",
       "Calculates Gneiting's Energy Score sample approximation for\n",
       "`y` and independent multivariate samples `y_sample1` and `y_sample2`.\n",
       "The Energy Score generalizes the CRPS (`beta`=1) in the multivariate setting.\n",
       "\n",
       "$$\n",
       "\\mathrm{ES}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}_{\\tau}, \\mathbf{\\hat{y}}_{\\tau}') \n",
       "= \\frac{1}{2} \\mathbb{E}_{\\hat{P}} \\left[ ||\\mathbf{\\hat{y}}_{\\tau} - \\mathbf{\\hat{y}}_{\\tau}'||^{\\beta} \\right]\n",
       "-  \\mathbb{E}_{\\hat{P}} \\left[ ||\\mathbf{y}_{\\tau} - \\mathbf{\\hat{y}}_{\\tau}||^{\\beta} \\right] \n",
       "\\quad \\beta \\in (0,2]\n",
       "$$\n",
       "\n",
       "where $\\mathbf{\\hat{y}}_{\\tau}, \\mathbf{\\hat{y}}_{\\tau}'$ are independent samples drawn from $\\hat{P}$.\n",
       "\n",
       "**Parameters:**<br>\n",
       "`y`: numpy array, Actual values of size (`n_series`, `horizon`).<br>\n",
       "`y_sample1`: numpy array, predictive distribution sample of size (`n_series`, `horizon`, `n_samples`).<br>\n",
       "`y_sample2`: numpy array, predictive distribution sample of size (`n_series`, `horizon`, `n_samples`).<br>\n",
       "`beta`: float in (0,2], defines the energy score's power for the euclidean metric.<br>\n",
       "\n",
       "**Returns:**<br>\n",
       "`score`: float.\n",
       "\n",
       "**References:**<br>\n",
       "- [Gneiting, Tilmann, and Adrian E. Raftery. (2007). \n",
       "\"Strictly proper scoring rules, prediction and estimation\". \n",
       "Journal of the American Statistical Association.](https://sites.stat.washington.edu/raftery/Research/PDF/Gneiting2007jasa.pdf)<br>\n",
       "- [Anastasios Panagiotelis, Puwasala Gamakumara, George Athanasopoulos, Rob J. Hyndman. (2022). \n",
       "\"Probabilistic forecast reconciliation: Properties, evaluation and score optimisation\". \n",
       "European Journal of Operational Research.](https://www.sciencedirect.com/science/article/pii/S0377221722006087)*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(energy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def log_score(y, y_hat, cov, allow_singular=True):\n",
    "    \"\"\" Log Score.\n",
    "\n",
    "    One of the simplest multivariate probability scoring rules,\n",
    "    it evaluates the negative density at the value of the realisation.\n",
    "\n",
    "    $$\n",
    "    \\mathrm{LS}(\\\\mathbf{y}_{\\\\tau}, \\\\mathbf{P}(\\\\theta_{\\\\tau}))\n",
    "    = - \\\\log(f(\\\\mathbf{y}_{\\\\tau}, \\\\theta_{\\\\tau}))\n",
    "    $$\n",
    "\n",
    "    where $f$ is the density, $\\\\mathbf{P}(\\\\theta_{\\\\tau})$ is a \n",
    "    parametric distribution and $f(\\\\mathbf{y}_{\\\\tau}, \\\\theta_{\\\\tau})$\n",
    "    represents its density. \n",
    "    For the moment we only support multivariate normal log score.\n",
    "\n",
    "    $$\n",
    "    f(\\\\mathbf{y}_{\\\\tau}, \\\\theta_{\\\\tau}) =\n",
    "    (2\\\\pi )^{-k/2}\\\\det({\\\\boldsymbol{\\Sigma }})^{-1/2}\n",
    "    \\,\\\\exp \\\\left(\n",
    "    -{\\\\frac {1}{2}}(\\mathbf{y}_{\\\\tau} -\\\\hat{\\mathbf{y}}_{\\\\tau})^{\\!{\\mathsf{T}}}\n",
    "    {\\\\boldsymbol{\\Sigma }}^{-1}\n",
    "    (\\mathbf{y}_{\\\\tau} -\\\\hat{\\mathbf{y}}_{\\\\tau})\n",
    "    \\\\right)\n",
    "    $$\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `y`: numpy array, Actual values of size (`n_series`, `horizon`).<br>\n",
    "    `y_hat`: numpy array, Predicted values (`n_series`, `horizon`).<br>\n",
    "    `cov`: numpy matrix, Predicted values covariance (`n_series`, `n_series`, `horizon`).<br>\n",
    "    `allow_singular`: bool=True, if true allows singular covariance.<br>\n",
    "\n",
    "    **Returns:**<br>\n",
    "    `score`: float.\n",
    "    \"\"\"\n",
    "    scores = [multivariate_normal.pdf(x=y[:,h], \n",
    "                  mean=y_hat[:,h], cov=cov[:,:,h], allow_singular=allow_singular) \\\n",
    "                  for h in range(y.shape[1])]\n",
    "    score = np.mean(scores)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/hierarchicalforecast/blob/main/hierarchicalforecast/evaluation.py#L295){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### log_score\n",
       "\n",
       ">      log_score (y, y_hat, cov, allow_singular=True)\n",
       "\n",
       "*Log Score.\n",
       "\n",
       "One of the simplest multivariate probability scoring rules,\n",
       "it evaluates the negative density at the value of the realisation.\n",
       "\n",
       "$$\n",
       "\\mathrm{LS}(\\mathbf{y}_{\\tau}, \\mathbf{P}(\\theta_{\\tau}))\n",
       "= - \\log(f(\\mathbf{y}_{\\tau}, \\theta_{\\tau}))\n",
       "$$\n",
       "\n",
       "where $f$ is the density, $\\mathbf{P}(\\theta_{\\tau})$ is a \n",
       "parametric distribution and $f(\\mathbf{y}_{\\tau}, \\theta_{\\tau})$\n",
       "represents its density. \n",
       "For the moment we only support multivariate normal log score.\n",
       "\n",
       "$$\n",
       "f(\\mathbf{y}_{\\tau}, \\theta_{\\tau}) =\n",
       "(2\\pi )^{-k/2}\\det({\\boldsymbol{\\Sigma }})^{-1/2}\n",
       "\\,\\exp \\left(\n",
       "-{\\frac {1}{2}}(\\mathbf{y}_{\\tau} -\\hat{\\mathbf{y}}_{\\tau})^{\\!{\\mathsf{T}}}\n",
       "{\\boldsymbol{\\Sigma }}^{-1}\n",
       "(\\mathbf{y}_{\\tau} -\\hat{\\mathbf{y}}_{\\tau})\n",
       "\\right)\n",
       "$$\n",
       "\n",
       "**Parameters:**<br>\n",
       "`y`: numpy array, Actual values of size (`n_series`, `horizon`).<br>\n",
       "`y_hat`: numpy array, Predicted values (`n_series`, `horizon`).<br>\n",
       "`cov`: numpy matrix, Predicted values covariance (`n_series`, `n_series`, `horizon`).<br>\n",
       "`allow_singular`: bool=True, if true allows singular covariance.<br>\n",
       "\n",
       "**Returns:**<br>\n",
       "`score`: float.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/hierarchicalforecast/blob/main/hierarchicalforecast/evaluation.py#L295){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### log_score\n",
       "\n",
       ">      log_score (y, y_hat, cov, allow_singular=True)\n",
       "\n",
       "*Log Score.\n",
       "\n",
       "One of the simplest multivariate probability scoring rules,\n",
       "it evaluates the negative density at the value of the realisation.\n",
       "\n",
       "$$\n",
       "\\mathrm{LS}(\\mathbf{y}_{\\tau}, \\mathbf{P}(\\theta_{\\tau}))\n",
       "= - \\log(f(\\mathbf{y}_{\\tau}, \\theta_{\\tau}))\n",
       "$$\n",
       "\n",
       "where $f$ is the density, $\\mathbf{P}(\\theta_{\\tau})$ is a \n",
       "parametric distribution and $f(\\mathbf{y}_{\\tau}, \\theta_{\\tau})$\n",
       "represents its density. \n",
       "For the moment we only support multivariate normal log score.\n",
       "\n",
       "$$\n",
       "f(\\mathbf{y}_{\\tau}, \\theta_{\\tau}) =\n",
       "(2\\pi )^{-k/2}\\det({\\boldsymbol{\\Sigma }})^{-1/2}\n",
       "\\,\\exp \\left(\n",
       "-{\\frac {1}{2}}(\\mathbf{y}_{\\tau} -\\hat{\\mathbf{y}}_{\\tau})^{\\!{\\mathsf{T}}}\n",
       "{\\boldsymbol{\\Sigma }}^{-1}\n",
       "(\\mathbf{y}_{\\tau} -\\hat{\\mathbf{y}}_{\\tau})\n",
       "\\right)\n",
       "$$\n",
       "\n",
       "**Parameters:**<br>\n",
       "`y`: numpy array, Actual values of size (`n_series`, `horizon`).<br>\n",
       "`y_hat`: numpy array, Predicted values (`n_series`, `horizon`).<br>\n",
       "`cov`: numpy matrix, Predicted values covariance (`n_series`, `n_series`, `horizon`).<br>\n",
       "`allow_singular`: bool=True, if true allows singular covariance.<br>\n",
       "\n",
       "**Returns:**<br>\n",
       "`score`: float.*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(log_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00108914, 0.01033349, 0.05946514, 0.20755375, 0.43939129,\n",
       "       0.56418958, 0.43939129, 0.20755375, 0.05946514, 0.01033349])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.linspace(0, 5, 10, endpoint=False)\n",
    "y = multivariate_normal.pdf(x, mean=2.5, cov=0.5)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class HierarchicalEvaluation:\n",
    "    \"\"\"Hierarchical Evaluation Class.\n",
    "    \n",
    "    You can use your own metrics to evaluate the performance of each level in the structure.\n",
    "    The metrics receive `y` and `y_hat` as arguments and they are numpy arrays of size `(series, horizon)`.\n",
    "    Consider, for example, the function `rmse` that calculates the root mean squared error.\n",
    "\n",
    "    This class facilitates measurements across the hierarchy, defined by the `tags` list.\n",
    "    See also the [aggregate method](https://nixtla.github.io/hierarchicalforecast/utils.html#aggregate).\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `evaluators`: functions with arguments `y`, `y_hat` (numpy arrays).<br>\n",
    "\n",
    "    **References:**<br>\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 evaluators: List[Callable]):\n",
    "        self.evaluators = evaluators\n",
    "\n",
    "    def evaluate(self, \n",
    "                 Y_hat_df: Frame,\n",
    "                 Y_test_df: Frame,\n",
    "                 tags: Dict[str, np.ndarray],\n",
    "                 Y_df: Optional[Frame] = None,\n",
    "                 benchmark: Optional[str] = None,\n",
    "                 id_col: str = \"unique_id\",\n",
    "                 time_col: str = \"ds\", \n",
    "                 target_col: str = \"y\", \n",
    "                 ):\n",
    "        \"\"\"Hierarchical Evaluation Method.\n",
    "\n",
    "        **Parameters:**<br>\n",
    "        `Y_hat_df`: DataFrame, Forecasts with columns `'unique_id'`, `'ds'` and models to evaluate.<br>\n",
    "        `Y_test_df`:  DataFrame, Observed values with columns `['unique_id', 'ds', 'y']`.<br>\n",
    "        `tags`: np.array, each str key is a level and its value contains tags associated to that level.<br>\n",
    "        `Y_df`: DataFrame, Training set of base time series with columns `['unique_id', 'ds', 'y']`.<br>\n",
    "        `benchmark`: str, If passed, evaluators are scaled by the error of this benchark.<br>\n",
    "        `id_col` : str='unique_id', column that identifies each serie.<br>\n",
    "        `time_col` : str='ds', column that identifies each timestep, its values can be timestamps or integers.<br>\n",
    "        `target_col` : str='y', column that contains the target.<br>\n",
    "\n",
    "        **Returns:**<br>\n",
    "        `evaluation`: DataFrame with accuracy measurements across hierarchical levels.\n",
    "        \"\"\"\n",
    "        Y_hat_nw = _to_narwhals_maybe_warn_and_reset_idx(Y_hat_df, id_col)\n",
    "        Y_test_nw = _to_narwhals_maybe_warn_and_reset_idx(Y_test_df, id_col)\n",
    "        native_namespace = nw.get_native_namespace(Y_hat_nw)\n",
    "        if Y_df is not None:\n",
    "            Y_nw = _to_narwhals_maybe_warn_and_reset_idx(Y_df, id_col)\n",
    "\n",
    "        n_series = len(set(Y_hat_nw[id_col]))\n",
    "        h = len(set(Y_hat_nw[time_col]))\n",
    "        if len(Y_hat_nw) != n_series * h:\n",
    "            raise Exception('Y_hat_df should have a forecast for each series and horizon')\n",
    "\n",
    "        fn_names = [fn.__name__ for fn in self.evaluators]\n",
    "        has_y_insample = any(['y_insample' in signature(fn).parameters for fn in self.evaluators])\n",
    "        if has_y_insample and Y_df is None:\n",
    "            raise Exception('At least one evaluator needs y_insample, please pass `Y_df`')\n",
    "\n",
    "        if benchmark is not None:\n",
    "            fn_names = [f'{fn_name}-scaled' for fn_name in fn_names]\n",
    "\n",
    "        tags_ = {'Overall': np.concatenate(list(tags.values()))}\n",
    "        tags_ = {**tags, **tags_}\n",
    "\n",
    "        model_names = [c for c in Y_hat_nw.columns if c not in [id_col, time_col, target_col]]\n",
    "        evaluation_np = np.empty((len(tags_), len(fn_names), len(model_names)), dtype=np.float64)\n",
    "        evaluation_index_np = np.empty((len(tags_) * len(fn_names), 2), dtype=object)\n",
    "        Y_h = Y_hat_nw.join(Y_test_nw, how=\"left\", on=[id_col, time_col]).sort(by=[id_col, time_col])\n",
    "        for i_level, (level, cats) in enumerate(tags_.items()):\n",
    "            Y_h_cats = Y_h.filter(nw.col(id_col).is_in(cats))\n",
    "            y_test_cats = Y_h_cats[target_col].to_numpy().reshape(-1, h)\n",
    "\n",
    "            if has_y_insample and Y_df is not None:\n",
    "                y_insample = Y_nw.pivot(on=time_col, index = id_col, values = target_col, sort_columns=True).sort(by=id_col)\n",
    "                y_insample_cols_ex_id_col = y_insample.columns\n",
    "                y_insample_cols_ex_id_col.remove(id_col)\n",
    "                y_insample = y_insample.filter(nw.col(id_col).is_in(cats))\n",
    "                y_insample = y_insample.select(nw.col(y_insample_cols_ex_id_col)).to_numpy()\n",
    "\n",
    "            for i_fn, fn in enumerate(self.evaluators):\n",
    "                if 'y_insample' in signature(fn).parameters:\n",
    "                    kwargs = {'y_insample': y_insample}\n",
    "                else:\n",
    "                    kwargs = {}\n",
    "                fn_name = fn_names[i_fn]\n",
    "                for i_model, model in enumerate(model_names):\n",
    "                    loss = fn(y_test_cats, Y_h_cats[model].to_numpy().reshape(-1, h), **kwargs)\n",
    "                    if benchmark is not None:\n",
    "                        scale = fn(y_test_cats, Y_h_cats[benchmark].to_numpy().reshape(-1, h), **kwargs)\n",
    "                        if np.isclose(scale, 0., atol=np.finfo(float).eps):\n",
    "                            scale += np.finfo(float).eps\n",
    "                            if np.isclose(scale, loss, atol=1e-8):\n",
    "                                scale = 1.\n",
    "                        loss /= scale\n",
    "\n",
    "                    evaluation_np[i_level, i_fn, i_model] = loss\n",
    "                    evaluation_index_np[i_level * len(fn_names) + i_fn, 0] = level\n",
    "                    evaluation_index_np[i_level * len(fn_names) + i_fn, 1] = fn_name\n",
    "\n",
    "        evaluation_np = evaluation_np.reshape(-1, len(model_names))\n",
    "        evaluation_index_dict = {\"level\": evaluation_index_np[:, 0], \"metric\": evaluation_index_np[:, 1]}\n",
    "        evaluation_index_nw = nw.from_dict(evaluation_index_dict, native_namespace=native_namespace)\n",
    "        evaluation_dict = dict(zip(model_names, evaluation_np.T))\n",
    "        evaluation_nw = evaluation_index_nw.with_columns(**evaluation_dict)\n",
    "        evaluation_nw = evaluation_nw[[\"level\", \"metric\"] + model_names]\n",
    "\n",
    "        evaluation = _to_native_maybe_set_index(evaluation_nw, [\"level\", \"metric\"])\n",
    "\n",
    "        return evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/hierarchicalforecast/blob/main/hierarchicalforecast/evaluation.py#L340){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### HierarchicalEvaluation\n",
       "\n",
       ">      HierarchicalEvaluation (evaluators:List[Callable])\n",
       "\n",
       "*Hierarchical Evaluation Class.\n",
       "\n",
       "You can use your own metrics to evaluate the performance of each level in the structure.\n",
       "The metrics receive `y` and `y_hat` as arguments and they are numpy arrays of size `(series, horizon)`.\n",
       "Consider, for example, the function `rmse` that calculates the root mean squared error.\n",
       "\n",
       "This class facilitates measurements across the hierarchy, defined by the `tags` list.\n",
       "See also the [aggregate method](https://nixtla.github.io/hierarchicalforecast/utils.html#aggregate).\n",
       "\n",
       "**Parameters:**<br>\n",
       "`evaluators`: functions with arguments `y`, `y_hat` (numpy arrays).<br>\n",
       "\n",
       "**References:**<br>*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/hierarchicalforecast/blob/main/hierarchicalforecast/evaluation.py#L340){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### HierarchicalEvaluation\n",
       "\n",
       ">      HierarchicalEvaluation (evaluators:List[Callable])\n",
       "\n",
       "*Hierarchical Evaluation Class.\n",
       "\n",
       "You can use your own metrics to evaluate the performance of each level in the structure.\n",
       "The metrics receive `y` and `y_hat` as arguments and they are numpy arrays of size `(series, horizon)`.\n",
       "Consider, for example, the function `rmse` that calculates the root mean squared error.\n",
       "\n",
       "This class facilitates measurements across the hierarchy, defined by the `tags` list.\n",
       "See also the [aggregate method](https://nixtla.github.io/hierarchicalforecast/utils.html#aggregate).\n",
       "\n",
       "**Parameters:**<br>\n",
       "`evaluators`: functions with arguments `y`, `y_hat` (numpy arrays).<br>\n",
       "\n",
       "**References:**<br>*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(HierarchicalEvaluation, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/hierarchicalforecast/blob/main/hierarchicalforecast/evaluation.py#L359){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### HierarchicalEvaluation.evaluate\n",
       "\n",
       ">      HierarchicalEvaluation.evaluate (Y_hat_df:Union[ForwardRef('DataFrame[Any\n",
       ">                                       ]'),ForwardRef('LazyFrame[Any]')], Y_tes\n",
       ">                                       t_df:Union[ForwardRef('DataFrame[Any]'),\n",
       ">                                       ForwardRef('LazyFrame[Any]')],\n",
       ">                                       tags:Dict[str,numpy.ndarray], Y_df:Union\n",
       ">                                       [ForwardRef('DataFrame[Any]'),ForwardRef\n",
       ">                                       ('LazyFrame[Any]'),NoneType]=None,\n",
       ">                                       benchmark:Optional[str]=None,\n",
       ">                                       id_col:str='unique_id',\n",
       ">                                       time_col:str='ds', target_col:str='y')\n",
       "\n",
       "*Hierarchical Evaluation Method.\n",
       "\n",
       "**Parameters:**<br>\n",
       "`Y_hat_df`: DataFrame, Forecasts with columns `'unique_id'`, `'ds'` and models to evaluate.<br>\n",
       "`Y_test_df`:  DataFrame, Observed values with columns `['unique_id', 'ds', 'y']`.<br>\n",
       "`tags`: np.array, each str key is a level and its value contains tags associated to that level.<br>\n",
       "`Y_df`: DataFrame, Training set of base time series with columns `['unique_id', 'ds', 'y']`.<br>\n",
       "`benchmark`: str, If passed, evaluators are scaled by the error of this benchark.<br>\n",
       "`id_col` : str='unique_id', column that identifies each serie.<br>\n",
       "`time_col` : str='ds', column that identifies each timestep, its values can be timestamps or integers.<br>\n",
       "`target_col` : str='y', column that contains the target.<br>\n",
       "\n",
       "**Returns:**<br>\n",
       "`evaluation`: DataFrame with accuracy measurements across hierarchical levels.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/hierarchicalforecast/blob/main/hierarchicalforecast/evaluation.py#L359){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### HierarchicalEvaluation.evaluate\n",
       "\n",
       ">      HierarchicalEvaluation.evaluate (Y_hat_df:Union[ForwardRef('DataFrame[Any\n",
       ">                                       ]'),ForwardRef('LazyFrame[Any]')], Y_tes\n",
       ">                                       t_df:Union[ForwardRef('DataFrame[Any]'),\n",
       ">                                       ForwardRef('LazyFrame[Any]')],\n",
       ">                                       tags:Dict[str,numpy.ndarray], Y_df:Union\n",
       ">                                       [ForwardRef('DataFrame[Any]'),ForwardRef\n",
       ">                                       ('LazyFrame[Any]'),NoneType]=None,\n",
       ">                                       benchmark:Optional[str]=None,\n",
       ">                                       id_col:str='unique_id',\n",
       ">                                       time_col:str='ds', target_col:str='y')\n",
       "\n",
       "*Hierarchical Evaluation Method.\n",
       "\n",
       "**Parameters:**<br>\n",
       "`Y_hat_df`: DataFrame, Forecasts with columns `'unique_id'`, `'ds'` and models to evaluate.<br>\n",
       "`Y_test_df`:  DataFrame, Observed values with columns `['unique_id', 'ds', 'y']`.<br>\n",
       "`tags`: np.array, each str key is a level and its value contains tags associated to that level.<br>\n",
       "`Y_df`: DataFrame, Training set of base time series with columns `['unique_id', 'ds', 'y']`.<br>\n",
       "`benchmark`: str, If passed, evaluators are scaled by the error of this benchark.<br>\n",
       "`id_col` : str='unique_id', column that identifies each serie.<br>\n",
       "`time_col` : str='ds', column that identifies each timestep, its values can be timestamps or integers.<br>\n",
       "`target_col` : str='y', column that contains the target.<br>\n",
       "\n",
       "**Returns:**<br>\n",
       "`evaluation`: DataFrame with accuracy measurements across hierarchical levels.*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(HierarchicalEvaluation.evaluate, title_level=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "def rmse(y, y_hat):\n",
    "    return np.mean(np.sqrt(np.mean((y-y_hat)**2, axis=1)))\n",
    "\n",
    "def mase(y, y_hat, y_insample, seasonality=4):\n",
    "    errors = np.mean(np.abs(y - y_hat), axis=1)\n",
    "    scale = np.mean(np.abs(y_insample[:, seasonality:] - y_insample[:, :-seasonality]), axis=1)\n",
    "    return np.mean(errors / scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from hierarchicalforecast.core import HierarchicalReconciliation\n",
    "from hierarchicalforecast.methods import BottomUp, MinTrace, ERM\n",
    "from hierarchicalforecast.utils import aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/Nixtla/transfer-learning-time-series/main/datasets/tourism.csv')\n",
    "df = df.rename({'Trips': 'y', 'Quarter': 'ds'}, axis=1)\n",
    "df.insert(0, 'Country', 'Australia')\n",
    "\n",
    "# non strictly hierarchical structure\n",
    "hiers_grouped = [\n",
    "    ['Country'],\n",
    "    ['Country', 'State'], \n",
    "    ['Country', 'Purpose'], \n",
    "    ['Country', 'State', 'Region'], \n",
    "    ['Country', 'State', 'Purpose'], \n",
    "    ['Country', 'State', 'Region', 'Purpose']\n",
    "]\n",
    "# strictly hierarchical structure\n",
    "hiers_strictly = [\n",
    "    ['Country'],\n",
    "    ['Country', 'State'], \n",
    "    ['Country', 'State', 'Region'], \n",
    "]\n",
    "\n",
    "# getting df\n",
    "hier_grouped_df, S_grouped, tags_grouped = aggregate(df, hiers_grouped)\n",
    "\n",
    "#split train/test\n",
    "hier_grouped_df['y_model'] = hier_grouped_df['y']\n",
    "# we should be able to recover y using the methods\n",
    "hier_grouped_df_h = hier_grouped_df.groupby('unique_id').tail(12).reset_index(drop=True)\n",
    "ds_h = hier_grouped_df_h['ds'].unique()\n",
    "hier_grouped_df = hier_grouped_df.query('~(ds in @ds_h)')\n",
    "#adding noise to `y_model` to avoid perfect fited values\n",
    "rng = np.random.default_rng(0)\n",
    "hier_grouped_df['y_model'] += rng.uniform(-1, 1, len(hier_grouped_df))\n",
    "\n",
    "#hierachical reconciliation\n",
    "hrec = HierarchicalReconciliation(reconcilers=[\n",
    "    #these methods should reconstruct the original y\n",
    "    BottomUp(),\n",
    "    MinTrace(method='ols'),\n",
    "    MinTrace(method='wls_struct'),\n",
    "    MinTrace(method='wls_var'),\n",
    "    MinTrace(method='mint_shrink'),\n",
    "    # ERM recovers but needs bigger eps\n",
    "    # ERM(method='reg_bu', lambda_reg=None),\n",
    "])\n",
    "reconciled = hrec.reconcile(Y_hat_df=hier_grouped_df_h, Y_df=hier_grouped_df, \n",
    "                            S=S_grouped, tags=tags_grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level</th>\n",
       "      <th>metric</th>\n",
       "      <th>y_model</th>\n",
       "      <th>y_model/BottomUp</th>\n",
       "      <th>y_model/MinTrace_method-ols</th>\n",
       "      <th>y_model/MinTrace_method-wls_struct</th>\n",
       "      <th>y_model/MinTrace_method-wls_var</th>\n",
       "      <th>y_model/MinTrace_method-mint_shrink</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Country</td>\n",
       "      <td>mse</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.514537e-23</td>\n",
       "      <td>5.514537e-23</td>\n",
       "      <td>8.271806e-23</td>\n",
       "      <td>3.970467e-23</td>\n",
       "      <td>3.749885e-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Country</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.425993e-12</td>\n",
       "      <td>7.425993e-12</td>\n",
       "      <td>9.094947e-12</td>\n",
       "      <td>6.301164e-12</td>\n",
       "      <td>6.123631e-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Country/State</td>\n",
       "      <td>mse</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.067575e-24</td>\n",
       "      <td>1.316939e-24</td>\n",
       "      <td>1.185412e-24</td>\n",
       "      <td>1.067979e-24</td>\n",
       "      <td>1.567969e-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Country/State</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.033235e-12</td>\n",
       "      <td>1.147580e-12</td>\n",
       "      <td>1.088766e-12</td>\n",
       "      <td>1.033431e-12</td>\n",
       "      <td>1.252186e-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Country/Purpose</td>\n",
       "      <td>mse</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.280622e-24</td>\n",
       "      <td>3.682462e-24</td>\n",
       "      <td>1.816997e-24</td>\n",
       "      <td>2.081953e-24</td>\n",
       "      <td>1.692058e-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Country/Purpose</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.131646e-12</td>\n",
       "      <td>1.918974e-12</td>\n",
       "      <td>1.347960e-12</td>\n",
       "      <td>1.442898e-12</td>\n",
       "      <td>1.300791e-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Country/State/Region</td>\n",
       "      <td>mse</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.424395e-27</td>\n",
       "      <td>5.732605e-26</td>\n",
       "      <td>2.453493e-26</td>\n",
       "      <td>3.601985e-26</td>\n",
       "      <td>2.967922e-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Country/State/Region</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.923815e-14</td>\n",
       "      <td>2.394286e-13</td>\n",
       "      <td>1.566363e-13</td>\n",
       "      <td>1.897890e-13</td>\n",
       "      <td>1.722766e-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Country/State/Purpose</td>\n",
       "      <td>mse</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.003747e-26</td>\n",
       "      <td>2.074433e-25</td>\n",
       "      <td>1.004139e-25</td>\n",
       "      <td>1.028776e-25</td>\n",
       "      <td>1.034770e-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Country/State/Purpose</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.415538e-13</td>\n",
       "      <td>4.554595e-13</td>\n",
       "      <td>3.168816e-13</td>\n",
       "      <td>3.207454e-13</td>\n",
       "      <td>3.216784e-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Country/State/Region/Purpose</td>\n",
       "      <td>mse</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>8.810919e-27</td>\n",
       "      <td>4.256675e-27</td>\n",
       "      <td>8.126653e-27</td>\n",
       "      <td>7.736350e-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Country/State/Region/Purpose</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>9.386650e-14</td>\n",
       "      <td>6.524320e-14</td>\n",
       "      <td>9.014795e-14</td>\n",
       "      <td>8.795652e-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Overall</td>\n",
       "      <td>mse</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.638445e-25</td>\n",
       "      <td>2.213746e-25</td>\n",
       "      <td>2.490383e-25</td>\n",
       "      <td>1.531210e-25</td>\n",
       "      <td>1.523049e-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Overall</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.047771e-13</td>\n",
       "      <td>4.705047e-13</td>\n",
       "      <td>4.990374e-13</td>\n",
       "      <td>3.913067e-13</td>\n",
       "      <td>3.902626e-13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           level metric  y_model  y_model/BottomUp  \\\n",
       "0                        Country    mse      0.0      5.514537e-23   \n",
       "1                        Country   rmse      0.0      7.425993e-12   \n",
       "2                  Country/State    mse      0.0      1.067575e-24   \n",
       "3                  Country/State   rmse      0.0      1.033235e-12   \n",
       "4                Country/Purpose    mse      0.0      1.280622e-24   \n",
       "5                Country/Purpose   rmse      0.0      1.131646e-12   \n",
       "6           Country/State/Region    mse      0.0      2.424395e-27   \n",
       "7           Country/State/Region   rmse      0.0      4.923815e-14   \n",
       "8          Country/State/Purpose    mse      0.0      2.003747e-26   \n",
       "9          Country/State/Purpose   rmse      0.0      1.415538e-13   \n",
       "10  Country/State/Region/Purpose    mse      0.0      0.000000e+00   \n",
       "11  Country/State/Region/Purpose   rmse      0.0      0.000000e+00   \n",
       "12                       Overall    mse      0.0      1.638445e-25   \n",
       "13                       Overall   rmse      0.0      4.047771e-13   \n",
       "\n",
       "    y_model/MinTrace_method-ols  y_model/MinTrace_method-wls_struct  \\\n",
       "0                  5.514537e-23                        8.271806e-23   \n",
       "1                  7.425993e-12                        9.094947e-12   \n",
       "2                  1.316939e-24                        1.185412e-24   \n",
       "3                  1.147580e-12                        1.088766e-12   \n",
       "4                  3.682462e-24                        1.816997e-24   \n",
       "5                  1.918974e-12                        1.347960e-12   \n",
       "6                  5.732605e-26                        2.453493e-26   \n",
       "7                  2.394286e-13                        1.566363e-13   \n",
       "8                  2.074433e-25                        1.004139e-25   \n",
       "9                  4.554595e-13                        3.168816e-13   \n",
       "10                 8.810919e-27                        4.256675e-27   \n",
       "11                 9.386650e-14                        6.524320e-14   \n",
       "12                 2.213746e-25                        2.490383e-25   \n",
       "13                 4.705047e-13                        4.990374e-13   \n",
       "\n",
       "    y_model/MinTrace_method-wls_var  y_model/MinTrace_method-mint_shrink  \n",
       "0                      3.970467e-23                         3.749885e-23  \n",
       "1                      6.301164e-12                         6.123631e-12  \n",
       "2                      1.067979e-24                         1.567969e-24  \n",
       "3                      1.033431e-12                         1.252186e-12  \n",
       "4                      2.081953e-24                         1.692058e-24  \n",
       "5                      1.442898e-12                         1.300791e-12  \n",
       "6                      3.601985e-26                         2.967922e-26  \n",
       "7                      1.897890e-13                         1.722766e-13  \n",
       "8                      1.028776e-25                         1.034770e-25  \n",
       "9                      3.207454e-13                         3.216784e-13  \n",
       "10                     8.126653e-27                         7.736350e-27  \n",
       "11                     9.014795e-14                         8.795652e-14  \n",
       "12                     1.531210e-25                         1.523049e-25  \n",
       "13                     3.913067e-13                         3.902626e-13  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "def mse(y, y_hat):\n",
    "    return np.mean((y-y_hat)**2)\n",
    "def rmse(y, y_hat):\n",
    "    return np.sqrt(mse(y, y_hat))\n",
    "\n",
    "evaluator = HierarchicalEvaluation([mse, rmse])\n",
    "evaluator.evaluate(Y_hat_df=reconciled.drop(columns='y'), \n",
    "                   Y_test_df=reconciled[['unique_id', 'ds', 'y']], \n",
    "                   tags=tags_grouped,\n",
    "                #    benchmark='y_model',\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# polars\n",
    "import polars as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (14, 8)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>level</th><th>metric</th><th>y_model</th><th>y_model/BottomUp</th><th>y_model/MinTrace_method-ols</th><th>y_model/MinTrace_method-wls_struct</th><th>y_model/MinTrace_method-wls_var</th><th>y_model/MinTrace_method-mint_shrink</th></tr><tr><td>str</td><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;Country&quot;</td><td>&quot;mse-scaled&quot;</td><td>0.0</td><td>5.5145e-23</td><td>5.5145e-23</td><td>8.2718e-23</td><td>3.9705e-23</td><td>3.7499e-23</td></tr><tr><td>&quot;Country&quot;</td><td>&quot;rmse-scaled&quot;</td><td>0.0</td><td>7.4260e-12</td><td>7.4260e-12</td><td>9.0949e-12</td><td>6.3012e-12</td><td>6.1236e-12</td></tr><tr><td>&quot;Country/State&quot;</td><td>&quot;mse-scaled&quot;</td><td>0.0</td><td>1.0676e-24</td><td>1.3169e-24</td><td>1.1854e-24</td><td>1.0680e-24</td><td>1.5680e-24</td></tr><tr><td>&quot;Country/State&quot;</td><td>&quot;rmse-scaled&quot;</td><td>0.0</td><td>1.0332e-12</td><td>1.1476e-12</td><td>1.0888e-12</td><td>1.0334e-12</td><td>1.2522e-12</td></tr><tr><td>&quot;Country/Purpose&quot;</td><td>&quot;mse-scaled&quot;</td><td>0.0</td><td>1.2806e-24</td><td>3.6825e-24</td><td>1.8170e-24</td><td>2.0820e-24</td><td>1.6921e-24</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;Country/State/Purpose&quot;</td><td>&quot;rmse-scaled&quot;</td><td>0.0</td><td>1.4155e-13</td><td>4.5546e-13</td><td>3.1688e-13</td><td>3.2075e-13</td><td>3.2168e-13</td></tr><tr><td>&quot;Country/State/Region/Purpose&quot;</td><td>&quot;mse-scaled&quot;</td><td>0.0</td><td>0.0</td><td>8.8109e-27</td><td>4.2567e-27</td><td>8.1267e-27</td><td>7.7364e-27</td></tr><tr><td>&quot;Country/State/Region/Purpose&quot;</td><td>&quot;rmse-scaled&quot;</td><td>0.0</td><td>0.0</td><td>9.3866e-14</td><td>6.5243e-14</td><td>9.0148e-14</td><td>8.7957e-14</td></tr><tr><td>&quot;Overall&quot;</td><td>&quot;mse-scaled&quot;</td><td>0.0</td><td>1.6384e-25</td><td>2.2137e-25</td><td>2.4904e-25</td><td>1.5312e-25</td><td>1.5230e-25</td></tr><tr><td>&quot;Overall&quot;</td><td>&quot;rmse-scaled&quot;</td><td>0.0</td><td>4.0478e-13</td><td>4.7050e-13</td><td>4.9904e-13</td><td>3.9131e-13</td><td>3.9026e-13</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (14, 8)\n",
       "\n",
       " level       metric      y_model  y_model/Bo  y_model/Mi  y_model/Mi  y_model/M  y_model/M \n",
       " ---         ---         ---      ttomUp      nTrace_met  nTrace_met  inTrace_m  inTrace_m \n",
       " str         str         f64      ---         hod-ols     hod-wls_st  ethod-wls  ethod-min \n",
       "                                  f64         ---                    _va       t_s      \n",
       "                                              f64         ---         ---        ---       \n",
       "                                                          f64         f64        f64       \n",
       "\n",
       " Country     mse-scaled  0.0      5.5145e-23  5.5145e-23  8.2718e-23  3.9705e-2  3.7499e-2 \n",
       "                                                                      3          3         \n",
       " Country     rmse-scale  0.0      7.4260e-12  7.4260e-12  9.0949e-12  6.3012e-1  6.1236e-1 \n",
       "             d                                                        2          2         \n",
       " Country/St  mse-scaled  0.0      1.0676e-24  1.3169e-24  1.1854e-24  1.0680e-2  1.5680e-2 \n",
       " ate                                                                  4          4         \n",
       " Country/St  rmse-scale  0.0      1.0332e-12  1.1476e-12  1.0888e-12  1.0334e-1  1.2522e-1 \n",
       " ate         d                                                        2          2         \n",
       " Country/Pu  mse-scaled  0.0      1.2806e-24  3.6825e-24  1.8170e-24  2.0820e-2  1.6921e-2 \n",
       " rpose                                                                4          4         \n",
       "                                                                                   \n",
       " Country/St  rmse-scale  0.0      1.4155e-13  4.5546e-13  3.1688e-13  3.2075e-1  3.2168e-1 \n",
       " ate/Purpos  d                                                        3          3         \n",
       " e                                                                                         \n",
       " Country/St  mse-scaled  0.0      0.0         8.8109e-27  4.2567e-27  8.1267e-2  7.7364e-2 \n",
       " ate/Region                                                           7          7         \n",
       " /Purpose                                                                                  \n",
       " Country/St  rmse-scale  0.0      0.0         9.3866e-14  6.5243e-14  9.0148e-1  8.7957e-1 \n",
       " ate/Region  d                                                        4          4         \n",
       " /Purpose                                                                                  \n",
       " Overall     mse-scaled  0.0      1.6384e-25  2.2137e-25  2.4904e-25  1.5312e-2  1.5230e-2 \n",
       "                                                                      5          5         \n",
       " Overall     rmse-scale  0.0      4.0478e-13  4.7050e-13  4.9904e-13  3.9131e-1  3.9026e-1 \n",
       "             d                                                        3          3         \n",
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "# polars\n",
    "reconciled_pl = pl.from_pandas(reconciled)\n",
    "evaluator.evaluate(Y_hat_df=reconciled_pl.drop('y'), \n",
    "                   Y_test_df=reconciled_pl[['unique_id', 'ds', 'y']], \n",
    "                   tags=tags_grouped,\n",
    "                   benchmark='y_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level</th>\n",
       "      <th>metric</th>\n",
       "      <th>y_model</th>\n",
       "      <th>y_model/BottomUp</th>\n",
       "      <th>y_model/MinTrace_method-ols</th>\n",
       "      <th>y_model/MinTrace_method-wls_struct</th>\n",
       "      <th>y_model/MinTrace_method-wls_var</th>\n",
       "      <th>y_model/MinTrace_method-mint_shrink</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Country</td>\n",
       "      <td>mase-scaled</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.953168e-15</td>\n",
       "      <td>5.953168e-15</td>\n",
       "      <td>8.268289e-15</td>\n",
       "      <td>5.953168e-15</td>\n",
       "      <td>5.291705e-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Country/State</td>\n",
       "      <td>mase-scaled</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.980839e-15</td>\n",
       "      <td>4.496669e-15</td>\n",
       "      <td>3.618865e-15</td>\n",
       "      <td>3.551700e-15</td>\n",
       "      <td>3.914717e-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Country/Purpose</td>\n",
       "      <td>mase-scaled</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.015436e-15</td>\n",
       "      <td>3.932052e-15</td>\n",
       "      <td>3.050845e-15</td>\n",
       "      <td>3.368266e-15</td>\n",
       "      <td>2.825466e-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Country/State/Region</td>\n",
       "      <td>mase-scaled</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.757523e-16</td>\n",
       "      <td>3.934661e-15</td>\n",
       "      <td>2.423732e-15</td>\n",
       "      <td>3.245747e-15</td>\n",
       "      <td>3.240262e-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Country/State/Purpose</td>\n",
       "      <td>mase-scaled</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.247942e-16</td>\n",
       "      <td>4.600730e-15</td>\n",
       "      <td>2.784470e-15</td>\n",
       "      <td>3.505178e-15</td>\n",
       "      <td>3.788154e-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Country/State/Region/Purpose</td>\n",
       "      <td>mase-scaled</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.048205e-15</td>\n",
       "      <td>2.349369e-15</td>\n",
       "      <td>4.152554e-15</td>\n",
       "      <td>4.260422e-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Overall</td>\n",
       "      <td>mase-scaled</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.590876e-16</td>\n",
       "      <td>4.081333e-15</td>\n",
       "      <td>2.439853e-15</td>\n",
       "      <td>3.927197e-15</td>\n",
       "      <td>4.024848e-15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          level       metric  y_model  y_model/BottomUp  \\\n",
       "0                       Country  mase-scaled      0.0      5.953168e-15   \n",
       "1                 Country/State  mase-scaled      0.0      1.980839e-15   \n",
       "2               Country/Purpose  mase-scaled      0.0      2.015436e-15   \n",
       "3          Country/State/Region  mase-scaled      0.0      2.757523e-16   \n",
       "4         Country/State/Purpose  mase-scaled      0.0      5.247942e-16   \n",
       "5  Country/State/Region/Purpose  mase-scaled      0.0      0.000000e+00   \n",
       "6                       Overall  mase-scaled      0.0      1.590876e-16   \n",
       "\n",
       "   y_model/MinTrace_method-ols  y_model/MinTrace_method-wls_struct  \\\n",
       "0                 5.953168e-15                        8.268289e-15   \n",
       "1                 4.496669e-15                        3.618865e-15   \n",
       "2                 3.932052e-15                        3.050845e-15   \n",
       "3                 3.934661e-15                        2.423732e-15   \n",
       "4                 4.600730e-15                        2.784470e-15   \n",
       "5                 4.048205e-15                        2.349369e-15   \n",
       "6                 4.081333e-15                        2.439853e-15   \n",
       "\n",
       "   y_model/MinTrace_method-wls_var  y_model/MinTrace_method-mint_shrink  \n",
       "0                     5.953168e-15                         5.291705e-15  \n",
       "1                     3.551700e-15                         3.914717e-15  \n",
       "2                     3.368266e-15                         2.825466e-15  \n",
       "3                     3.245747e-15                         3.240262e-15  \n",
       "4                     3.505178e-15                         3.788154e-15  \n",
       "5                     4.152554e-15                         4.260422e-15  \n",
       "6                     3.927197e-15                         4.024848e-15  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "def mase(y, y_hat, y_insample, seasonality=4):\n",
    "    errors = np.mean(np.abs(y - y_hat), axis=1)\n",
    "    scale = np.mean(np.abs(y_insample[:, seasonality:] - y_insample[:, :-seasonality]), axis=1)\n",
    "    return np.mean(errors / scale)\n",
    "\n",
    "evaluator = HierarchicalEvaluation([mase])\n",
    "evaluator.evaluate(Y_hat_df=reconciled.drop(columns='y'), \n",
    "                   Y_test_df=reconciled[['unique_id', 'ds', 'y']], \n",
    "                   tags=tags_grouped,\n",
    "                   Y_df=hier_grouped_df,\n",
    "                   benchmark='y_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (7, 8)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>level</th><th>metric</th><th>y_model</th><th>y_model/BottomUp</th><th>y_model/MinTrace_method-ols</th><th>y_model/MinTrace_method-wls_struct</th><th>y_model/MinTrace_method-wls_var</th><th>y_model/MinTrace_method-mint_shrink</th></tr><tr><td>str</td><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;Country&quot;</td><td>&quot;mase-scaled&quot;</td><td>0.0</td><td>5.9532e-15</td><td>5.9532e-15</td><td>8.2683e-15</td><td>5.9532e-15</td><td>5.2917e-15</td></tr><tr><td>&quot;Country/State&quot;</td><td>&quot;mase-scaled&quot;</td><td>0.0</td><td>1.9808e-15</td><td>4.4967e-15</td><td>3.6189e-15</td><td>3.5517e-15</td><td>3.9147e-15</td></tr><tr><td>&quot;Country/Purpose&quot;</td><td>&quot;mase-scaled&quot;</td><td>0.0</td><td>2.0154e-15</td><td>3.9321e-15</td><td>3.0508e-15</td><td>3.3683e-15</td><td>2.8255e-15</td></tr><tr><td>&quot;Country/State/Region&quot;</td><td>&quot;mase-scaled&quot;</td><td>0.0</td><td>2.7575e-16</td><td>3.9347e-15</td><td>2.4237e-15</td><td>3.2457e-15</td><td>3.2403e-15</td></tr><tr><td>&quot;Country/State/Purpose&quot;</td><td>&quot;mase-scaled&quot;</td><td>0.0</td><td>5.2479e-16</td><td>4.6007e-15</td><td>2.7845e-15</td><td>3.5052e-15</td><td>3.7882e-15</td></tr><tr><td>&quot;Country/State/Region/Purpose&quot;</td><td>&quot;mase-scaled&quot;</td><td>0.0</td><td>0.0</td><td>4.0482e-15</td><td>2.3494e-15</td><td>4.1526e-15</td><td>4.2604e-15</td></tr><tr><td>&quot;Overall&quot;</td><td>&quot;mase-scaled&quot;</td><td>0.0</td><td>1.5909e-16</td><td>4.0813e-15</td><td>2.4399e-15</td><td>3.9272e-15</td><td>4.0248e-15</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (7, 8)\n",
       "\n",
       " level       metric      y_model  y_model/Bo  y_model/Mi  y_model/Mi  y_model/M  y_model/M \n",
       " ---         ---         ---      ttomUp      nTrace_met  nTrace_met  inTrace_m  inTrace_m \n",
       " str         str         f64      ---         hod-ols     hod-wls_st  ethod-wls  ethod-min \n",
       "                                  f64         ---                    _va       t_s      \n",
       "                                              f64         ---         ---        ---       \n",
       "                                                          f64         f64        f64       \n",
       "\n",
       " Country     mase-scale  0.0      5.9532e-15  5.9532e-15  8.2683e-15  5.9532e-1  5.2917e-1 \n",
       "             d                                                        5          5         \n",
       " Country/St  mase-scale  0.0      1.9808e-15  4.4967e-15  3.6189e-15  3.5517e-1  3.9147e-1 \n",
       " ate         d                                                        5          5         \n",
       " Country/Pu  mase-scale  0.0      2.0154e-15  3.9321e-15  3.0508e-15  3.3683e-1  2.8255e-1 \n",
       " rpose       d                                                        5          5         \n",
       " Country/St  mase-scale  0.0      2.7575e-16  3.9347e-15  2.4237e-15  3.2457e-1  3.2403e-1 \n",
       " ate/Region  d                                                        5          5         \n",
       " Country/St  mase-scale  0.0      5.2479e-16  4.6007e-15  2.7845e-15  3.5052e-1  3.7882e-1 \n",
       " ate/Purpos  d                                                        5          5         \n",
       " e                                                                                         \n",
       " Country/St  mase-scale  0.0      0.0         4.0482e-15  2.3494e-15  4.1526e-1  4.2604e-1 \n",
       " ate/Region  d                                                        5          5         \n",
       " /Purpose                                                                                  \n",
       " Overall     mase-scale  0.0      1.5909e-16  4.0813e-15  2.4399e-15  3.9272e-1  4.0248e-1 \n",
       "             d                                                        5          5         \n",
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "# polars\n",
    "reconciled_pl = pl.from_pandas(reconciled)\n",
    "hier_grouped_df_pl = pl.from_pandas(hier_grouped_df)\n",
    "\n",
    "evaluator.evaluate(Y_hat_df=reconciled_pl.drop('y'), \n",
    "                   Y_test_df=reconciled_pl[['unique_id', 'ds', 'y']], \n",
    "                   tags=tags_grouped,\n",
    "                   Y_df=hier_grouped_df_pl,\n",
    "                   benchmark='y_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level</th>\n",
       "      <th>metric</th>\n",
       "      <th>y_model</th>\n",
       "      <th>y_model/BottomUp</th>\n",
       "      <th>y_model/MinTrace_method-ols</th>\n",
       "      <th>y_model/MinTrace_method-wls_struct</th>\n",
       "      <th>y_model/MinTrace_method-wls_var</th>\n",
       "      <th>y_model/MinTrace_method-mint_shrink</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Country</td>\n",
       "      <td>mase-scaled</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.190634e-14</td>\n",
       "      <td>3.968779e-15</td>\n",
       "      <td>1.190634e-14</td>\n",
       "      <td>7.937557e-15</td>\n",
       "      <td>3.968779e-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Country/State</td>\n",
       "      <td>mase-scaled</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.937913e-15</td>\n",
       "      <td>4.988451e-15</td>\n",
       "      <td>2.785879e-15</td>\n",
       "      <td>3.790787e-15</td>\n",
       "      <td>3.077111e-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Country/Purpose</td>\n",
       "      <td>mase-scaled</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.812372e-16</td>\n",
       "      <td>5.812527e-15</td>\n",
       "      <td>1.246659e-15</td>\n",
       "      <td>4.596299e-15</td>\n",
       "      <td>2.190113e-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Country/State/Region</td>\n",
       "      <td>mase-scaled</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.531390e-16</td>\n",
       "      <td>3.806263e-15</td>\n",
       "      <td>1.876343e-15</td>\n",
       "      <td>3.213134e-15</td>\n",
       "      <td>3.428183e-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Country/State/Purpose</td>\n",
       "      <td>mase-scaled</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.054006e-16</td>\n",
       "      <td>5.302327e-15</td>\n",
       "      <td>2.481178e-15</td>\n",
       "      <td>4.302359e-15</td>\n",
       "      <td>3.255292e-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Country/State/Region/Purpose</td>\n",
       "      <td>mase-scaled</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.312915e-15</td>\n",
       "      <td>2.098706e-15</td>\n",
       "      <td>4.913715e-15</td>\n",
       "      <td>4.313319e-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Overall</td>\n",
       "      <td>mase-scaled</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.448140e-16</td>\n",
       "      <td>4.322831e-15</td>\n",
       "      <td>2.115733e-15</td>\n",
       "      <td>4.546570e-15</td>\n",
       "      <td>4.031309e-15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          level       metric  y_model  y_model/BottomUp  \\\n",
       "0                       Country  mase-scaled      0.0      1.190634e-14   \n",
       "1                 Country/State  mase-scaled      0.0      1.937913e-15   \n",
       "2               Country/Purpose  mase-scaled      0.0      4.812372e-16   \n",
       "3          Country/State/Region  mase-scaled      0.0      2.531390e-16   \n",
       "4         Country/State/Purpose  mase-scaled      0.0      4.054006e-16   \n",
       "5  Country/State/Region/Purpose  mase-scaled      0.0      0.000000e+00   \n",
       "6                       Overall  mase-scaled      0.0      1.448140e-16   \n",
       "\n",
       "   y_model/MinTrace_method-ols  y_model/MinTrace_method-wls_struct  \\\n",
       "0                 3.968779e-15                        1.190634e-14   \n",
       "1                 4.988451e-15                        2.785879e-15   \n",
       "2                 5.812527e-15                        1.246659e-15   \n",
       "3                 3.806263e-15                        1.876343e-15   \n",
       "4                 5.302327e-15                        2.481178e-15   \n",
       "5                 4.312915e-15                        2.098706e-15   \n",
       "6                 4.322831e-15                        2.115733e-15   \n",
       "\n",
       "   y_model/MinTrace_method-wls_var  y_model/MinTrace_method-mint_shrink  \n",
       "0                     7.937557e-15                         3.968779e-15  \n",
       "1                     3.790787e-15                         3.077111e-15  \n",
       "2                     4.596299e-15                         2.190113e-15  \n",
       "3                     3.213134e-15                         3.428183e-15  \n",
       "4                     4.302359e-15                         3.255292e-15  \n",
       "5                     4.913715e-15                         4.313319e-15  \n",
       "6                     4.546570e-15                         4.031309e-15  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "# test work for h=1\n",
    "evaluator = HierarchicalEvaluation([mase])\n",
    "evaluator.evaluate(Y_hat_df=reconciled.groupby('unique_id').tail(1).drop(columns='y'), \n",
    "                   Y_test_df=reconciled.groupby('unique_id').tail(1)[['unique_id', 'ds', 'y']], \n",
    "                   tags=tags_grouped,\n",
    "                   Y_df=hier_grouped_df,\n",
    "                   benchmark='y_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (7, 8)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>level</th><th>metric</th><th>y_model</th><th>y_model/BottomUp</th><th>y_model/MinTrace_method-ols</th><th>y_model/MinTrace_method-wls_struct</th><th>y_model/MinTrace_method-wls_var</th><th>y_model/MinTrace_method-mint_shrink</th></tr><tr><td>str</td><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;Country&quot;</td><td>&quot;mase-scaled&quot;</td><td>0.0</td><td>1.1906e-14</td><td>3.9688e-15</td><td>1.1906e-14</td><td>7.9376e-15</td><td>3.9688e-15</td></tr><tr><td>&quot;Country/State&quot;</td><td>&quot;mase-scaled&quot;</td><td>0.0</td><td>1.9379e-15</td><td>4.9885e-15</td><td>2.7859e-15</td><td>3.7908e-15</td><td>3.0771e-15</td></tr><tr><td>&quot;Country/Purpose&quot;</td><td>&quot;mase-scaled&quot;</td><td>0.0</td><td>4.8124e-16</td><td>5.8125e-15</td><td>1.2467e-15</td><td>4.5963e-15</td><td>2.1901e-15</td></tr><tr><td>&quot;Country/State/Region&quot;</td><td>&quot;mase-scaled&quot;</td><td>0.0</td><td>2.5314e-16</td><td>3.8063e-15</td><td>1.8763e-15</td><td>3.2131e-15</td><td>3.4282e-15</td></tr><tr><td>&quot;Country/State/Purpose&quot;</td><td>&quot;mase-scaled&quot;</td><td>0.0</td><td>4.0540e-16</td><td>5.3023e-15</td><td>2.4812e-15</td><td>4.3024e-15</td><td>3.2553e-15</td></tr><tr><td>&quot;Country/State/Region/Purpose&quot;</td><td>&quot;mase-scaled&quot;</td><td>0.0</td><td>0.0</td><td>4.3129e-15</td><td>2.0987e-15</td><td>4.9137e-15</td><td>4.3133e-15</td></tr><tr><td>&quot;Overall&quot;</td><td>&quot;mase-scaled&quot;</td><td>0.0</td><td>1.4481e-16</td><td>4.3228e-15</td><td>2.1157e-15</td><td>4.5466e-15</td><td>4.0313e-15</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (7, 8)\n",
       "\n",
       " level       metric      y_model  y_model/Bo  y_model/Mi  y_model/Mi  y_model/M  y_model/M \n",
       " ---         ---         ---      ttomUp      nTrace_met  nTrace_met  inTrace_m  inTrace_m \n",
       " str         str         f64      ---         hod-ols     hod-wls_st  ethod-wls  ethod-min \n",
       "                                  f64         ---                    _va       t_s      \n",
       "                                              f64         ---         ---        ---       \n",
       "                                                          f64         f64        f64       \n",
       "\n",
       " Country     mase-scale  0.0      1.1906e-14  3.9688e-15  1.1906e-14  7.9376e-1  3.9688e-1 \n",
       "             d                                                        5          5         \n",
       " Country/St  mase-scale  0.0      1.9379e-15  4.9885e-15  2.7859e-15  3.7908e-1  3.0771e-1 \n",
       " ate         d                                                        5          5         \n",
       " Country/Pu  mase-scale  0.0      4.8124e-16  5.8125e-15  1.2467e-15  4.5963e-1  2.1901e-1 \n",
       " rpose       d                                                        5          5         \n",
       " Country/St  mase-scale  0.0      2.5314e-16  3.8063e-15  1.8763e-15  3.2131e-1  3.4282e-1 \n",
       " ate/Region  d                                                        5          5         \n",
       " Country/St  mase-scale  0.0      4.0540e-16  5.3023e-15  2.4812e-15  4.3024e-1  3.2553e-1 \n",
       " ate/Purpos  d                                                        5          5         \n",
       " e                                                                                         \n",
       " Country/St  mase-scale  0.0      0.0         4.3129e-15  2.0987e-15  4.9137e-1  4.3133e-1 \n",
       " ate/Region  d                                                        5          5         \n",
       " /Purpose                                                                                  \n",
       " Overall     mase-scale  0.0      1.4481e-16  4.3228e-15  2.1157e-15  4.5466e-1  4.0313e-1 \n",
       "             d                                                        5          5         \n",
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "# polars\n",
    "# test work for h=1\n",
    "evaluator.evaluate(Y_hat_df=reconciled_pl.group_by('unique_id').tail(1).drop('y'), \n",
    "                   Y_test_df=reconciled_pl.group_by('unique_id').tail(1)[['unique_id', 'ds', 'y']], \n",
    "                   tags=tags_grouped,\n",
    "                   Y_df=hier_grouped_df_pl,\n",
    "                   benchmark='y_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from statsforecast.core import StatsForecast\n",
    "from statsforecast.models import AutoETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "# Load TourismSmall dataset\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/Nixtla/transfer-learning-time-series/main/datasets/tourism.csv')\n",
    "df = df.rename({'Trips': 'y', 'Quarter': 'ds'}, axis=1)\n",
    "df.insert(0, 'Country', 'Australia')\n",
    "qs = df['ds'].str.replace(r'(\\d+) (Q\\d)', r'\\1-\\2', regex=True)\n",
    "df['ds'] = pd.PeriodIndex(qs, freq='Q').to_timestamp()\n",
    "\n",
    "# Create hierarchical seires based on geographic levels and purpose\n",
    "# And Convert quarterly ds string to pd.datetime format\n",
    "hierarchy_levels = [['Country'],\n",
    "                    ['Country', 'State'], \n",
    "                    ['Country', 'Purpose'], \n",
    "                    ['Country', 'State', 'Region'], \n",
    "                    ['Country', 'State', 'Purpose'], \n",
    "                    ['Country', 'State', 'Region', 'Purpose']]\n",
    "\n",
    "Y_df, S_df, tags = aggregate(df=df, spec=hierarchy_levels)\n",
    "\n",
    "# Split train/test sets\n",
    "Y_test_df  = Y_df.groupby('unique_id').tail(8)\n",
    "Y_train_df = Y_df.drop(Y_test_df.index)\n",
    "\n",
    "# Compute base auto-ETS predictions\n",
    "# Careful identifying correct data freq, this data quarterly 'Q'\n",
    "fcst = StatsForecast(models=[AutoETS(season_length=4, model='ZZA')], freq='QS', n_jobs=-1)\n",
    "Y_hat_df = fcst.forecast(df=Y_train_df, h=8, fitted=True)\n",
    "Y_fitted_df = fcst.forecast_fitted_values()\n",
    "\n",
    "def rmse(y, y_hat):\n",
    "    return np.mean(np.sqrt(np.mean((y-y_hat)**2, axis=1)))\n",
    "\n",
    "def mase(y, y_hat, y_insample, seasonality=4):\n",
    "    errors = np.mean(np.abs(y - y_hat), axis=1)\n",
    "    scale = np.mean(np.abs(y_insample[:, seasonality:] - y_insample[:, :-seasonality]), axis=1)\n",
    "    return np.mean(errors / scale)\n",
    "\n",
    "reconcilers = [\n",
    "                BottomUp(),\n",
    "                MinTrace(method='ols'),\n",
    "                MinTrace(method='mint_shrink'),\n",
    "               ]\n",
    "hrec = HierarchicalReconciliation(reconcilers=reconcilers)\n",
    "Y_rec_df = hrec.reconcile(Y_hat_df=Y_hat_df, \n",
    "                          Y_df=Y_fitted_df,\n",
    "                          S=S_df, tags=tags)\n",
    "\n",
    "# Evaluate\n",
    "eval_tags = {}\n",
    "eval_tags['Total'] = tags['Country']\n",
    "eval_tags['Purpose'] = tags['Country/Purpose']\n",
    "eval_tags['State'] = tags['Country/State']\n",
    "eval_tags['Regions'] = tags['Country/State/Region']\n",
    "eval_tags['Bottom'] = tags['Country/State/Region/Purpose']\n",
    "\n",
    "evaluator = HierarchicalEvaluation(evaluators=[mase, rmse])\n",
    "evaluation = evaluator.evaluate(\n",
    "        Y_hat_df=Y_rec_df, Y_test_df=Y_test_df,\n",
    "        tags=eval_tags, Y_df=Y_train_df\n",
    ")\n",
    "numeric_cols = evaluation.select_dtypes(include=\"number\").columns\n",
    "evaluation[numeric_cols] = evaluation[numeric_cols].map('{:.2f}'.format).astype(np.float64)\n",
    "\n",
    "evaluation_check = pd.DataFrame({\n",
    "    'level': ['Total', 'Purpose', 'State', 'Regions', 'Bottom', 'Overall'],\n",
    "    'metric': 6 * ['mase'],\n",
    "    'AutoETS': [1.59, 1.32, 1.39, 1.12, 0.98, 1.02],\n",
    "    'AutoETS/BottomUp': [3.16, 2.28, 1.90, 1.19, 0.98, 1.06],\n",
    "})\n",
    "\n",
    "pd.testing.assert_frame_equal(evaluation.query(\"metric == 'mase'\")[[\"level\", \"metric\", \"AutoETS\", \"AutoETS/BottomUp\"]].reset_index(drop=True), evaluation_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# polars\n",
    "\n",
    "# Test if polars gives equal results to pandas\n",
    "df_pl = pl.from_pandas(df)\n",
    "Y_df, S_df, tags = aggregate(df=df_pl, spec=hierarchy_levels)\n",
    "\n",
    "# Split train/test sets\n",
    "Y_test_df  = Y_df.group_by('unique_id').tail(8)\n",
    "Y_train_df = Y_df.group_by('unique_id').head(72)\n",
    "\n",
    "# Compute base auto-ETS predictions\n",
    "# Careful identifying correct data freq, this data quarterly 'Q'\n",
    "fcst = StatsForecast(models=[AutoETS(season_length=4, model='ZZA')], freq='1q', n_jobs=-1)\n",
    "Y_hat_df_pl = fcst.forecast(df=Y_train_df, h=8, fitted=True)\n",
    "Y_fitted_df = fcst.forecast_fitted_values()\n",
    "\n",
    "# Reconcile the base predictions\n",
    "reconcilers = [BottomUp(),\n",
    "                MinTrace(method='ols'),               \n",
    "               MinTrace(method='mint_shrink')]\n",
    "hrec = HierarchicalReconciliation(reconcilers=reconcilers)\n",
    "Y_rec_df_pl = hrec.reconcile(Y_hat_df=Y_hat_df_pl, \n",
    "                          Y_df=Y_fitted_df,\n",
    "                          S=S_df, tags=tags)\n",
    "\n",
    "# Evaluate\n",
    "evaluator = HierarchicalEvaluation(evaluators=[mase, rmse])\n",
    "evaluation_pl = evaluator.evaluate(\n",
    "        Y_hat_df=Y_rec_df_pl, Y_test_df=Y_test_df,\n",
    "        tags=eval_tags, Y_df=Y_train_df\n",
    ")\n",
    "evaluation_pl = evaluation_pl.with_columns(pl.col(numeric_cols).round(2).cast(pl.Float64))\n",
    "\n",
    "# Check if polars gives identical results as pandas\n",
    "pd.testing.assert_frame_equal(Y_hat_df_pl.to_pandas(), Y_hat_df)\n",
    "pd.testing.assert_frame_equal(Y_rec_df_pl.to_pandas(), Y_rec_df)\n",
    "pd.testing.assert_frame_equal(evaluation_pl.to_pandas(), evaluation)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "- [Gneiting, Tilmann, and Adrian E. Raftery. (2007). \n",
    "\\\"Strictly proper scoring rules, prediction and estimation\\\". \n",
    "Journal of the American Statistical Association.](https://sites.stat.washington.edu/raftery/Research/PDF/Gneiting2007jasa.pdf)\n",
    "- [Gneiting, Tilmann. (2011). \\\"Quantiles as optimal point forecasts\\\". \n",
    "International Journal of Forecasting.](https://www.sciencedirect.com/science/article/pii/S0169207010000063)\n",
    "- [Spyros Makridakis, Evangelos Spiliotis, Vassilios Assimakopoulos, Zhi Chen, Anil Gaba, Ilia Tsetlin, Robert L. Winkler. (2022). \n",
    "\\\"The M5 uncertainty competition: Results, findings and conclusions\\\". \n",
    "International Journal of Forecasting.](https://www.sciencedirect.com/science/article/pii/S0169207021001722)\n",
    "- [Anastasios Panagiotelis, Puwasala Gamakumara, George Athanasopoulos, Rob J. Hyndman. (2022). \n",
    "\\\"Probabilistic forecast reconciliation: Properties, evaluation and score optimisation\\\". \n",
    "European Journal of Operational Research.](https://www.sciencedirect.com/science/article/pii/S0377221722006087)\n",
    "- [Syama Sundar Rangapuram, Lucien D Werner, Konstantinos Benidis, Pedro Mercado, Jan Gasthaus, Tim Januschowski. (2021). \n",
    "\\\"End-to-End Learning of Coherent Probabilistic Forecasts for Hierarchical Time Series\\\". \n",
    "Proceedings of the 38th International Conference on Machine Learning (ICML).](https://proceedings.mlr.press/v139/rangapuram21a.html)\n",
    "- [Kin G. Olivares, O. Nganba Meetei, Ruijun Ma, Rohan Reddy, Mengfei Cao, Lee Dicker (2022). \n",
    "\"Probabilistic Hierarchical Forecasting with Deep Poisson Mixtures\". \n",
    "Submitted to the International Journal Forecasting, Working paper available at arxiv.](https://arxiv.org/pdf/2110.13179.pdf)\n",
    "- [Makridakis, S., Spiliotis E., and Assimakopoulos V. (2022). \n",
    "\"M5 Accuracy Competition: Results, Findings, and Conclusions.\",\n",
    "International Journal of Forecasting, Volume 38, Issue 4.](https://www.sciencedirect.com/science/article/pii/S0169207021001874)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
