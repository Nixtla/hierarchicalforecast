{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconciliation Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Large collections of time series organized into structures at different aggregation levels often require their forecasts to follow their aggregation constraints, which poses the challenge of creating novel algorithms capable of coherent forecasts. <br><br> The `HierarchicalForecast` package provides the most comprehensive collection of Python implementations of hierarchical forecasting algorithms that follow classic hierarchical reconciliation. All the methods have a `reconcile` function capable of reconcile base forecasts using `numpy` arrays.\n",
    "\n",
    "Most reconciliation methods can be described by the following convenient linear algebra notation: \n",
    "\n",
    "$$\\tilde{\\mathbf{y}}_{[a,b],\\tau} = \\mathbf{S}_{[a,b][b]} \\mathbf{P}_{[b][a,b]} \\hat{\\mathbf{y}}_{[a,b],\\tau}$$\n",
    "\n",
    "where $a, b$ represent the aggregate and bottom levels, $\\mathbf{S}_{[a,b][b]}$ contains the hierarchical aggregation constraints, and $\\mathbf{P}_{[b][a,b]}$ varies across \n",
    "reconciliation methods. The reconciled predictions are $\\tilde{\\mathbf{y}}_{[a,b],\\tau}$, and the base predictions $\\hat{\\mathbf{y}}_{[a,b],\\tau}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import warnings\n",
    "from collections import OrderedDict\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from copy import deepcopy\n",
    "from typing import Optional, Union\n",
    "\n",
    "import clarabel\n",
    "import numpy as np\n",
    "from quadprog import solve_qp\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from hierarchicalforecast.probabilistic_methods import PERMBU, Bootstrap, Normality\n",
    "from hierarchicalforecast.utils import (\n",
    "    _construct_adjacency_matrix,\n",
    "    _is_strictly_hierarchical,\n",
    "    _lasso,\n",
    "    _ma_cov,\n",
    "    _shrunk_covariance_schaferstrimmer_no_nans,\n",
    "    _shrunk_covariance_schaferstrimmer_with_nans,\n",
    "    is_strictly_hierarchical,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from fastcore.test import ExceptionExpected, test_close, test_eq, test_fail\n",
    "from nbdev.showdoc import add_docs, show_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "class HReconciler:\n",
    "    fitted = False\n",
    "    is_sparse_method = False\n",
    "    insample = False\n",
    "    P = None\n",
    "    sampler = None\n",
    "\n",
    "    def _get_sampler(\n",
    "        self,\n",
    "        intervals_method,\n",
    "        S,\n",
    "        P,\n",
    "        y_hat,\n",
    "        y_insample,\n",
    "        y_hat_insample,\n",
    "        W,\n",
    "        sigmah,\n",
    "        num_samples,\n",
    "        seed,\n",
    "        tags,\n",
    "    ):\n",
    "        if intervals_method == \"normality\":\n",
    "            sampler = Normality(S=S, P=P, y_hat=y_hat, W=W, sigmah=sigmah, seed=seed)\n",
    "        elif intervals_method == \"permbu\":\n",
    "            sampler = PERMBU(\n",
    "                S=S,\n",
    "                P=P,\n",
    "                y_hat=(S @ (P @ y_hat)),\n",
    "                tags=tags,\n",
    "                y_insample=y_insample,\n",
    "                y_hat_insample=y_hat_insample,\n",
    "                sigmah=sigmah,\n",
    "                num_samples=num_samples,\n",
    "                seed=seed,\n",
    "            )\n",
    "        elif intervals_method == \"bootstrap\":\n",
    "            sampler = Bootstrap(\n",
    "                S=S,\n",
    "                P=P,\n",
    "                y_hat=y_hat,\n",
    "                y_insample=y_insample,\n",
    "                y_hat_insample=y_hat_insample,\n",
    "                num_samples=num_samples,\n",
    "                seed=seed,\n",
    "            )\n",
    "        else:\n",
    "            sampler = None\n",
    "        return sampler\n",
    "\n",
    "    def _reconcile(\n",
    "        self,\n",
    "        S: np.ndarray,\n",
    "        P: np.ndarray,\n",
    "        y_hat: np.ndarray,\n",
    "        SP: np.ndarray = None,\n",
    "        level: Optional[list[int]] = None,\n",
    "        sampler: Optional[Union[Normality, PERMBU, Bootstrap]] = None,\n",
    "    ):\n",
    "\n",
    "        # Mean reconciliation\n",
    "        res = {\"mean\": (S @ (P @ y_hat))}\n",
    "\n",
    "        # Probabilistic reconciliation\n",
    "        if (level is not None) and (sampler is not None):\n",
    "            # Update results dictionary within\n",
    "            # Vectorized quantiles\n",
    "            quantiles = np.concatenate(\n",
    "                [[(100 - lv) / 200, ((100 - lv) / 200) + lv / 100] for lv in level]\n",
    "            )\n",
    "            quantiles = np.sort(quantiles)\n",
    "            res = sampler.get_prediction_quantiles(res, quantiles)\n",
    "\n",
    "        return res\n",
    "\n",
    "    def predict(\n",
    "        self, S: np.ndarray, y_hat: np.ndarray, level: Optional[list[int]] = None\n",
    "    ):\n",
    "        \"\"\"Predict using reconciler.\n",
    "\n",
    "        Predict using fitted mean and probabilistic reconcilers.\n",
    "\n",
    "        **Parameters:**<br>\n",
    "        `S`: Summing matrix of size (`base`, `bottom`).<br>\n",
    "        `y_hat`: Forecast values of size (`base`, `horizon`).<br>\n",
    "        `level`: float list 0-100, confidence levels for prediction intervals.<br>\n",
    "\n",
    "        **Returns:**<br>\n",
    "        `y_tilde`: Reconciliated predictions.\n",
    "        \"\"\"\n",
    "        if not self.fitted:\n",
    "            raise Exception(\"This model instance is not fitted yet, Call fit method.\")\n",
    "\n",
    "        return self._reconcile(\n",
    "            S=S, P=self.P, y_hat=y_hat, sampler=self.sampler, level=level\n",
    "        )\n",
    "\n",
    "    def sample(self, num_samples: int):\n",
    "        \"\"\"Sample probabilistic coherent distribution.\n",
    "\n",
    "        Generates n samples from a probabilistic coherent distribution.\n",
    "        The method uses fitted mean and probabilistic reconcilers, defined by\n",
    "        the `intervals_method` selected during the reconciler's\n",
    "        instantiation. Currently available: `normality`, `bootstrap`, `permbu`.\n",
    "\n",
    "        **Parameters:**<br>\n",
    "        `num_samples`: int, number of samples generated from coherent distribution.<br>\n",
    "\n",
    "        **Returns:**<br>\n",
    "        `samples`: Coherent samples of size (`num_series`, `horizon`, `num_samples`).\n",
    "        \"\"\"\n",
    "        if not self.fitted:\n",
    "            raise Exception(\"This model instance is not fitted yet, Call fit method.\")\n",
    "        if self.sampler is None:\n",
    "            raise ValueError(\n",
    "                \"This model instance does not have sampler. Call fit with `intervals_method`.\"\n",
    "            )\n",
    "\n",
    "        samples = self.sampler.get_samples(num_samples=num_samples)\n",
    "        return samples\n",
    "\n",
    "    def fit(self, *args, **kwargs):\n",
    "\n",
    "        raise NotImplementedError(\"This method is not implemented yet.\")\n",
    "\n",
    "    def fit_predict(self, *args, **kwargs):\n",
    "\n",
    "        raise NotImplementedError(\"This method is not implemented yet.\")\n",
    "\n",
    "    __call__ = fit_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Bottom-Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BottomUp(HReconciler):\n",
    "    \"\"\"Bottom Up Reconciliation Class.\n",
    "    The most basic hierarchical reconciliation is performed using an Bottom-Up strategy. It was proposed for\n",
    "    the first time by Orcutt in 1968.\n",
    "    The corresponding hierarchical \\\"projection\\\" matrix is defined as:\n",
    "    $$\\mathbf{P}_{\\\\text{BU}} = [\\mathbf{0}_{\\mathrm{[b],[a]}}\\;|\\;\\mathbf{I}_{\\mathrm{[b][b]}}]$$\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    None\n",
    "\n",
    "    **References:**<br>\n",
    "    - [Orcutt, G.H., Watts, H.W., & Edwards, J.B.(1968). \\\"Data aggregation and information loss\\\". The American\n",
    "    Economic Review, 58 , 773(787)](http://www.jstor.org/stable/1815532).\n",
    "    \"\"\"\n",
    "\n",
    "    insample = False\n",
    "\n",
    "    def _get_PW_matrices(self, S, idx_bottom):\n",
    "        n_hiers, n_bottom = S.shape\n",
    "        P = np.eye(n_bottom, n_hiers, n_hiers - n_bottom, np.float64)\n",
    "        if getattr(self, \"intervals_method\", False) is None:\n",
    "            W = None\n",
    "        else:\n",
    "            W = np.eye(n_hiers, dtype=np.float64)\n",
    "        return P, W\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        S: np.ndarray,\n",
    "        y_hat: np.ndarray,\n",
    "        idx_bottom: np.ndarray,\n",
    "        y_insample: Optional[np.ndarray] = None,\n",
    "        y_hat_insample: Optional[np.ndarray] = None,\n",
    "        sigmah: Optional[np.ndarray] = None,\n",
    "        intervals_method: Optional[str] = None,\n",
    "        num_samples: Optional[int] = None,\n",
    "        seed: Optional[int] = None,\n",
    "        tags: Optional[dict[str, np.ndarray]] = None,\n",
    "    ):\n",
    "        \"\"\"Bottom Up Fit Method.\n",
    "\n",
    "        **Parameters:**<br>\n",
    "        `S`: Summing matrix of size (`base`, `bottom`).<br>\n",
    "        `y_hat`: Forecast values of size (`base`, `horizon`).<br>\n",
    "        `idx_bottom`: Indices corresponding to the bottom level of `S`, size (`bottom`).<br>\n",
    "        `y_insample`: In-sample values of size (`base`, `horizon`).<br>\n",
    "        `y_hat_insample`: In-sample forecast values of size (`base`, `horizon`).<br>\n",
    "        `sigmah`: Estimated standard deviation of the conditional marginal distribution.<br>        \n",
    "        `intervals_method`: Sampler for prediction intervals, one of `normality`, `bootstrap`, `permbu`.<br>\n",
    "        `num_samples`: Number of samples for probabilistic coherent distribution.<br>\n",
    "        `seed`: Seed for reproducibility.<br>\n",
    "        `**sampler_kwargs`: Coherent sampler instantiation arguments.<br>\n",
    "\n",
    "        **Returns:**<br>\n",
    "        `self`: object, fitted reconciler.\n",
    "        \"\"\"\n",
    "        self.intervals_method = intervals_method\n",
    "        self.P, self.W = self._get_PW_matrices(S=S, idx_bottom=idx_bottom)\n",
    "        self.sampler = self._get_sampler(\n",
    "            S=S,\n",
    "            P=self.P,\n",
    "            W=self.W,\n",
    "            y_hat=y_hat,\n",
    "            y_insample=y_insample,\n",
    "            y_hat_insample=y_hat_insample,\n",
    "            sigmah=sigmah,\n",
    "            intervals_method=intervals_method,\n",
    "            num_samples=num_samples,\n",
    "            seed=seed,\n",
    "            tags=tags,\n",
    "        )\n",
    "        self.fitted = True\n",
    "        return self\n",
    "\n",
    "    def fit_predict(\n",
    "        self,\n",
    "        S: np.ndarray,\n",
    "        y_hat: np.ndarray,\n",
    "        idx_bottom: np.ndarray,\n",
    "        y_insample: Optional[np.ndarray] = None,\n",
    "        y_hat_insample: Optional[np.ndarray] = None,\n",
    "        sigmah: Optional[np.ndarray] = None,\n",
    "        level: Optional[list[int]] = None,\n",
    "        intervals_method: Optional[str] = None,\n",
    "        num_samples: Optional[int] = None,\n",
    "        seed: Optional[int] = None,\n",
    "        tags: Optional[dict[str, np.ndarray]] = None,\n",
    "    ):\n",
    "        \"\"\"BottomUp Reconciliation Method.\n",
    "\n",
    "        **Parameters:**<br>\n",
    "        `S`: Summing matrix of size (`base`, `bottom`).<br>\n",
    "        `y_hat`: Forecast values of size (`base`, `horizon`).<br>\n",
    "        `idx_bottom`: Indices corresponding to the bottom level of `S`, size (`bottom`).<br>\n",
    "        `y_insample`: In-sample values of size (`base`, `insample_size`).<br>\n",
    "        `y_hat_insample`: In-sample forecast values of size (`base`, `insample_size`).<br>\n",
    "        `sigmah`: Estimated standard deviation of the conditional marginal distribution.<br>\t\n",
    "        `level`: float list 0-100, confidence levels for prediction intervals.<br>\n",
    "        `intervals_method`: Sampler for prediction intervals, one of `normality`, `bootstrap`, `permbu`.<br>\n",
    "        `num_samples`: Number of samples for probabilistic coherent distribution.<br>\n",
    "        `seed`: Seed for reproducibility.<br>\n",
    "        `**sampler_kwargs`: Coherent sampler instantiation arguments.<br>\n",
    "\n",
    "        **Returns:**<br>\n",
    "        `y_tilde`: Reconciliated y_hat using the Bottom Up approach.\n",
    "        \"\"\"\n",
    "        # Fit creates P, W and sampler attributes\n",
    "        self.fit(\n",
    "            S=S,\n",
    "            y_hat=y_hat,\n",
    "            y_insample=y_insample,\n",
    "            y_hat_insample=y_hat_insample,\n",
    "            sigmah=sigmah,\n",
    "            intervals_method=intervals_method,\n",
    "            num_samples=num_samples,\n",
    "            seed=seed,\n",
    "            tags=tags,\n",
    "            idx_bottom=idx_bottom,\n",
    "        )\n",
    "\n",
    "        return self._reconcile(\n",
    "            S=S, P=self.P, y_hat=y_hat, sampler=self.sampler, level=level\n",
    "        )\n",
    "\n",
    "    __call__ = fit_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(BottomUp, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(BottomUp.fit, name=\"BottomUp.fit\", title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(BottomUp.predict, name=\"BottomUp.predict\", title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(BottomUp.fit_predict, name=\"BottomUp.fit_predict\", title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(BottomUp.sample, name=\"BottomUp.sample\", title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class BottomUpSparse(BottomUp):\n",
    "    \"\"\"BottomUpSparse Reconciliation Class.\n",
    "\n",
    "    This is the implementation of a Bottom Up reconciliation using the sparse\n",
    "    matrix approach. It works much more efficient on datasets with many time series.\n",
    "    [makoren: At least I hope so, I only checked up until ~20k time series, and\n",
    "    there's no real improvement, it would be great to check for smth like 1M time\n",
    "    series, where the dense S matrix really stops fitting in memory]\n",
    "\n",
    "    See the parent class for more details.\n",
    "    \"\"\"\n",
    "\n",
    "    is_sparse_method = True\n",
    "\n",
    "    def _get_PW_matrices(self, S, idx_bottom):\n",
    "        n_hiers, n_bottom = S.shape\n",
    "        P = sparse.eye(n_bottom, n_hiers, n_hiers - n_bottom, np.float64, \"csr\")\n",
    "        if getattr(self, \"intervals_method\", False) is None:\n",
    "            W = None\n",
    "        else:\n",
    "            W = sparse.eye(n_hiers, dtype=np.float64, format=\"csr\")\n",
    "        return P, W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(BottomUpSparse, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(BottomUpSparse.fit, name=\"BottomUpSparse.fit\", title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(BottomUpSparse.predict, name=\"BottomUpSparse.predict\", title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(BottomUpSparse.fit_predict, name=\"BottomUpSparse.fit_predict\", title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(BottomUpSparse.sample, name=\"BottomUpSparse.sample\", title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "S = np.array(\n",
    "    [\n",
    "        [1.0, 1.0, 1.0, 1.0],\n",
    "        [1.0, 1.0, 0.0, 0.0],\n",
    "        [0.0, 0.0, 1.0, 1.0],\n",
    "        [1.0, 0.0, 0.0, 0.0],\n",
    "        [0.0, 1.0, 0.0, 0.0],\n",
    "        [0.0, 0.0, 1.0, 0.0],\n",
    "        [0.0, 0.0, 0.0, 1.0],\n",
    "    ]\n",
    ")\n",
    "h = 2\n",
    "_y = np.array([10.0, 5.0, 4.0, 2.0, 1.0])\n",
    "y_bottom = np.vstack([i * _y for i in range(1, 5)])\n",
    "y_hat_bottom_insample = np.roll(y_bottom, 1)\n",
    "y_hat_bottom_insample[:, 0] = np.nan\n",
    "y_hat_bottom = np.vstack([i * np.ones(h) for i in range(1, 5)])\n",
    "idx_bottom = [3, 4, 5, 6]\n",
    "tags = {\"level1\": np.array([0]), \"level2\": np.array([1, 2]), \"level3\": idx_bottom}\n",
    "\n",
    "# sigmah for all levels in the hierarchy\n",
    "# sigmah for Naive method\n",
    "# as calculated here:\n",
    "# https://otexts.com/fpp3/prediction-intervals.html\n",
    "y_base = S @ y_bottom\n",
    "y_hat_base = S @ y_hat_bottom\n",
    "y_hat_base_insample = S @ y_hat_bottom_insample\n",
    "sigma = np.nansum((y_base - y_hat_base_insample) ** 2, axis=1) / (y_base.shape[1] - 1)\n",
    "sigma = np.sqrt(sigma)\n",
    "sigmah = sigma[:, None] * np.sqrt(\n",
    "    np.vstack([np.arange(1, h + 1) for _ in range(y_base.shape[0])])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "cls_bottom_up = BottomUp()\n",
    "test_eq(\n",
    "    cls_bottom_up(S=S, y_hat=S @ y_hat_bottom, idx_bottom=idx_bottom)[\"mean\"],\n",
    "    S @ y_hat_bottom,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test bottom up forecast recovery\n",
    "cls_bottom_up = BottomUp()\n",
    "bu_bootstrap_intervals = cls_bottom_up(\n",
    "    S=S,\n",
    "    y_hat=S @ y_hat_bottom,\n",
    "    idx_bottom=idx_bottom,\n",
    ")\n",
    "test_eq(bu_bootstrap_intervals[\"mean\"], S @ y_hat_bottom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test forecast recovery with fit -> predict\n",
    "cls_bottom_up = BottomUp()\n",
    "cls_bottom_up.fit(S=S, y_hat=S @ y_hat_bottom, idx_bottom=idx_bottom)\n",
    "y_tilde = cls_bottom_up.predict(S=S, y_hat=S @ y_hat_bottom)[\"mean\"]\n",
    "test_eq(y_tilde, S @ y_hat_bottom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test not fitted message, for unfitted predict\n",
    "cls_bottom_up = BottomUp()\n",
    "test_fail(\n",
    "    cls_bottom_up.predict,\n",
    "    contains=\"not fitted yet\",\n",
    "    args=(S, S @ y_hat_bottom),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Top-Down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "assert is_strictly_hierarchical(S, tags)\n",
    "S_non_hier = np.array(\n",
    "    [\n",
    "        [1.0, 1.0, 1.0, 1.0],  # total\n",
    "        [1.0, 1.0, 0.0, 0.0],  # city 1\n",
    "        [0.0, 0.0, 1.0, 1.0],  # city 2\n",
    "        [1.0, 0.0, 1.0, 0.0],  # transgender\n",
    "        [0.0, 1.0, 0.0, 1.0],  # no transgender\n",
    "        [1.0, 0.0, 0.0, 0.0],  # city 1 - transgender\n",
    "        [0.0, 1.0, 0.0, 0.0],  # city 1 - no transgender\n",
    "        [0.0, 0.0, 1.0, 0.0],  # city 2 - transgender\n",
    "        [0.0, 0.0, 0.0, 1.0],  # city 2 - no transgender\n",
    "    ]\n",
    ")\n",
    "tags_non_hier = {\n",
    "    \"Country\": np.array([0]),\n",
    "    \"Country/City\": np.array([2, 1]),\n",
    "    \"Country/Transgender\": np.array([3, 4]),\n",
    "    \"Country-City-Transgender\": np.array([5, 6, 7, 8]),\n",
    "}\n",
    "assert not is_strictly_hierarchical(S_non_hier, tags_non_hier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "A = np.array(\n",
    "    [\n",
    "        [0, 1, 1, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 1, 1, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 1, 1],\n",
    "    ]\n",
    ")\n",
    "test_eq(_construct_adjacency_matrix(sparse.csr_matrix(S), tags).toarray(), A)\n",
    "assert _is_strictly_hierarchical(sparse.csr_matrix(A, dtype=bool))\n",
    "A_non_hier = np.array(\n",
    "    [\n",
    "        [0, 1, 1, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 1, 1, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 1, 1, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 1, 0, 1, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 1, 0, 1],\n",
    "    ]\n",
    ")\n",
    "test_eq(_construct_adjacency_matrix(sparse.csr_matrix(S_non_hier), tags_non_hier).toarray(), A_non_hier)\n",
    "assert not _is_strictly_hierarchical(sparse.csr_matrix(A_non_hier, dtype=bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _get_child_nodes(\n",
    "    S: Union[np.ndarray, sparse.csr_matrix], tags: dict[str, np.ndarray]\n",
    "):\n",
    "    if isinstance(S, sparse.spmatrix):\n",
    "        S = S.toarray()\n",
    "    level_names = list(tags.keys())\n",
    "    nodes = OrderedDict()\n",
    "    for i_level, level in enumerate(level_names[:-1]):\n",
    "        parent = tags[level]\n",
    "        child = np.zeros_like(S)\n",
    "        idx_child = tags[level_names[i_level + 1]]\n",
    "        child[idx_child] = S[idx_child]\n",
    "        nodes_level = {}\n",
    "        for idx_parent_node in parent:\n",
    "            parent_node = S[idx_parent_node]\n",
    "            idx_node = child * parent_node.astype(bool)\n",
    "            (idx_node,) = np.where(idx_node.sum(axis=1) > 0)\n",
    "            nodes_level[idx_parent_node] = [idx for idx in idx_child if idx in idx_node]\n",
    "        nodes[level] = nodes_level\n",
    "    return nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _reconcile_fcst_proportions(\n",
    "    S: np.ndarray,\n",
    "    y_hat: np.ndarray,\n",
    "    tags: dict[str, np.ndarray],\n",
    "    nodes: dict[str, dict[int, np.ndarray]],\n",
    "    idx_top: int,\n",
    "):\n",
    "    reconciled = np.zeros_like(y_hat)\n",
    "    reconciled[idx_top] = y_hat[idx_top]\n",
    "    level_names = list(tags.keys())\n",
    "    for i_level, level in enumerate(level_names[:-1]):\n",
    "        nodes_level = nodes[level]\n",
    "        for idx_parent, idx_childs in nodes_level.items():\n",
    "            fcst_parent = reconciled[idx_parent]\n",
    "            childs_sum = y_hat[idx_childs].sum()\n",
    "            for idx_child in idx_childs:\n",
    "                if np.abs(childs_sum) < 1e-8:\n",
    "                    n_children = len(idx_childs)\n",
    "                    reconciled[idx_child] = fcst_parent / n_children\n",
    "                else:\n",
    "                    reconciled[idx_child] = y_hat[idx_child] * fcst_parent / childs_sum\n",
    "    return reconciled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TopDown(HReconciler):\n",
    "    \"\"\"Top Down Reconciliation Class.\n",
    "\n",
    "    The Top Down hierarchical reconciliation method, distributes the total aggregate predictions and decomposes\n",
    "    it down the hierarchy using proportions $\\mathbf{p}_{\\mathrm{[b]}}$ that can be actual historical values\n",
    "    or estimated.\n",
    "\n",
    "    $$\\mathbf{P}=[\\mathbf{p}_{\\mathrm{[b]}}\\;|\\;\\mathbf{0}_{\\mathrm{[b][a,b\\;-1]}}]$$\n",
    "    **Parameters:**<br>\n",
    "    `method`: One of `forecast_proportions`, `average_proportions` and `proportion_averages`.<br>\n",
    "\n",
    "    **References:**<br>\n",
    "    - [CW. Gross (1990). \\\"Disaggregation methods to expedite product line forecasting\\\". Journal of Forecasting, 9 , 233–254.\n",
    "    doi:10.1002/for.3980090304](https://onlinelibrary.wiley.com/doi/abs/10.1002/for.3980090304).<br>\n",
    "    - [G. Fliedner (1999). \\\"An investigation of aggregate variable time series forecast strategies with specific subaggregate\n",
    "    time series statistical correlation\\\". Computers and Operations Research, 26 , 1133–1149.\n",
    "    doi:10.1016/S0305-0548(99)00017-9](https://doi.org/10.1016/S0305-0548(99)00017-9).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, method: str):\n",
    "        self.method = method\n",
    "        self.insample = method in [\"average_proportions\", \"proportion_averages\"]\n",
    "\n",
    "    def _get_PW_matrices(\n",
    "        self,\n",
    "        S: np.ndarray,\n",
    "        y_hat: np.ndarray,\n",
    "        y_insample: np.ndarray,\n",
    "        tags: Optional[dict[str, np.ndarray]] = None,\n",
    "    ):\n",
    "\n",
    "        n_hiers, n_bottom = S.shape\n",
    "\n",
    "        # Check if the data structure is strictly hierarchical.\n",
    "        if tags is not None:\n",
    "            if not is_strictly_hierarchical(S, tags):\n",
    "                raise ValueError(\n",
    "                    \"Top-down reconciliation requires strictly hierarchical structures.\"\n",
    "                )\n",
    "            idx_top = int(S.sum(axis=1).argmax())\n",
    "            levels_ = dict(sorted(tags.items(), key=lambda x: len(x[1])))\n",
    "            idx_bottom = levels_[list(levels_)[-1]]\n",
    "            y_btm = y_insample[idx_bottom]\n",
    "        else:\n",
    "            idx_top = 0\n",
    "            y_btm = y_insample[(n_hiers - n_bottom) :]\n",
    "\n",
    "        y_top = y_insample[idx_top]\n",
    "\n",
    "        if self.method == \"average_proportions\":\n",
    "            prop = np.nanmean(y_btm / y_top, axis=1)\n",
    "        elif self.method == \"proportion_averages\":\n",
    "            prop = np.nanmean(y_btm, axis=1) / np.nanmean(y_top)\n",
    "        elif self.method == \"forecast_proportions\":\n",
    "            raise NotImplementedError(f\"Fit method not implemented for {self.method} yet\")\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown method {self.method}\")\n",
    "        \n",
    "        if np.isnan(y_btm).any() or np.isnan(y_top).any():\n",
    "            warnings.warn(\n",
    "                '''\n",
    "                Warning: There are NaN values in one or more levels of Y_df.\n",
    "                This may lead to unexpected behavior when computing average proportions and proportion averages.\n",
    "                '''\n",
    "            )\n",
    "\n",
    "        P = np.zeros_like(\n",
    "            S, np.float64\n",
    "        ).T  # float 64 if prop is too small, happens with wiki2\n",
    "        P[:, idx_top] = prop\n",
    "        W = np.eye(n_hiers, dtype=np.float64)\n",
    "        return P, W\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        S,\n",
    "        y_hat,\n",
    "        y_insample: np.ndarray,\n",
    "        y_hat_insample: Optional[np.ndarray] = None,\n",
    "        sigmah: Optional[np.ndarray] = None,\n",
    "        intervals_method: Optional[str] = None,\n",
    "        num_samples: Optional[int] = None,\n",
    "        seed: Optional[int] = None,\n",
    "        tags: Optional[dict[str, np.ndarray]] = None,\n",
    "        idx_bottom: Optional[np.ndarray] = None,\n",
    "    ):\n",
    "        \"\"\"TopDown Fit Method.\n",
    "\n",
    "        **Parameters:**<br>\n",
    "        `S`: Summing matrix of size (`base`, `bottom`).<br>\n",
    "        `y_hat`: Forecast values of size (`base`, `horizon`).<br>\n",
    "        `y_insample`: Insample values of size (`base`, `insample_size`). Optional for `forecast_proportions` method.<br>\n",
    "        `y_hat_insample`: Insample forecast values of size (`base`, `insample_size`). Optional for `forecast_proportions` method.<br>\n",
    "        `sigmah`: Estimated standard deviation of the conditional marginal distribution.<br>\n",
    "        `interval_method`: Sampler for prediction intervals, one of `normality`, `bootstrap`, `permbu`.<br>\n",
    "        `num_samples`: Number of samples for probabilistic coherent distribution.<br>\n",
    "        `seed`: Seed for reproducibility.<br>\n",
    "        `tags`: Each key is a level and each value its `S` indices.<br>\n",
    "        `idx_bottom`: Indices corresponding to the bottom level of `S`, size (`bottom`).<br>\n",
    "\n",
    "        **Returns:**<br>\n",
    "        `self`: object, fitted reconciler.\n",
    "        \"\"\"\n",
    "        self.intervals_method = intervals_method\n",
    "        self.P, self.W = self._get_PW_matrices(\n",
    "            S=S, y_hat=y_hat, tags=tags, y_insample=y_insample\n",
    "        )\n",
    "        self.sampler = self._get_sampler(\n",
    "            S=S,\n",
    "            P=self.P,\n",
    "            W=self.W,\n",
    "            y_hat=y_hat,\n",
    "            y_insample=y_insample,\n",
    "            y_hat_insample=y_hat_insample,\n",
    "            sigmah=sigmah,\n",
    "            intervals_method=intervals_method,\n",
    "            num_samples=num_samples,\n",
    "            seed=seed,\n",
    "            tags=tags,\n",
    "        )\n",
    "        self.fitted = True\n",
    "        return self\n",
    "\n",
    "    def fit_predict(\n",
    "        self,\n",
    "        S: np.ndarray,\n",
    "        y_hat: np.ndarray,\n",
    "        tags: dict[str, np.ndarray],\n",
    "        idx_bottom: np.ndarray = None,\n",
    "        y_insample: Optional[np.ndarray] = None,\n",
    "        y_hat_insample: Optional[np.ndarray] = None,\n",
    "        sigmah: Optional[np.ndarray] = None,\n",
    "        level: Optional[list[int]] = None,\n",
    "        intervals_method: Optional[str] = None,\n",
    "        num_samples: Optional[int] = None,\n",
    "        seed: Optional[int] = None,\n",
    "    ):\n",
    "        \"\"\"Top Down Reconciliation Method.\n",
    "\n",
    "        **Parameters:**<br>\n",
    "        `S`: Summing matrix of size (`base`, `bottom`).<br>\n",
    "        `y_hat`: Forecast values of size (`base`, `horizon`).<br>\n",
    "        `tags`: Each key is a level and each value its `S` indices.<br>\n",
    "        `idx_bottom`: Indices corresponding to the bottom level of `S`, size (`bottom`).<br>\n",
    "        `y_insample`: Insample values of size (`base`, `insample_size`). Optional for `forecast_proportions` method.<br>\n",
    "        `y_hat_insample`: Insample forecast values of size (`base`, `insample_size`). Optional for `forecast_proportions` method.<br>\n",
    "        `sigmah`: Estimated standard deviation of the conditional marginal distribution.<br>\n",
    "        `level`: float list 0-100, confidence levels for prediction intervals.<br>\n",
    "        `intervals_method`: Sampler for prediction intervals, one of `normality`, `bootstrap`, `permbu`.<br>\n",
    "        `num_samples`: Number of samples for probabilistic coherent distribution.<br>\n",
    "        `seed`: Seed for reproducibility.<br>\n",
    "\n",
    "        **Returns:**<br>\n",
    "        `y_tilde`: Reconciliated y_hat using the Top Down approach.\n",
    "        \"\"\"\n",
    "        if self.method == \"forecast_proportions\":\n",
    "            idx_top = int(S.sum(axis=1).argmax())\n",
    "            levels_ = dict(sorted(tags.items(), key=lambda x: len(x[1])))\n",
    "            if level is not None:\n",
    "                raise ValueError(\"Prediction intervals not implemented for `forecast_proportions`\")\n",
    "            nodes = _get_child_nodes(S=S, tags=levels_)\n",
    "            reconciled = [\n",
    "                _reconcile_fcst_proportions(\n",
    "                    S=S,\n",
    "                    y_hat=y_hat_[:, None],\n",
    "                    tags=levels_,\n",
    "                    nodes=nodes,\n",
    "                    idx_top=idx_top,\n",
    "                )\n",
    "                for y_hat_ in y_hat.T\n",
    "            ]\n",
    "            reconciled = np.hstack(reconciled)\n",
    "            return {\"mean\": reconciled}\n",
    "        else:\n",
    "            # Fit creates P, W and sampler attributes\n",
    "            self.fit(\n",
    "                S=S,\n",
    "                y_hat=y_hat,\n",
    "                y_insample=y_insample,\n",
    "                y_hat_insample=y_hat_insample,\n",
    "                sigmah=sigmah,\n",
    "                intervals_method=intervals_method,\n",
    "                num_samples=num_samples,\n",
    "                seed=seed,\n",
    "                tags=tags,\n",
    "                idx_bottom=idx_bottom,\n",
    "            )\n",
    "            return self._reconcile(\n",
    "                S=S, P=self.P, y_hat=y_hat, level=level, sampler=self.sampler\n",
    "            )\n",
    "\n",
    "    __call__ = fit_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(TopDown, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(TopDown.fit, name=\"TopDown.fit\", title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(TopDown.predict, name=\"TopDown.predict\", title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(TopDown.fit_predict, name=\"TopDown.fit_predict\", title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(TopDown.sample, name=\"TopDown.sample\", title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TopDownSparse(TopDown):\n",
    "    \"\"\"TopDownSparse Reconciliation Class.\n",
    "\n",
    "    This is an implementation of top-down reconciliation using the sparse matrix\n",
    "    approach. It works much more efficiently on data sets with many time series.\n",
    "\n",
    "    See the parent class for more details.\n",
    "    \"\"\"\n",
    "\n",
    "    is_sparse_method = True\n",
    "    is_strictly_hierarchical = False\n",
    "\n",
    "    def _get_PW_matrices(\n",
    "        self,\n",
    "        S: sparse.csr_matrix,\n",
    "        y_hat: np.ndarray,\n",
    "        y_insample: np.ndarray,\n",
    "        tags: Optional[dict[str, np.ndarray]] = None,\n",
    "    ):\n",
    "        # Avoid a redundant check during middle-out reconciliation.\n",
    "        if not self.is_strictly_hierarchical:\n",
    "            # Check if the data structure is strictly hierarchical.\n",
    "            if tags is not None and not _is_strictly_hierarchical(\n",
    "                _construct_adjacency_matrix(S, tags)\n",
    "            ):\n",
    "                raise ValueError(\n",
    "                    \"Top-down reconciliation requires strictly hierarchical structures.\"\n",
    "                )\n",
    "\n",
    "        # Get the dimensions of the \"summing\" matrix.\n",
    "        n_hiers, n_bottom = S.shape\n",
    "\n",
    "        # Get the in-sample values of the top node and bottom nodes.\n",
    "        y_top = y_insample[0]\n",
    "        y_btm = y_insample[(n_hiers - n_bottom) :]\n",
    "\n",
    "        # Calculate the disaggregation proportions.\n",
    "        if self.method == \"average_proportions\":\n",
    "            prop = np.mean(y_btm / y_top, 1)\n",
    "        elif self.method == \"proportion_averages\":\n",
    "            prop = np.mean(y_btm, 1) / np.mean(y_top)\n",
    "        elif self.method == \"forecast_proportions\":\n",
    "            raise ValueError(f\"Fit method not yet implemented for {self.method}.\")\n",
    "        else:\n",
    "            raise ValueError(f\"{self.method} is an unknown disaggregation method.\")\n",
    "\n",
    "        # Instantiate and allocate the \"projection\" matrix to distribute the\n",
    "        # disaggregated base forecast of the top node to the bottom nodes.\n",
    "        P = sparse.csr_matrix(\n",
    "            (\n",
    "                prop,\n",
    "                np.zeros_like(prop, np.uint8),\n",
    "                np.arange(len(prop) + 1, dtype=np.min_scalar_type(n_bottom)),\n",
    "            ),\n",
    "            shape=(n_bottom, n_hiers),\n",
    "            dtype=np.float64,\n",
    "        )\n",
    "\n",
    "        # Instantiate and allocate the \"weight\" matrix.\n",
    "        if getattr(self, \"intervals_method\", False) is None:\n",
    "            W = None\n",
    "        else:\n",
    "            W = sparse.eye(n_hiers, dtype=np.float64, format=\"csr\")\n",
    "\n",
    "        return P, W\n",
    "\n",
    "    def fit_predict(\n",
    "        self,\n",
    "        S: sparse.csr_matrix,\n",
    "        y_hat: np.ndarray,\n",
    "        tags: dict[str, np.ndarray],\n",
    "        idx_bottom: np.ndarray = None,\n",
    "        y_insample: Optional[np.ndarray] = None,\n",
    "        y_hat_insample: Optional[np.ndarray] = None,\n",
    "        sigmah: Optional[np.ndarray] = None,\n",
    "        level: Optional[list[int]] = None,\n",
    "        intervals_method: Optional[str] = None,\n",
    "        num_samples: Optional[int] = None,\n",
    "        seed: Optional[int] = None,\n",
    "    ) -> dict[str, np.ndarray]:\n",
    "        if self.method == \"forecast_proportions\":\n",
    "            # Check if probabilistic reconciliation is required.\n",
    "            if level is not None:\n",
    "                raise NotImplementedError(\n",
    "                    \"Prediction intervals are not implemented for `forecast_proportions`.\"\n",
    "                )\n",
    "            # Construct the adjacency matrix.\n",
    "            A = _construct_adjacency_matrix(S, tags)\n",
    "            # Avoid a redundant check during middle-out reconciliation.\n",
    "            if not self.is_strictly_hierarchical:\n",
    "                # Check if the data structure is strictly hierarchical.\n",
    "                if tags is not None and not _is_strictly_hierarchical(A):\n",
    "                    raise ValueError(\n",
    "                        \"Top-down reconciliation requires strictly hierarchical structures.\"\n",
    "                    )\n",
    "            # As we may have zero sibling sums, replace any zeroes with eps.\n",
    "            y_hat[y_hat == 0.0] = np.finfo(np.float64).eps\n",
    "            # Calculate the relative proportions for each node.\n",
    "            with np.errstate(divide=\"ignore\"):\n",
    "                P = y_hat / ((A.T @ A) @ y_hat)\n",
    "            # Set the relative proportion of the root node.\n",
    "            P[P == np.inf] = 1.0\n",
    "            # Precompute the transpose of the summing matrix.\n",
    "            S_T = S.T\n",
    "            # Propagate the relative proportions for the nodes along each leaf\n",
    "            # node's disaggregation pathway, convert the resultant sparse\n",
    "            # matrix to a LIL matrix for an efficient dense conversion, stack\n",
    "            # the lists, calculate the row-wise product to get the forecast\n",
    "            # proportions, and use these to reconcile the forecasts.\n",
    "            y_tilde = np.array(\n",
    "                [\n",
    "                    S\n",
    "                    @ (\n",
    "                        np.prod(np.vstack(S_T.multiply(P[:, i]).tolil().data), 1)\n",
    "                        * y_hat[0, i]\n",
    "                    )\n",
    "                    for i in range(y_hat.shape[1])\n",
    "                ]\n",
    "            ).T\n",
    "            return {\"mean\": y_tilde}\n",
    "        else:\n",
    "            # Fit creates the P, W, and sampler attributes.\n",
    "            self.fit(\n",
    "                S=S,\n",
    "                y_hat=y_hat,\n",
    "                y_insample=y_insample,\n",
    "                y_hat_insample=y_hat_insample,\n",
    "                sigmah=sigmah,\n",
    "                intervals_method=intervals_method,\n",
    "                num_samples=num_samples,\n",
    "                seed=seed,\n",
    "                tags=tags,\n",
    "                idx_bottom=idx_bottom,\n",
    "            )\n",
    "            return self._reconcile(\n",
    "                S=S, P=self.P, y_hat=y_hat, level=level, sampler=self.sampler\n",
    "            )\n",
    "\n",
    "    __call__ = fit_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(TopDownSparse, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(TopDownSparse.fit, name=\"TopDownSparse.fit\", title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(TopDownSparse.predict, name=\"TopDownSparse.predict\", title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(TopDownSparse.fit_predict, name=\"TopDownSparse.fit_predict\", title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(TopDownSparse.sample, name=\"TopDownSparse.sample\", title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# we are able to recover forecasts\n",
    "# from top_down in this example\n",
    "# because the time series\n",
    "# share the same proportion\n",
    "# across time\n",
    "# but it is not a general case\n",
    "for method in [\"forecast_proportions\", \"average_proportions\", \"proportion_averages\"]:\n",
    "    cls_top_down = TopDown(method=method)\n",
    "    if cls_top_down.insample:\n",
    "        assert method in [\"average_proportions\", \"proportion_averages\"]\n",
    "        test_close(\n",
    "            cls_top_down(\n",
    "                S=S, y_hat=S @ y_hat_bottom, y_insample=S @ y_bottom, tags=tags\n",
    "            )[\"mean\"],\n",
    "            S @ y_hat_bottom,\n",
    "        )\n",
    "    else:\n",
    "        test_close(\n",
    "            cls_top_down(S=S, y_hat=S @ y_hat_bottom, tags=tags)[\"mean\"],\n",
    "            S @ y_hat_bottom,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_top_down(\n",
    "                S=S, y_hat=S @ y_hat_bottom, y_insample=S @ y_bottom, tags=tags\n",
    "            )[\"mean\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\\ hide\n",
    "cls_top_down = TopDownSparse(method=\"average_proportions\")\n",
    "test_fail(\n",
    "    cls_top_down,\n",
    "    contains=\"Top-down reconciliation requires strictly hierarchical structures.\",\n",
    "    args=(sparse.csr_matrix(S_non_hier), None, tags_non_hier),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "for method in [\"forecast_proportions\", \"average_proportions\", \"proportion_averages\"]:\n",
    "    cls_top_down = TopDownSparse(method=method)\n",
    "    if cls_top_down.insample:\n",
    "        assert method in [\"average_proportions\", \"proportion_averages\"]\n",
    "        test_close(\n",
    "            cls_top_down(\n",
    "                S=sparse.csr_matrix(S),\n",
    "                y_hat=S @ y_hat_bottom,\n",
    "                y_insample=S @ y_bottom,\n",
    "                tags=tags,\n",
    "            )[\"mean\"],\n",
    "            S @ y_hat_bottom,\n",
    "        )\n",
    "    else:\n",
    "        test_close(\n",
    "            cls_top_down(S=sparse.csr_matrix(S), y_hat=S @ y_hat_bottom, tags=tags)[\n",
    "                \"mean\"\n",
    "            ],\n",
    "            S @ y_hat_bottom,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "for method in [\"forecast_proportions\", \"average_proportions\", \"proportion_averages\"]:\n",
    "    cls_top_down = TopDown(method=method)\n",
    "    cls_top_down_sparse = TopDownSparse(method=method)\n",
    "    if cls_top_down.insample:\n",
    "        assert method in [\"average_proportions\", \"proportion_averages\"]\n",
    "        test_close(\n",
    "            cls_top_down(\n",
    "                S=S,\n",
    "                y_hat=S @ y_hat_bottom,\n",
    "                y_insample=S @ y_bottom,\n",
    "                tags=tags,\n",
    "            )[\"mean\"],\n",
    "            cls_top_down_sparse(\n",
    "                S=sparse.csr_matrix(S),\n",
    "                y_hat=S @ y_hat_bottom,\n",
    "                y_insample=S @ y_bottom,\n",
    "                tags=tags,\n",
    "            )[\"mean\"],\n",
    "            np.finfo(np.float64).eps,\n",
    "        )\n",
    "    else:\n",
    "        test_close(\n",
    "            cls_top_down(S=S, y_hat=S @ y_hat_bottom, tags=tags)[\"mean\"],\n",
    "            cls_top_down_sparse(\n",
    "                S=sparse.csr_matrix(S),\n",
    "                y_hat=S @ y_hat_bottom,\n",
    "                y_insample=S @ y_bottom,\n",
    "                tags=tags,\n",
    "            )[\"mean\"],\n",
    "            1e-9,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "for method in [\"average_proportions\", \"proportion_averages\"]:\n",
    "    cls_top_down = TopDown(method=method)\n",
    "    y_insample_orig = S @ y_bottom\n",
    "    y_insample_orig[-1, :] = 0\n",
    "    result_orig = cls_top_down(\n",
    "        S=S, y_hat=S @ y_hat_bottom, y_insample=S @ y_bottom, tags=tags\n",
    "    )[\"mean\"]\n",
    "    y_insample_nan = y_insample_orig.copy()\n",
    "    y_insample_nan[-1, 0] = np.nan\n",
    "    result_nan = cls_top_down(\n",
    "        S=S, y_hat=S @ y_hat_bottom, y_insample=S @ y_bottom, tags=tags\n",
    "    )[\"mean\"]\n",
    "    test_close(result_orig, result_nan)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Middle-Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MiddleOut(HReconciler):\n",
    "    \"\"\"Middle Out Reconciliation Class.\n",
    "\n",
    "    This method is only available for **strictly hierarchical structures**. It anchors the base predictions\n",
    "    in a middle level. The levels above the base predictions use the Bottom-Up approach, while the levels\n",
    "    below use a Top-Down.\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `middle_level`: Middle level.<br>\n",
    "    `top_down_method`: One of `forecast_proportions`, `average_proportions` and `proportion_averages`.<br>\n",
    "\n",
    "    **References:**<br>\n",
    "    - [Hyndman, R.J., & Athanasopoulos, G. (2021). \\\"Forecasting: principles and practice, 3rd edition:\n",
    "    Chapter 11: Forecasting hierarchical and grouped series.\\\". OTexts: Melbourne, Australia. OTexts.com/fpp3\n",
    "    Accessed on July 2022.](https://otexts.com/fpp3/hierarchical.html)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, middle_level: str, top_down_method: str):\n",
    "        self.middle_level = middle_level\n",
    "        self.top_down_method = top_down_method\n",
    "        self.insample = top_down_method in [\n",
    "            \"average_proportions\",\n",
    "            \"proportion_averages\",\n",
    "        ]\n",
    "\n",
    "    def _get_PW_matrices(self, **kwargs):\n",
    "        raise NotImplementedError(\"Not implemented\")\n",
    "\n",
    "    def fit(self, **kwargs):\n",
    "        raise NotImplementedError(\"Not implemented\")\n",
    "\n",
    "    def predict(self, **kwargs):\n",
    "        raise NotImplementedError(\"Not implemented\")\n",
    "\n",
    "    def fit_predict(\n",
    "        self,\n",
    "        S: np.ndarray,\n",
    "        y_hat: np.ndarray,\n",
    "        tags: dict[str, np.ndarray],\n",
    "        y_insample: Optional[np.ndarray] = None,\n",
    "        level: Optional[list[int]] = None,\n",
    "        intervals_method: Optional[str] = None,\n",
    "    ):\n",
    "        \"\"\"Middle Out Reconciliation Method.\n",
    "\n",
    "        **Parameters:**<br>\n",
    "        `S`: Summing matrix of size (`base`, `bottom`).<br>\n",
    "        `y_hat`: Forecast values of size (`base`, `horizon`).<br>\n",
    "        `tags`: Each key is a level and each value its `S` indices.<br>\n",
    "        `y_insample`: Insample values of size (`base`, `insample_size`). Only used for `forecast_proportions`<br>\n",
    "        `level`: Not supported. <br>\n",
    "        `intervals_method`: Not supported.<br>\n",
    "\n",
    "        **Returns:**<br>\n",
    "        `y_tilde`: Reconciliated y_hat using the Middle Out approach.\n",
    "        \"\"\"\n",
    "        if level is not None or intervals_method is not None:\n",
    "            raise NotImplementedError(\"Prediction intervals are not implemented for `MiddleOut`\")\n",
    "\n",
    "        if not is_strictly_hierarchical(S, tags):\n",
    "            raise ValueError(\n",
    "                \"Middle out reconciliation requires strictly hierarchical structures.\"\n",
    "            )\n",
    "        if self.middle_level not in tags.keys():\n",
    "            raise ValueError(\"You have to provide a `middle_level` in `tags`.\")\n",
    "\n",
    "        levels_ = dict(sorted(tags.items(), key=lambda x: len(x[1])))\n",
    "        reconciled = np.full_like(y_hat, fill_value=np.nan)\n",
    "        cut_nodes = levels_[self.middle_level]\n",
    "        # bottom up reconciliation\n",
    "        idxs_bu = []\n",
    "        for node, idx_node in levels_.items():\n",
    "            idxs_bu.append(idx_node)\n",
    "            if node == self.middle_level:\n",
    "                break\n",
    "        idxs_bu = np.hstack(idxs_bu)\n",
    "        # bottom up forecasts\n",
    "        bu = BottomUp().fit_predict(\n",
    "            S=np.fliplr(np.unique(S[idxs_bu], axis=1)),\n",
    "            y_hat=y_hat[idxs_bu],\n",
    "            idx_bottom=np.arange(len(idxs_bu))[-len(cut_nodes) :],\n",
    "        )\n",
    "        reconciled[idxs_bu] = bu[\"mean\"]\n",
    "\n",
    "        # top down\n",
    "        child_nodes = _get_child_nodes(S, levels_)\n",
    "        # parents contains each node in the middle out level\n",
    "        # as key. The values of each node are the levels that\n",
    "        # are conected to that node.\n",
    "        parents = {node: {self.middle_level: np.array([node])} for node in cut_nodes}\n",
    "        level_names = list(levels_.keys())\n",
    "        for lv, lv_child in zip(level_names[:-1], level_names[1:]):\n",
    "            # if lv is not part of the middle out to bottom\n",
    "            # structure we continue\n",
    "            if lv not in list(parents.values())[0].keys():\n",
    "                continue\n",
    "            for idx_middle_out in parents.keys():\n",
    "                idxs_parents = parents[idx_middle_out].values()\n",
    "                complete_idxs_child = []\n",
    "                for idx_parent, idxs_child in child_nodes[lv].items():\n",
    "                    if any(idx_parent in val for val in idxs_parents):\n",
    "                        complete_idxs_child.append(idxs_child)\n",
    "                parents[idx_middle_out][lv_child] = np.hstack(complete_idxs_child)\n",
    "\n",
    "        for node, levels_node in parents.items():\n",
    "            idxs_node = np.hstack(list(levels_node.values()))\n",
    "            S_node = S[idxs_node]\n",
    "            S_node = S_node[:, ~np.all(S_node == 0, axis=0)]\n",
    "            counter = 0\n",
    "            levels_node_ = deepcopy(levels_node)\n",
    "            for lv_name, idxs_level in levels_node_.items():\n",
    "                idxs_len = len(idxs_level)\n",
    "                levels_node_[lv_name] = np.arange(counter, idxs_len + counter)\n",
    "                counter += idxs_len\n",
    "            td = TopDown(self.top_down_method).fit_predict(\n",
    "                S=S_node,\n",
    "                y_hat=y_hat[idxs_node],\n",
    "                y_insample=y_insample[idxs_node] if y_insample is not None else None,\n",
    "                tags=levels_node_,\n",
    "            )\n",
    "            reconciled[idxs_node] = td[\"mean\"]\n",
    "        return {\"mean\": reconciled}\n",
    "\n",
    "    __call__ = fit_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(MiddleOut, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(MiddleOut.fit, name=\"MiddleOut.fit\", title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(MiddleOut.predict, name=\"MiddleOut.predict\", title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(MiddleOut.fit_predict, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(MiddleOut.sample, name=\"MiddleOut.sample\", title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MiddleOutSparse(MiddleOut):\n",
    "    \"\"\"MiddleOutSparse Reconciliation Class.\n",
    "\n",
    "    This is an implementation of middle-out reconciliation using the sparse matrix\n",
    "    approach. It works much more efficiently on data sets with many time series.\n",
    "\n",
    "    See the parent class for more details.\n",
    "    \"\"\"\n",
    "\n",
    "    # Although this is a sparse method, as we need to a dense representation of the\n",
    "    # \"summing\" matrix for the required transformations in the fit_predict method\n",
    "    # prior to bottom-up and top-down reconciliation, we can avoid a redundant\n",
    "    # conversion.\n",
    "    is_sparse_method = False\n",
    "\n",
    "    def fit_predict(\n",
    "        self,\n",
    "        S: np.ndarray,\n",
    "        y_hat: np.ndarray,\n",
    "        tags: dict[str, np.ndarray],\n",
    "        y_insample: Optional[np.ndarray] = None,\n",
    "        level: Optional[list[int]] = None,\n",
    "        intervals_method: Optional[str] = None,\n",
    "    ) -> dict[str, np.ndarray]:\n",
    "        # Check if probabilistic reconciliation is required.\n",
    "        if level is not None or intervals_method is not None:\n",
    "            raise NotImplementedError(\n",
    "                \"Prediction intervals are not implemented for `MiddleOutSparse`.\"\n",
    "            )\n",
    "        # Check if the middle level exists in the level to nodes mapping.\n",
    "        if self.middle_level not in tags.keys():\n",
    "            raise KeyError(f\"{self.middle_level} is not a key in `tags`.\")\n",
    "        # Check if the data structure is strictly hierarchical.\n",
    "        if not _is_strictly_hierarchical(\n",
    "            _construct_adjacency_matrix(sparse.csr_matrix(S), tags)\n",
    "        ):\n",
    "            raise ValueError(\n",
    "                \"Middle-out reconciliation requires strictly hierarchical structures.\"\n",
    "            )\n",
    "\n",
    "        # Sort the levels by the number of nodes.\n",
    "        levels = dict(sorted(tags.items(), key=lambda x: len(x[1])))\n",
    "        # Allocate an array to store the reconciled point forecasts.\n",
    "        y_tilde = np.full_like(y_hat, np.nan)\n",
    "        # Find the nodes that constitute the middle level.\n",
    "        cut_nodes = levels[self.middle_level]\n",
    "\n",
    "        # Calculate the cut that separates the middle level from the lower levels.\n",
    "        cut_idx = max(cut_nodes) + 1\n",
    "\n",
    "        # Perform sparse bottom-up reconciliation from the middle level.\n",
    "        y_tilde[:cut_idx, :] = BottomUpSparse().fit_predict(\n",
    "            S=sparse.csr_matrix(np.fliplr(np.unique(S[:cut_idx, :], axis=1))),\n",
    "            y_hat=y_hat[:cut_idx, :],\n",
    "            idx_bottom=None,\n",
    "        )[\"mean\"]\n",
    "\n",
    "        # Set up the reconciler for top-down reconciliation.\n",
    "        cls_top_down = TopDownSparse(self.top_down_method)\n",
    "        cls_top_down.is_strictly_hierarchical = True\n",
    "\n",
    "        # Perform sparse top-down reconciliation from the middle level.\n",
    "        for cut_node in cut_nodes:\n",
    "            # Find the leaf nodes of the subgraph for the cut node.\n",
    "            leaf_idx = np.flatnonzero(S[cut_node, :])\n",
    "            # Find all the nodes in the subgraph for the cut node.\n",
    "            sub_idx = np.hstack(\n",
    "                (cut_node, cut_idx + np.flatnonzero((np.any(S[cut_idx:, leaf_idx], 1))))\n",
    "            )\n",
    "\n",
    "            # Construct the \"tags\" argument for the cut node.\n",
    "            if self.insample:\n",
    "                # It is not required for in-sample disaggregation methods.\n",
    "                sub_tags = None\n",
    "            else:\n",
    "                # Disaggregating using forecast proportions requires the \"tags\" for\n",
    "                # the subgraph.\n",
    "                sub_tags = {}\n",
    "                acc = 0\n",
    "                for level_, nodes in levels.items():\n",
    "                    # Find all the nodes in the subgraph for the level.\n",
    "                    nodes = np.intersect1d(nodes, sub_idx, True)\n",
    "                    # Get the number of nodes in the level.\n",
    "                    n = len(nodes)\n",
    "                    # Exclude any levels above the cut node or empty ones below.\n",
    "                    if len(nodes) > 0:\n",
    "                        sub_tags[level_] = np.arange(acc, n + acc)\n",
    "                        acc += n\n",
    "\n",
    "            # Perform sparse top-down reconciliation from the cut node.\n",
    "            y_tilde[sub_idx, :] = cls_top_down.fit_predict(\n",
    "                S=sparse.csr_matrix(S[sub_idx[:, None], leaf_idx]),\n",
    "                y_hat=y_hat[sub_idx, :],\n",
    "                y_insample=y_insample[sub_idx, :] if y_insample is not None else None,\n",
    "                tags=sub_tags,\n",
    "            )[\"mean\"]\n",
    "\n",
    "        return {\"mean\": y_tilde}\n",
    "\n",
    "    __call__ = fit_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(MiddleOutSparse, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(MiddleOutSparse.fit, name=\"MiddleOutSparse.fit\", title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(MiddleOutSparse.predict, name=\"MiddleOutSparse.predict\", title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(MiddleOutSparse.fit_predict, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(MiddleOutSparse.sample, name=\"MiddleOutSparse.sample\", title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# we are able to recover forecasts\n",
    "# from middle out in this example\n",
    "# because the time series\n",
    "# share the same proportion\n",
    "# across time\n",
    "# but it is not a general case\n",
    "for method in [\"forecast_proportions\", \"average_proportions\", \"proportion_averages\"]:\n",
    "    cls_middle_out = MiddleOut(middle_level=\"level2\", top_down_method=method)\n",
    "    if cls_middle_out.insample:\n",
    "        assert method in [\"average_proportions\", \"proportion_averages\"]\n",
    "        test_close(\n",
    "            cls_middle_out(\n",
    "                S=S, y_hat=S @ y_hat_bottom, y_insample=S @ y_bottom, tags=tags\n",
    "            )[\"mean\"],\n",
    "            S @ y_hat_bottom,\n",
    "        )\n",
    "    else:\n",
    "        test_close(\n",
    "            cls_middle_out(S=S, y_hat=S @ y_hat_bottom, tags=tags)[\"mean\"],\n",
    "            S @ y_hat_bottom,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "for method in [\"forecast_proportions\", \"average_proportions\", \"proportion_averages\"]:\n",
    "    cls_middle_out = MiddleOutSparse(middle_level=\"level2\", top_down_method=method)\n",
    "    if cls_middle_out.insample:\n",
    "        assert method in [\"average_proportions\", \"proportion_averages\"]\n",
    "        test_close(\n",
    "            cls_middle_out(\n",
    "                S=S,\n",
    "                y_hat=S @ y_hat_bottom,\n",
    "                y_insample=S @ y_bottom,\n",
    "                tags=tags,\n",
    "            )[\"mean\"],\n",
    "            S @ y_hat_bottom,\n",
    "        )\n",
    "    else:\n",
    "        test_close(\n",
    "            cls_middle_out(S=S, y_hat=S @ y_hat_bottom, tags=tags)[\"mean\"],\n",
    "            S @ y_hat_bottom,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "for method in [\"forecast_proportions\", \"average_proportions\", \"proportion_averages\"]:\n",
    "    cls_middle_out = MiddleOut(middle_level=\"level2\", top_down_method=method)\n",
    "    cls_middle_out_sparse = MiddleOutSparse(\n",
    "        middle_level=\"level2\", top_down_method=method\n",
    "    )\n",
    "    if cls_middle_out.insample:\n",
    "        assert method in [\"average_proportions\", \"proportion_averages\"]\n",
    "        test_close(\n",
    "            cls_middle_out(\n",
    "                S=S,\n",
    "                y_hat=S @ y_hat_bottom,\n",
    "                y_insample=S @ y_bottom,\n",
    "                tags=tags,\n",
    "            )[\"mean\"],\n",
    "            cls_middle_out_sparse(\n",
    "                S=S,\n",
    "                y_hat=S @ y_hat_bottom,\n",
    "                y_insample=S @ y_bottom,\n",
    "                tags=tags,\n",
    "            )[\"mean\"],\n",
    "            np.finfo(np.float64).eps,\n",
    "        )\n",
    "    else:\n",
    "        test_close(\n",
    "            cls_middle_out(S=S, y_hat=S @ y_hat_bottom, tags=tags)[\"mean\"],\n",
    "            cls_middle_out_sparse(\n",
    "                S=S,\n",
    "                y_hat=S @ y_hat_bottom,\n",
    "                y_insample=S @ y_bottom,\n",
    "                tags=tags,\n",
    "            )[\"mean\"],\n",
    "            np.finfo(np.float64).eps,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Min-Trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MinTrace(HReconciler):\n",
    "    \"\"\"MinTrace Reconciliation Class.\n",
    "\n",
    "    This reconciliation algorithm proposed by Wickramasuriya et al. depends on a generalized least squares estimator\n",
    "    and an estimator of the covariance matrix of the coherency errors $\\mathbf{W}_{h}$. The Min Trace algorithm\n",
    "    minimizes the squared errors for the coherent forecasts under an unbiasedness assumption; the solution has a\n",
    "    closed form.<br>\n",
    "\n",
    "    $$\n",
    "    \\mathbf{P}_{\\\\text{MinT}}=\\\\left(\\mathbf{S}^{\\intercal}\\mathbf{W}_{h}\\mathbf{S}\\\\right)^{-1}\n",
    "    \\mathbf{S}^{\\intercal}\\mathbf{W}^{-1}_{h}\n",
    "    $$\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `method`: str, one of `ols`, `wls_struct`, `wls_var`, `mint_shrink`, `mint_cov`.<br>\n",
    "    `nonnegative`: bool, reconciled forecasts should be nonnegative?<br>\n",
    "    `mint_shr_ridge`: float=2e-8, ridge numeric protection to MinTrace-shr covariance estimator.<br>\n",
    "    `num_threads`: int=1, number of threads to use for solving the optimization problems (when nonnegative=True).\n",
    "\n",
    "    **References:**<br>\n",
    "    - [Wickramasuriya, S. L., Athanasopoulos, G., & Hyndman, R. J. (2019). \\\"Optimal forecast reconciliation for\n",
    "    hierarchical and grouped time series through trace minimization\\\". Journal of the American Statistical Association,\n",
    "    114 , 804–819. doi:10.1080/01621459.2018.1448825.](https://robjhyndman.com/publications/mint/).\n",
    "    - [Wickramasuriya, S.L., Turlach, B.A. & Hyndman, R.J. (2020). \\\"Optimal non-negative\n",
    "    forecast reconciliation\". Stat Comput 30, 1167–1182,\n",
    "    https://doi.org/10.1007/s11222-020-09930-0](https://robjhyndman.com/publications/nnmint/).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        method: str,\n",
    "        nonnegative: bool = False,\n",
    "        mint_shr_ridge: Optional[float] = 2e-8,\n",
    "        num_threads: int = 1,\n",
    "    ):\n",
    "        self.method = method\n",
    "        self.nonnegative = nonnegative\n",
    "        self.insample = method in [\"wls_var\", \"mint_cov\", \"mint_shrink\"]\n",
    "        if method == \"mint_shrink\":\n",
    "            self.mint_shr_ridge = mint_shr_ridge\n",
    "        self.num_threads = num_threads\n",
    "        if not self.nonnegative and self.num_threads > 1:\n",
    "            warnings.warn(\"`num_threads` is only used when `nonnegative=True`\")\n",
    "\n",
    "    def _get_PW_matrices(\n",
    "        self,\n",
    "        S: np.ndarray,\n",
    "        y_hat: np.ndarray,\n",
    "        y_insample: Optional[np.ndarray] = None,\n",
    "        y_hat_insample: Optional[np.ndarray] = None,\n",
    "        idx_bottom: Optional[list[int]] = None,\n",
    "    ):\n",
    "        # shape residuals_insample (n_hiers, obs)\n",
    "        res_methods = [\"wls_var\", \"mint_cov\", \"mint_shrink\"]\n",
    "        if self.method in res_methods and (y_insample is None or y_hat_insample is None):\n",
    "            raise ValueError(\n",
    "                f\"Check `Y_df`. For method `{self.method}` you need to pass insample predictions and insample values.\"\n",
    "            )\n",
    "        n_hiers, n_bottom = S.shape\n",
    "        n_aggs = n_hiers - n_bottom\n",
    "        # Construct J and U.T\n",
    "        J = np.concatenate(\n",
    "            (np.zeros((n_bottom, n_aggs), dtype=np.float64), S[n_aggs:]), axis=1\n",
    "        )\n",
    "        Ut = np.concatenate((np.eye(n_aggs, dtype=np.float64), -S[:n_aggs]), axis=1)\n",
    "        if self.method == \"ols\":\n",
    "            W = np.eye(n_hiers)\n",
    "            UtW = Ut\n",
    "        elif self.method == \"wls_struct\":\n",
    "            Wdiag = np.sum(S, axis=1, dtype=np.float64)\n",
    "            UtW = Ut * Wdiag\n",
    "            W = np.diag(Wdiag)\n",
    "        elif (\n",
    "            self.method in res_methods\n",
    "            and y_insample is not None\n",
    "            and y_hat_insample is not None\n",
    "        ):\n",
    "            # Residuals with shape (obs, n_hiers)\n",
    "            residuals = (y_insample - y_hat_insample).T\n",
    "            n, _ = residuals.shape\n",
    "\n",
    "            # Protection: against overfitted model\n",
    "            residuals_sum = np.sum(residuals, axis=0)\n",
    "            zero_residual_prc = np.abs(residuals_sum) < 1e-4\n",
    "            zero_residual_prc = np.mean(zero_residual_prc)\n",
    "            if zero_residual_prc > 0.98:\n",
    "                raise Exception(\n",
    "                    f\"Insample residuals close to 0, zero_residual_prc={zero_residual_prc}. Check `Y_df`\"\n",
    "                )\n",
    "\n",
    "            if self.method == \"wls_var\":\n",
    "                Wdiag = (\n",
    "                    np.nansum(residuals**2, axis=0, dtype=np.float64)\n",
    "                    / residuals.shape[0]\n",
    "                )\n",
    "                Wdiag += np.full(n_hiers, 2e-8, dtype=np.float64)\n",
    "                W = np.diag(Wdiag)\n",
    "                UtW = Ut * Wdiag\n",
    "            elif self.method == \"mint_cov\":\n",
    "                # Compute nans\n",
    "                nan_mask = np.isnan(residuals.T)\n",
    "                if np.any(nan_mask):\n",
    "                    W = _ma_cov(residuals.T, ~nan_mask)\n",
    "                else:\n",
    "                    W = np.cov(residuals.T)\n",
    "\n",
    "                UtW = Ut @ W\n",
    "            elif self.method == \"mint_shrink\":\n",
    "                # Compute nans\n",
    "                nan_mask = np.isnan(residuals.T)\n",
    "                # Compute shrunk empirical covariance\n",
    "                if np.any(nan_mask):\n",
    "                    W = _shrunk_covariance_schaferstrimmer_with_nans(\n",
    "                        residuals.T, ~nan_mask, self.mint_shr_ridge\n",
    "                    )\n",
    "                else:\n",
    "                    W = _shrunk_covariance_schaferstrimmer_no_nans(\n",
    "                        residuals.T, self.mint_shr_ridge\n",
    "                    )\n",
    "\n",
    "                UtW = Ut @ W\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown reconciliation method {self.method}\")\n",
    "\n",
    "        try:\n",
    "            P = (\n",
    "                J\n",
    "                - np.linalg.solve(\n",
    "                    UtW[:, n_aggs:] @ Ut.T[n_aggs:] + UtW[:, :n_aggs],\n",
    "                    UtW[:, n_aggs:] @ J.T[n_aggs:],\n",
    "                ).T\n",
    "                @ Ut\n",
    "            )\n",
    "        except np.linalg.LinAlgError:\n",
    "            if self.method == \"mint_shrink\":\n",
    "                raise Exception(\n",
    "                    f\"min_trace ({self.method}) is ill-conditioned. Increase the value of parameter 'mint_shr_ridge' or use another reconciliation method.\"\n",
    "                )\n",
    "            else:\n",
    "                raise Exception(\n",
    "                    f\"min_trace ({self.method}) is ill-conditioned. Please use another reconciliation method.\"\n",
    "                )\n",
    "\n",
    "        return P, W\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        S,\n",
    "        y_hat,\n",
    "        y_insample: Optional[np.ndarray] = None,\n",
    "        y_hat_insample: Optional[np.ndarray] = None,\n",
    "        sigmah: Optional[np.ndarray] = None,\n",
    "        intervals_method: Optional[str] = None,\n",
    "        num_samples: Optional[int] = None,\n",
    "        seed: Optional[int] = None,\n",
    "        tags: Optional[dict[str, np.ndarray]] = None,\n",
    "        idx_bottom: Optional[np.ndarray] = None,\n",
    "    ):\n",
    "        \"\"\"MinTrace Fit Method.\n",
    "\n",
    "        **Parameters:**<br>\n",
    "        `S`: Summing matrix of size (`base`, `bottom`).<br>\n",
    "        `y_hat`: Forecast values of size (`base`, `horizon`).<br>\n",
    "        `y_insample`: Insample values of size (`base`, `insample_size`). Only used with \"wls_var\", \"mint_cov\", \"mint_shrink\".<br>\n",
    "        `y_hat_insample`: Insample forecast values of size (`base`, `insample_size`). Only used with \"wls_var\", \"mint_cov\", \"mint_shrink\"<br>\n",
    "        `sigmah`: Estimated standard deviation of the conditional marginal distribution.<br>\n",
    "        `intervals_method`: Sampler for prediction intervals, one of `normality`, `bootstrap`, `permbu`.<br>\n",
    "        `num_samples`: Number of samples for probabilistic coherent distribution.<br>\n",
    "        `seed`: Seed for reproducibility.<br>\n",
    "        `tags`: Each key is a level and each value its `S` indices.<br>\n",
    "        `idx_bottom`: Indices corresponding to the bottom level of `S`, size (`bottom`).<br>\n",
    "\n",
    "        **Returns:**<br>\n",
    "        `self`: object, fitted reconciler.\n",
    "        \"\"\"\n",
    "        self.y_hat = y_hat\n",
    "        self.P, self.W = self._get_PW_matrices(\n",
    "            S=S,\n",
    "            y_hat=y_hat,\n",
    "            y_insample=y_insample,\n",
    "            y_hat_insample=y_hat_insample,\n",
    "            idx_bottom=idx_bottom,\n",
    "        )\n",
    "\n",
    "        if self.nonnegative:\n",
    "            _, n_bottom = S.shape\n",
    "            W_inv = np.linalg.pinv(self.W)\n",
    "            negatives = y_hat < 0\n",
    "            if negatives.any():\n",
    "                warnings.warn(\"Replacing negative forecasts with zero.\")\n",
    "                y_hat = np.copy(y_hat)\n",
    "                y_hat[negatives] = 0.0\n",
    "            # Quadratic progamming formulation\n",
    "            # here we are solving the quadratic programming problem\n",
    "            # formulated in the origial paper\n",
    "            # https://robjhyndman.com/publications/nnmint/\n",
    "            # The library quadprog was chosen\n",
    "            # based on these benchmarks:\n",
    "            # https://scaron.info/blog/quadratic-programming-in-python.html\n",
    "            a = S.T @ W_inv\n",
    "            G = a @ S\n",
    "            try:\n",
    "                _ = np.linalg.cholesky(G)\n",
    "            except np.linalg.LinAlgError:\n",
    "                raise Exception(\n",
    "                    f\"min_trace ({self.method}) is ill-conditioned. Try setting nonnegative=False or use another reconciliation method.\"\n",
    "                )\n",
    "            C = np.eye(n_bottom)\n",
    "            b = np.zeros(n_bottom)\n",
    "            # the quadratic programming problem\n",
    "            # returns the forecasts of the bottom series\n",
    "            if self.num_threads == 1:\n",
    "                bottom_fcts = np.apply_along_axis(\n",
    "                    lambda y_hat: solve_qp(G=G, a=a @ y_hat, C=C, b=b)[0],\n",
    "                    axis=0,\n",
    "                    arr=y_hat,\n",
    "                )\n",
    "            else:\n",
    "                futures = []\n",
    "                with ThreadPoolExecutor(self.num_threads) as executor:\n",
    "                    for j in range(y_hat.shape[1]):\n",
    "                        future = executor.submit(\n",
    "                            solve_qp, G=G, a=a @ y_hat[:, j], C=C, b=b\n",
    "                        )\n",
    "                        futures.append(future)\n",
    "                    bottom_fcts = np.hstack([f.result()[0][:, None] for f in futures])\n",
    "            if not np.all(bottom_fcts > -1e-8):\n",
    "                raise Exception(\"nonnegative optimization failed\")\n",
    "            # remove negative values close to zero\n",
    "            bottom_fcts = np.clip(np.float32(bottom_fcts), a_min=0, a_max=None)\n",
    "            self.y_hat = S @ bottom_fcts  # Hack\n",
    "\n",
    "            # Overwrite P, W and sampler attributes with BottomUp's\n",
    "            self.P, self.W = BottomUp()._get_PW_matrices(S=S, idx_bottom=idx_bottom)\n",
    "\n",
    "        self.sampler = self._get_sampler(\n",
    "            S=S,\n",
    "            P=self.P,\n",
    "            W=self.W,\n",
    "            y_hat=y_hat,\n",
    "            y_insample=y_insample,\n",
    "            y_hat_insample=y_hat_insample,\n",
    "            sigmah=sigmah,\n",
    "            intervals_method=intervals_method,\n",
    "            num_samples=num_samples,\n",
    "            seed=seed,\n",
    "            tags=tags,\n",
    "        )\n",
    "        self.fitted = True\n",
    "        return self\n",
    "\n",
    "    def fit_predict(\n",
    "        self,\n",
    "        S: np.ndarray,\n",
    "        y_hat: np.ndarray,\n",
    "        idx_bottom: np.ndarray = None,\n",
    "        y_insample: Optional[np.ndarray] = None,\n",
    "        y_hat_insample: Optional[np.ndarray] = None,\n",
    "        sigmah: Optional[np.ndarray] = None,\n",
    "        level: Optional[list[int]] = None,\n",
    "        intervals_method: Optional[str] = None,\n",
    "        num_samples: Optional[int] = None,\n",
    "        seed: Optional[int] = None,\n",
    "        tags: Optional[dict[str, np.ndarray]] = None,\n",
    "    ):\n",
    "        \"\"\"MinTrace Reconciliation Method.\n",
    "\n",
    "        **Parameters:**<br>\n",
    "        `S`: Summing matrix of size (`base`, `bottom`).<br>\n",
    "        `y_hat`: Forecast values of size (`base`, `horizon`).<br>\n",
    "        `idx_bottom`: Indices corresponding to the bottom level of `S`, size (`bottom`).<br>\n",
    "        `y_insample`: Insample values of size (`base`, `insample_size`). Only used by `wls_var`, `mint_cov`, `mint_shrink`<br>\n",
    "        `y_hat_insample`: Insample fitted values of size (`base`, `insample_size`). Only used by `wls_var`, `mint_cov`, `mint_shrink`<br>\n",
    "        `sigmah`: Estimated standard deviation of the conditional marginal distribution.<br>\n",
    "        `level`: float list 0-100, confidence levels for prediction intervals.<br>\n",
    "        `intervals_method`: Sampler for prediction intervals, one of `normality`, `bootstrap`, `permbu`.<br>\n",
    "        `num_samples`: Number of samples for probabilistic coherent distribution.<br>\n",
    "        `seed`: Seed for reproducibility.<br>\n",
    "        `tags`: Each key is a level and each value its `S` indices.<br>\n",
    "        \n",
    "        **Returns:**<br>\n",
    "        `y_tilde`: Reconciliated y_hat using the MinTrace approach.\n",
    "        \"\"\"\n",
    "        if self.nonnegative:\n",
    "            if (level is not None) and intervals_method in [\"bootstrap\", \"permbu\"]:\n",
    "                raise ValueError(\n",
    "                    \"nonnegative reconciliation is not compatible with bootstrap or permbu forecasts\"\n",
    "                )\n",
    "            if idx_bottom is None:\n",
    "                raise ValueError(\"`idx_bottom` cannot be None with nonnegative reconciliation\")\n",
    "\n",
    "        # Fit creates P, W and sampler attributes\n",
    "        self.fit(\n",
    "            S=S,\n",
    "            y_hat=y_hat,\n",
    "            y_insample=y_insample,\n",
    "            y_hat_insample=y_hat_insample,\n",
    "            sigmah=sigmah,\n",
    "            intervals_method=intervals_method,\n",
    "            num_samples=num_samples,\n",
    "            seed=seed,\n",
    "            tags=tags,\n",
    "            idx_bottom=idx_bottom,\n",
    "        )\n",
    "\n",
    "        return self._reconcile(\n",
    "            S=S, P=self.P, y_hat=self.y_hat, level=level, sampler=self.sampler\n",
    "        )\n",
    "\n",
    "    __call__ = fit_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(MinTrace, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(MinTrace.fit, name=\"MinTrace.fit\", title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(MinTrace.predict, name=\"MinTrace.predict\", title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(MinTrace.fit_predict, name=\"MinTrace.fit_predict\", title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(MinTrace.sample, name=\"MinTrace.sample\", title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MinTraceSparse(MinTrace):\n",
    "    \"\"\"MinTraceSparse Reconciliation Class.\n",
    "\n",
    "    This is the implementation of OLS and WLS estimators using sparse matrices. It is not guaranteed\n",
    "    to give identical results to the non-sparse version, but works much more efficiently on data sets\n",
    "    with many time series.<br>\n",
    "\n",
    "    See the parent class for more details.<br>\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `method`: str, one of `ols`, `wls_struct`, or `wls_var`.<br>\n",
    "    `nonnegative`: bool, return non-negative reconciled forecasts.<br>\n",
    "    `num_threads`: int, number of threads to execute non-negative quadratic programming calls.<br>\n",
    "    `qp`: bool, implement non-negativity constraint with a quadratic programming approach. Setting \n",
    "    this to True generally gives better results, but at the expense of higher cost to compute. <br>\n",
    "    \"\"\"\n",
    "\n",
    "    is_sparse_method = True\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        method: str,\n",
    "        nonnegative: bool = False,\n",
    "        num_threads: int = 1,\n",
    "        qp: bool = True,\n",
    "    ) -> None:\n",
    "        if method not in [\"ols\", \"wls_struct\", \"wls_var\"]:\n",
    "            raise NotImplementedError(\n",
    "                f\"`{method}` is not supported for MinTraceSparse. Choose from `ols`, `wls_struct`, or `wls_var`.\"\n",
    "            )\n",
    "        # Call the parent constructor.\n",
    "        super().__init__(method, nonnegative, num_threads=num_threads)\n",
    "        # Assign the attributes specific to the sparse class.\n",
    "        self.qp = qp\n",
    "\n",
    "    def _get_PW_matrices(\n",
    "        self,\n",
    "        S: Union[np.ndarray, sparse.spmatrix],\n",
    "        y_hat: np.ndarray,\n",
    "        y_insample: Optional[np.ndarray] = None,\n",
    "        y_hat_insample: Optional[np.ndarray] = None,\n",
    "        idx_bottom: Optional[list[int]] = None,\n",
    "    ):\n",
    "        # shape residuals_insample (n_hiers, obs)\n",
    "        res_methods = [\"wls_var\", \"mint_cov\", \"mint_shrink\"]\n",
    "\n",
    "        S = sparse.csr_matrix(S)\n",
    "\n",
    "        if self.method in res_methods and (y_insample is None or y_hat_insample is None):\n",
    "            raise ValueError(\n",
    "                f\"Check `Y_df`. For method `{self.method}` you need to pass insample predictions and insample values.\"\n",
    "            )\n",
    "        n_hiers, n_bottom = S.shape\n",
    "\n",
    "        if self.method == \"ols\":\n",
    "            W_diag = np.ones(n_hiers)\n",
    "        elif self.method == \"wls_struct\":\n",
    "            W_diag = S @ np.ones((n_bottom,))\n",
    "        elif (\n",
    "            self.method == \"wls_var\"\n",
    "            and y_insample is not None\n",
    "            and y_hat_insample is not None\n",
    "        ):\n",
    "            # Residuals with shape (obs, n_hiers)\n",
    "            residuals = (y_insample - y_hat_insample).T\n",
    "            n, _ = residuals.shape\n",
    "\n",
    "            # Protection: against overfitted model\n",
    "            residuals_sum = np.sum(residuals, axis=0)\n",
    "            zero_residual_prc = np.abs(residuals_sum) < 1e-4\n",
    "            zero_residual_prc = np.mean(zero_residual_prc)\n",
    "            if zero_residual_prc > 0.98:\n",
    "                raise Exception(\n",
    "                    f\"Insample residuals close to 0, zero_residual_prc={zero_residual_prc}. Check `Y_df`\"\n",
    "                )\n",
    "\n",
    "            # Protection: cases where data is unavailable/nan\n",
    "            # makoren: this masking stuff causes more harm than good, I found the results in the presence\n",
    "            # of nan-s can often be rubbish, I'd argue it's better to fail than give rubbish results, here\n",
    "            # the code is simply failing if it encounters nan in the variance vector.\n",
    "            # masked_res = np.ma.array(residuals, mask=np.isnan(residuals))\n",
    "            # covm = np.ma.cov(masked_res, rowvar=False, allow_masked=True).data\n",
    "\n",
    "            W_diag = np.nanvar(residuals, axis=0, ddof=1)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown reconciliation method {self.method}\")\n",
    "\n",
    "        if any(W_diag < 1e-8):\n",
    "            raise Exception(\n",
    "                f\"min_trace ({self.method}) needs covariance matrix to be positive definite.\"\n",
    "            )\n",
    "\n",
    "        if any(np.isnan(W_diag)):\n",
    "            raise Exception(\n",
    "                f\"min_trace ({self.method}) needs covariance matrix to be positive definite (not nan).\"\n",
    "            )\n",
    "\n",
    "        M = sparse.spdiags(np.reciprocal(W_diag), 0, W_diag.size, W_diag.size)\n",
    "        R = sparse.csr_matrix(S.T @ M)\n",
    "\n",
    "        # The implementation of P acting on a vector:\n",
    "        def get_P_action(y):\n",
    "            b = R @ y\n",
    "\n",
    "            A = sparse.linalg.LinearOperator(\n",
    "                (b.size, b.size), matvec=lambda v: R @ (S @ v)\n",
    "            )\n",
    "\n",
    "            x_tilde, exit_code = sparse.linalg.bicgstab(A, b, atol=1e-5)\n",
    "\n",
    "            return x_tilde\n",
    "\n",
    "        P = sparse.linalg.LinearOperator(\n",
    "            (S.shape[1], y_hat.shape[0]), matvec=get_P_action\n",
    "        )\n",
    "        W = sparse.spdiags(W_diag, 0, W_diag.size, W_diag.size)\n",
    "\n",
    "        return P, W\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        S: sparse.csr_matrix,\n",
    "        y_hat: np.ndarray,\n",
    "        y_insample: Optional[np.ndarray] = None,\n",
    "        y_hat_insample: Optional[np.ndarray] = None,\n",
    "        sigmah: Optional[np.ndarray] = None,\n",
    "        intervals_method: Optional[str] = None,\n",
    "        num_samples: Optional[int] = None,\n",
    "        seed: Optional[int] = None,\n",
    "        tags: Optional[dict[str, np.ndarray]] = None,\n",
    "        idx_bottom: Optional[np.ndarray] = None,\n",
    "    ) -> \"MinTraceSparse\":\n",
    "        \"\"\"MinTraceSparse Fit Method.\n",
    "\n",
    "        **Parameters:**<br>\n",
    "        `S`: Summing matrix of size (`base`, `bottom`).<br>\n",
    "        `y_hat`: Forecast values of size (`base`, `horizon`).<br>\n",
    "        `y_insample`: Insample values of size (`base`, `insample_size`). Only used with \"wls_var\".<br>\n",
    "        `y_hat_insample`: Insample forecast values of size (`base`, `insample_size`). Only used with \"wls_var\"<br>\n",
    "        `sigmah`: Estimated standard deviation of the conditional marginal distribution.<br>\n",
    "        `intervals_method`: Sampler for prediction intervals, one of `normality`, `bootstrap`, `permbu`.<br>\n",
    "        `num_samples`: Number of samples for probabilistic coherent distribution.<br>\n",
    "        `seed`: Seed for reproducibility.<br>\n",
    "        `tags`: Each key is a level and each value its `S` indices.<br>\n",
    "        `idx_bottom`: Indices corresponding to the bottom level of `S`, size (`bottom`).<br>\n",
    "\n",
    "        **Returns:**<br>\n",
    "        `self`: object, fitted reconciler.\n",
    "        \"\"\"\n",
    "        if self.nonnegative:\n",
    "            # Clip the base forecasts to align them with their use in practice.\n",
    "            self.y_hat = np.clip(y_hat, 0, None)\n",
    "            # Get the number of nodes, leaf nodes, and parent nodes.\n",
    "            n, n_b = S.shape\n",
    "            n_a = n - n_b\n",
    "            # Find the optimal non-negative forecasts.\n",
    "            if self.qp:\n",
    "                # Get the diagonal weight matrix, i.e., precision matrix, for\n",
    "                # the problem.\n",
    "                if self.method == \"ols\":\n",
    "                    W = sparse.eye(n, format=\"csc\")\n",
    "                elif self.method == \"wls_struct\":\n",
    "                    W = sparse.csc_matrix(\n",
    "                        (\n",
    "                            (1.0 / S.sum(axis=1)).A1,\n",
    "                            np.arange(n, dtype=np.min_scalar_type(n - 1)),\n",
    "                            np.arange(n + 1, dtype=np.min_scalar_type(n)),\n",
    "                        )\n",
    "                    )\n",
    "                elif self.method == \"wls_var\":\n",
    "                    # Check that we have the in-sample values.\n",
    "                    if y_insample is None or y_hat_insample is None:\n",
    "                        raise ValueError(\n",
    "                            \"`y_insample` and `y_hat_insample` are required to calculate residuals.\"\n",
    "                        )\n",
    "                    # Add a small jitter to the variance to improve the condition\n",
    "                    # of the variance matrix.\n",
    "                    W = sparse.csc_matrix(\n",
    "                        (\n",
    "                            1.0\n",
    "                            / (\n",
    "                                np.nanvar(y_insample - y_hat_insample, 1, ddof=1) + 2e-8\n",
    "                            ),\n",
    "                            np.arange(n, dtype=np.min_scalar_type(n - 1)),\n",
    "                            np.arange(n + 1, dtype=np.min_scalar_type(n)),\n",
    "                        )\n",
    "                    )\n",
    "                # Get the linear constraints matrix by vertically stacking the\n",
    "                # (n_a x n) constraint matrix in the zero-constrained\n",
    "                # represenation, which has the set of all reconciled forecasts\n",
    "                # in its null space, and a horizontally stacked (n_b x n_a)\n",
    "                # zero matrix and a negated (n_b x n_b) identity matrix.\n",
    "                A = sparse.vstack(\n",
    "                    (\n",
    "                        sparse.hstack(\n",
    "                            (sparse.eye(n_a, format=\"csc\"), -S[:n_a, :].tocsc())\n",
    "                        ),\n",
    "                        -sparse.eye(n_b, n, n_a, format=\"csc\"),\n",
    "                    )\n",
    "                )\n",
    "                # Get the linear constraints vector.\n",
    "                b = np.zeros(n)\n",
    "                # Get the composition of convex cones to solve the problem.\n",
    "                cones = [clarabel.ZeroConeT(n_a), clarabel.NonnegativeConeT(n_b)]\n",
    "                # Set up the settings for the solver.\n",
    "                settings = clarabel.DefaultSettings()\n",
    "                settings.verbose = False\n",
    "\n",
    "                def solve_clarabel(\n",
    "                    y: np.ndarray,\n",
    "                    P: sparse.csc_matrix,\n",
    "                    A: sparse.csc_matrix,\n",
    "                    b: np.ndarray,\n",
    "                    cones: list,\n",
    "                    settings: clarabel.DefaultSettings,\n",
    "                    n_b: int,\n",
    "                ) -> tuple[bool, Optional[np.ndarray]]:\n",
    "                    # Get the linear coefficients, i.e., the cost vector.\n",
    "                    q = P @ -y\n",
    "                    # Set up the Clarabel solver.\n",
    "                    solver = clarabel.DefaultSolver(P, q, A, b, cones, settings)\n",
    "                    # Solve the problem.\n",
    "                    solution = solver.solve()\n",
    "                    # Resolve the solver exit status.\n",
    "                    if status := solution.status == clarabel.SolverStatus.Solved:\n",
    "                        # Return the slice of the primal solution that\n",
    "                        # represents the optimal non-negative reconciled\n",
    "                        # bottom level forecasts.\n",
    "                        return status, solution.x[-n_b:]\n",
    "                    else:\n",
    "                        # As the solver failed, discard the empty primal\n",
    "                        # solution.\n",
    "                        return status, None\n",
    "\n",
    "                with ThreadPoolExecutor(self.num_threads) as executor:\n",
    "                    # Dispatch the jobs.\n",
    "                    futures = [\n",
    "                        executor.submit(\n",
    "                            solve_clarabel, y, W, A, b, cones, settings, n_b\n",
    "                        )\n",
    "                        for y in self.y_hat.transpose()\n",
    "                    ]\n",
    "                    # Yield the futures as they complete.\n",
    "                    for future in as_completed(futures):\n",
    "                        # Return the exit status of the solver and the primal\n",
    "                        # solution.\n",
    "                        status, x = future.result()\n",
    "                        # Check that the problem is successfully solved and the\n",
    "                        # primal solution is within tolerance.\n",
    "                        if not (status and np.min(x) > -1e-8):\n",
    "                            raise Exception(\"Non-negative optimisation failed.\")\n",
    "\n",
    "                # Extract the optimal non-negative reconciled bottom level\n",
    "                # forecasts.\n",
    "                x = np.vstack([future.result()[1] for future in futures]).transpose()\n",
    "                # Clip the negative forecasts within tolerance.\n",
    "                x = np.clip(x, 0, None)\n",
    "                # Aggregate the clipped bottom level forecasts and overwrite\n",
    "                # the base forecasts with the solution.\n",
    "                self.y_hat = S @ x\n",
    "                # Overwrite the attributes for the P and W matrices with those\n",
    "                # for bottom-up reconciliation to force projection onto the\n",
    "                # non-negative coherent subspace.\n",
    "                self.P, self.W = BottomUpSparse()._get_PW_matrices(S=S, idx_bottom=None)\n",
    "            else:\n",
    "                # Get the reconciliation matrices.\n",
    "                self.P, self.W = self._get_PW_matrices(\n",
    "                    S=S,\n",
    "                    y_hat=self.y_hat,\n",
    "                    y_insample=y_insample,\n",
    "                    y_hat_insample=y_hat_insample,\n",
    "                    idx_bottom=idx_bottom,\n",
    "                )\n",
    "                # Although it is now sufficient to ensure that all of the\n",
    "                # entries in P are positive, as it is implemented as a linear\n",
    "                # operator for the iterative method to solve the sparse linear\n",
    "                # system, we need to reconcile to find if any of the coherent\n",
    "                # bottom level point forecasts are negative.\n",
    "                y_tilde = self._reconcile(\n",
    "                    S=S, P=self.P, y_hat=self.y_hat, level=None, sampler=None\n",
    "                )[\"mean\"][-n_b:, :]\n",
    "                # Find if any of the forecasts are negative.\n",
    "                if np.any(y_tilde < 0):\n",
    "                    # Clip the negative forecasts.\n",
    "                    y_tilde = np.clip(y_tilde, 0, None)\n",
    "                    # Force non-negative coherence by overwriting the base\n",
    "                    # forecasts with the aggregated, clipped bottom level\n",
    "                    # forecasts.\n",
    "                    self.y_hat = S @ y_tilde\n",
    "                    # Overwrite the attributes for the P and W matrices with\n",
    "                    # those for bottom-up reconciliation to force projection\n",
    "                    # onto the non-negative coherent subspace.\n",
    "                    self.P, self.W = BottomUpSparse()._get_PW_matrices(\n",
    "                        S=S, idx_bottom=None\n",
    "                    )\n",
    "        else:\n",
    "            # Get the reconciliation matrices.\n",
    "            self.y_hat = y_hat\n",
    "            self.P, self.W = self._get_PW_matrices(\n",
    "                S=S,\n",
    "                y_hat=self.y_hat,\n",
    "                y_insample=y_insample,\n",
    "                y_hat_insample=y_hat_insample,\n",
    "                idx_bottom=idx_bottom,\n",
    "            )\n",
    "\n",
    "        # Get the sampler for probabilistic reconciliation.\n",
    "        self.sampler = self._get_sampler(\n",
    "            S=S,\n",
    "            P=self.P,\n",
    "            W=self.W,\n",
    "            y_hat=self.y_hat,\n",
    "            y_insample=y_insample,\n",
    "            y_hat_insample=y_hat_insample,\n",
    "            sigmah=sigmah,\n",
    "            intervals_method=intervals_method,\n",
    "            num_samples=num_samples,\n",
    "            seed=seed,\n",
    "            tags=tags,\n",
    "        )\n",
    "        # Set the instance as fitted.\n",
    "        self.fitted = True\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(MinTraceSparse, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(MinTraceSparse.fit, name=\"MinTraceSparse.fit\", title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(MinTraceSparse.predict, name=\"MinTraceSparse.predict\", title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(MinTraceSparse.fit_predict, name=\"MinTraceSparse.fit_predict\", title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(MinTraceSparse.sample, name=\"MinTraceSparse.sample\", title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "for method in [\"ols\", \"wls_struct\", \"wls_var\", \"mint_shrink\"]:\n",
    "    for nonnegative in [False, True]:\n",
    "        # test nonnegative behavior\n",
    "        # we should be able to recover the same forecasts\n",
    "        # in this example\n",
    "        cls_min_trace = MinTrace(method=method, nonnegative=nonnegative)\n",
    "        assert cls_min_trace.nonnegative is nonnegative\n",
    "        if cls_min_trace.insample:\n",
    "            assert method in [\"wls_var\", \"mint_cov\", \"mint_shrink\"]\n",
    "            test_close(\n",
    "                cls_min_trace(\n",
    "                    S=S,\n",
    "                    y_hat=S @ y_hat_bottom,\n",
    "                    y_insample=S @ y_bottom,\n",
    "                    y_hat_insample=S @ y_hat_bottom_insample,\n",
    "                    idx_bottom=idx_bottom if nonnegative else None,\n",
    "                )[\"mean\"],\n",
    "                S @ y_hat_bottom,\n",
    "            )\n",
    "        else:\n",
    "            test_close(\n",
    "                cls_min_trace(\n",
    "                    S=S,\n",
    "                    y_hat=S @ y_hat_bottom,\n",
    "                    idx_bottom=idx_bottom if nonnegative else None,\n",
    "                )[\"mean\"],\n",
    "                S @ y_hat_bottom,\n",
    "            )\n",
    "mintrace_1thr = MinTrace(method=\"ols\", nonnegative=False, num_threads=1).fit(\n",
    "    S=S, y_hat=S @ y_hat_bottom\n",
    ")\n",
    "mintrace_2thr = MinTrace(method=\"ols\", nonnegative=False, num_threads=2).fit(\n",
    "    S=S, y_hat=S @ y_hat_bottom\n",
    ")\n",
    "np.testing.assert_allclose(mintrace_1thr.y_hat, mintrace_2thr.y_hat)\n",
    "with ExceptionExpected(regex=\"min_trace (mint_cov)*\"):\n",
    "    for nonnegative in [False, True]:\n",
    "        cls_min_trace = MinTrace(method=\"mint_cov\", nonnegative=nonnegative)\n",
    "        cls_min_trace(\n",
    "            S=S,\n",
    "            y_hat=S @ y_hat_bottom,\n",
    "            y_insample=S @ y_bottom,\n",
    "            y_hat_insample=S @ y_hat_bottom_insample,\n",
    "            idx_bottom=idx_bottom if nonnegative else None,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# MinTrace-shr covariance's stress test\n",
    "diff_len_y_insample = S @ y_bottom\n",
    "diff_len_y_hat_insample = S @ y_hat_bottom_insample\n",
    "diff_len_y_insample[-1, :-1] = np.nan\n",
    "diff_len_y_hat_insample[-1, :-1] = np.nan\n",
    "cls_min_trace = MinTrace(method=\"mint_shrink\")\n",
    "result_min_trace = cls_min_trace(\n",
    "    S=S,\n",
    "    y_hat=S @ y_hat_bottom,\n",
    "    y_insample=diff_len_y_insample,\n",
    "    y_hat_insample=diff_len_y_hat_insample,\n",
    "    idx_bottom=idx_bottom,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test levels\n",
    "for method in [\"ols\", \"wls_struct\", \"wls_var\", \"mint_shrink\"]:\n",
    "    for nonnegative in [False, True]:\n",
    "        cls_min_trace = MinTrace(method=method, nonnegative=nonnegative)\n",
    "        test_close(\n",
    "            cls_min_trace(\n",
    "                S=S,\n",
    "                y_hat=S @ y_hat_bottom,\n",
    "                y_insample=S @ y_bottom,\n",
    "                y_hat_insample=S @ y_hat_bottom_insample,\n",
    "                idx_bottom=idx_bottom if nonnegative else None,\n",
    "            )[\"mean\"],\n",
    "            S @ y_hat_bottom,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# Test the sparse functionality.\n",
    "for method in [\"ols\", \"wls_struct\", \"wls_var\"]:\n",
    "    # Check both the non-negative heuristic and QP solutions.\n",
    "    for nonnegative, qp in [(False, False), (True, False), (True, True)]:\n",
    "        cls_min_trace = MinTraceSparse(method=method, nonnegative=nonnegative, qp=qp)\n",
    "        test_close(\n",
    "            cls_min_trace(\n",
    "                S=sparse.csr_matrix(S),\n",
    "                y_hat=S @ y_hat_bottom,\n",
    "                y_insample=S @ y_bottom,\n",
    "                y_hat_insample=S @ y_hat_bottom_insample,\n",
    "                idx_bottom=idx_bottom,\n",
    "            )[\"mean\"],\n",
    "            S @ y_hat_bottom,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Optimal Combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class OptimalCombination(MinTrace):\n",
    "    \"\"\"Optimal Combination Reconciliation Class.\n",
    "\n",
    "    This reconciliation algorithm was proposed by Hyndman et al. 2011, the method uses generalized least squares\n",
    "    estimator using the coherency errors covariance matrix. Consider the covariance of the base forecast\n",
    "    $\\\\textrm{Var}(\\epsilon_{h}) = \\Sigma_{h}$, the $\\mathbf{P}$ matrix of this method is defined by:\n",
    "    $$ \\mathbf{P} = \\\\left(\\mathbf{S}^{\\intercal}\\Sigma_{h}^{\\dagger}\\mathbf{S}\\\\right)^{-1}\\mathbf{S}^{\\intercal}\\Sigma^{\\dagger}_{h}$$\n",
    "    where $\\Sigma_{h}^{\\dagger}$ denotes the variance pseudo-inverse. The method was later proven equivalent to\n",
    "    `MinTrace` variants.\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `method`: str, allowed optimal combination methods: 'ols', 'wls_struct'.<br>\n",
    "    `nonnegative`: bool, reconciled forecasts should be nonnegative?<br>\n",
    "\n",
    "    **References:**<br>\n",
    "    - [Rob J. Hyndman, Roman A. Ahmed, George Athanasopoulos, Han Lin Shang (2010). \\\"Optimal Combination Forecasts for\n",
    "    Hierarchical Time Series\\\".](https://robjhyndman.com/papers/Hierarchical6.pdf).<br>\n",
    "    - [Shanika L. Wickramasuriya, George Athanasopoulos and Rob J. Hyndman (2010). \\\"Optimal Combination Forecasts for\n",
    "    Hierarchical Time Series\\\".](https://robjhyndman.com/papers/MinT.pdf).\n",
    "    - [Wickramasuriya, S.L., Turlach, B.A. & Hyndman, R.J. (2020). \\\"Optimal non-negative\n",
    "    forecast reconciliation\". Stat Comput 30, 1167–1182,\n",
    "    https://doi.org/10.1007/s11222-020-09930-0](https://robjhyndman.com/publications/nnmint/).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, method: str, nonnegative: bool = False, num_threads: int = 1):\n",
    "        comb_methods = [\"ols\", \"wls_struct\"]\n",
    "        if method not in comb_methods:\n",
    "            raise ValueError(\n",
    "                f'Optimal Combination class does not support method: \"{method}\"'\n",
    "            )\n",
    "        super().__init__(\n",
    "            method=method, nonnegative=nonnegative, num_threads=num_threads\n",
    "        )\n",
    "        self.insample = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(OptimalCombination, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(OptimalCombination.fit, name=\"OptimalCombination.fit\", title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(OptimalCombination.predict, name=\"OptimalCombination.predict\", title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(\n",
    "    OptimalCombination.fit_predict, name=\"OptimalCombination.fit_predict\", title_level=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(OptimalCombination.sample, name=\"OptimalCombination.sample\", title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "for method in [\"ols\", \"wls_struct\"]:\n",
    "    for nonnegative in [False, True]:\n",
    "        # test nonnegative behavior\n",
    "        # we should be able to recover the same forecasts\n",
    "        # in this example\n",
    "        cls_optimal_combination = OptimalCombination(\n",
    "            method=method, nonnegative=nonnegative\n",
    "        )\n",
    "        test_close(\n",
    "            cls_optimal_combination(\n",
    "                S=S,\n",
    "                y_hat=S @ y_hat_bottom,\n",
    "                idx_bottom=idx_bottom if nonnegative else None,\n",
    "            )[\"mean\"],\n",
    "            S @ y_hat_bottom,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Emp. Risk Minimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ERM(HReconciler):\n",
    "    \"\"\"Optimal Combination Reconciliation Class.\n",
    "\n",
    "    The Empirical Risk Minimization reconciliation strategy relaxes the unbiasedness assumptions from\n",
    "    previous reconciliation methods like MinT and optimizes square errors between the reconciled predictions\n",
    "    and the validation data to obtain an optimal reconciliation matrix P.\n",
    "\n",
    "    The exact solution for $\\mathbf{P}$ (`method='closed'`) follows the expression:\n",
    "    $$\\mathbf{P}^{*} = \\\\left(\\mathbf{S}^{\\intercal}\\mathbf{S}\\\\right)^{-1}\\mathbf{Y}^{\\intercal}\\hat{\\mathbf{Y}}\\\\left(\\hat{\\mathbf{Y}}\\hat{\\mathbf{Y}}\\\\right)^{-1}$$\n",
    "\n",
    "    The alternative Lasso regularized $\\mathbf{P}$ solution (`method='reg_bu'`) is useful when the observations\n",
    "    of validation data is limited or the exact solution has low numerical stability.\n",
    "    $$\\mathbf{P}^{*} = \\\\text{argmin}_{\\mathbf{P}} ||\\mathbf{Y}-\\mathbf{S} \\mathbf{P} \\hat{Y} ||^{2}_{2} + \\lambda ||\\mathbf{P}-\\mathbf{P}_{\\\\text{BU}}||_{1}$$\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `method`: str, one of `closed`, `reg` and `reg_bu`.<br>\n",
    "    `lambda_reg`: float, l1 regularizer for `reg` and `reg_bu`.<br>\n",
    "\n",
    "    **References:**<br>\n",
    "    - [Ben Taieb, S., & Koo, B. (2019). Regularized regression for hierarchical forecasting without\n",
    "    unbiasedness conditions. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge\n",
    "    Discovery & Data Mining KDD '19 (p. 1337-1347). New York, NY, USA: Association for Computing Machinery.](https://doi.org/10.1145/3292500.3330976).<br>\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, method: str, lambda_reg: float = 1e-2):\n",
    "        self.method = method\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.insample = True\n",
    "\n",
    "    def _get_PW_matrices(\n",
    "        self,\n",
    "        S: np.ndarray,\n",
    "        y_hat: np.ndarray,\n",
    "        y_insample: np.ndarray,\n",
    "        y_hat_insample: np.ndarray,\n",
    "        idx_bottom: np.ndarray,\n",
    "    ):\n",
    "        n_hiers, n_bottom = S.shape\n",
    "        # y_hat_insample shape (n_hiers, obs)\n",
    "        if y_insample is None or y_hat_insample is None:\n",
    "            raise ValueError(\n",
    "                \"Check `Y_df`. For method `ERM` you need to pass insample predictions and insample values.\"\n",
    "            )\n",
    "        # remove obs with nan values\n",
    "        nan_idx = np.isnan(y_hat_insample).any(axis=0)\n",
    "        y_insample = y_insample[:, ~nan_idx]\n",
    "        y_hat_insample = y_hat_insample[:, ~nan_idx]\n",
    "        # only using h validation steps to avoid\n",
    "        # computational burden\n",
    "        # print(y_hat.shape)\n",
    "        h = min(y_hat.shape[1], y_hat_insample.shape[1])\n",
    "        y_hat_insample = y_hat_insample[:, -h:]  # shape (h, n_hiers)\n",
    "        y_insample = y_insample[:, -h:]\n",
    "        if self.method == \"closed\":\n",
    "            B = np.linalg.inv(S.T @ S) @ S.T @ y_insample\n",
    "            B = B.T\n",
    "            P = np.linalg.pinv(y_hat_insample.T) @ B\n",
    "            P = P.T\n",
    "        elif self.method == \"reg\":\n",
    "            X = np.kron(S, y_hat_insample.T)\n",
    "            z = y_insample.reshape(-1)\n",
    "\n",
    "            if self.lambda_reg is None:\n",
    "                lambda_reg = np.max(np.abs(X.T.dot(z)))\n",
    "            else:\n",
    "                lambda_reg = self.lambda_reg\n",
    "\n",
    "            beta = _lasso(X, z, lambda_reg, max_iters=1000, tol=1e-4)\n",
    "            P = beta.reshape(S.shape).T\n",
    "        elif self.method == \"reg_bu\":\n",
    "            X = np.kron(S, y_hat_insample.T)\n",
    "            Pbu = np.zeros_like(S)\n",
    "            Pbu[idx_bottom] = S[idx_bottom]\n",
    "            z = y_insample.reshape(-1) - X @ Pbu.reshape(-1)\n",
    "\n",
    "            if self.lambda_reg is None:\n",
    "                lambda_reg = np.max(np.abs(X.T.dot(z)))\n",
    "            else:\n",
    "                lambda_reg = self.lambda_reg\n",
    "\n",
    "            beta = _lasso(X, z, lambda_reg, max_iters=1000, tol=1e-4)\n",
    "            P = beta + Pbu.reshape(-1)\n",
    "            P = P.reshape(S.shape).T\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown reconciliation method {self.method}\")\n",
    "\n",
    "        W = np.eye(n_hiers, dtype=np.float64)\n",
    "\n",
    "        return P, W\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        S,\n",
    "        y_hat,\n",
    "        y_insample,\n",
    "        y_hat_insample,\n",
    "        sigmah: Optional[np.ndarray] = None,\n",
    "        intervals_method: Optional[str] = None,\n",
    "        num_samples: Optional[int] = None,\n",
    "        seed: Optional[int] = None,\n",
    "        tags: Optional[dict[str, np.ndarray]] = None,\n",
    "        idx_bottom: Optional[np.ndarray] = None,\n",
    "    ):\n",
    "        \"\"\"ERM Fit Method.\n",
    "\n",
    "        **Parameters:**<br>\n",
    "        `S`: Summing matrix of size (`base`, `bottom`).<br>\n",
    "        `y_hat`: Forecast values of size (`base`, `horizon`).<br>\n",
    "        `y_insample`: Train values of size (`base`, `insample_size`).<br>\n",
    "        `y_hat_insample`: Insample train predictions of size (`base`, `insample_size`).<br>\n",
    "        `sigmah`: Estimated standard deviation of the conditional marginal distribution.<br>\n",
    "        `intervals_method`: Sampler for prediction intervals, one of `normality`, `bootstrap`, `permbu`.<br>\n",
    "        `num_samples`: Number of samples for probabilistic coherent distribution.<br>\n",
    "        `seed`: Seed for reproducibility.<br>\n",
    "        `tags`: Each key is a level and each value its `S` indices.<br>\n",
    "        `idx_bottom`: Indices corresponding to the bottom level of `S`, size (`bottom`).<br>\n",
    "        \n",
    "        **Returns:**<br>\n",
    "        `self`: object, fitted reconciler.\n",
    "        \"\"\"\n",
    "        self.P, self.W = self._get_PW_matrices(\n",
    "            S=S,\n",
    "            y_hat=y_hat,\n",
    "            y_insample=y_insample,\n",
    "            y_hat_insample=y_hat_insample,\n",
    "            idx_bottom=idx_bottom,\n",
    "        )\n",
    "        self.sampler = self._get_sampler(\n",
    "            S=S,\n",
    "            P=self.P,\n",
    "            W=self.W,\n",
    "            y_hat=y_hat,\n",
    "            y_insample=y_insample,\n",
    "            y_hat_insample=y_hat_insample,\n",
    "            sigmah=sigmah,\n",
    "            intervals_method=intervals_method,\n",
    "            num_samples=num_samples,\n",
    "            seed=seed,\n",
    "            tags=tags,\n",
    "        )\n",
    "        self.fitted = True\n",
    "        return self\n",
    "\n",
    "    def fit_predict(\n",
    "        self,\n",
    "        S: np.ndarray,\n",
    "        y_hat: np.ndarray,\n",
    "        idx_bottom: np.ndarray = None,\n",
    "        y_insample: Optional[np.ndarray] = None,\n",
    "        y_hat_insample: Optional[np.ndarray] = None,\n",
    "        sigmah: Optional[np.ndarray] = None,\n",
    "        level: Optional[list[int]] = None,\n",
    "        intervals_method: Optional[str] = None,\n",
    "        num_samples: Optional[int] = None,\n",
    "        seed: Optional[int] = None,\n",
    "        tags: Optional[dict[str, np.ndarray]] = None,\n",
    "    ):\n",
    "        \"\"\"ERM Reconciliation Method.\n",
    "\n",
    "        **Parameters:**<br>\n",
    "        `S`: Summing matrix of size (`base`, `bottom`).<br>\n",
    "        `y_hat`: Forecast values of size (`base`, `horizon`).<br>\n",
    "        `idx_bottom`: Indices corresponding to the bottom level of `S`, size (`bottom`).<br>\n",
    "        `y_insample`: Train values of size (`base`, `insample_size`).<br>\n",
    "        `y_hat_insample`: Insample train predictions of size (`base`, `insample_size`).<br>\n",
    "        `sigmah`: Estimated standard deviation of the conditional marginal distribution.<br>\n",
    "        `level`: float list 0-100, confidence levels for prediction intervals.<br>\n",
    "        `intervals_method`: Sampler for prediction intervals, one of `normality`, `bootstrap`, `permbu`.<br>\n",
    "        `num_samples`: Number of samples for probabilistic coherent distribution.<br>\n",
    "        `seed`: Seed for reproducibility.<br>\n",
    "        `tags`: Each key is a level and each value its `S` indices.<br>\n",
    "\n",
    "        **Returns:**<br>\n",
    "        `y_tilde`: Reconciliated y_hat using the ERM approach.\n",
    "        \"\"\"\n",
    "        # Fit creates P, W and sampler attributes\n",
    "        self.fit(\n",
    "            S=S,\n",
    "            y_hat=y_hat,\n",
    "            y_insample=y_insample,\n",
    "            y_hat_insample=y_hat_insample,\n",
    "            sigmah=sigmah,\n",
    "            intervals_method=intervals_method,\n",
    "            num_samples=num_samples,\n",
    "            seed=seed,\n",
    "            tags=tags,\n",
    "            idx_bottom=idx_bottom,\n",
    "        )\n",
    "\n",
    "        return self._reconcile(\n",
    "            S=S, P=self.P, y_hat=y_hat, level=level, sampler=self.sampler\n",
    "        )\n",
    "\n",
    "    __call__ = fit_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(ERM, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(ERM.fit, name=\"ERM.fit\", title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(ERM.predict, name=\"ERM.predict\", title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(ERM.fit_predict, name=\"ERM.fit_predict\", title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(ERM.sample, name=\"ERM.sample\", title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "for method in [\"reg_bu\"]:\n",
    "    cls_erm = ERM(method=method, lambda_reg=None)\n",
    "    test_close(\n",
    "        cls_erm(\n",
    "            S=S,\n",
    "            y_hat=S @ y_hat_bottom,\n",
    "            y_insample=S @ y_bottom,\n",
    "            y_hat_insample=S @ y_hat_bottom_insample,\n",
    "            idx_bottom=idx_bottom,\n",
    "        )[\"mean\"],\n",
    "        S @ y_hat_bottom,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S @ y_hat_bottom_insample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test intervals\n",
    "reconciler_args = dict(\n",
    "    S=S,\n",
    "    y_hat=y_hat_base,\n",
    "    y_insample=y_base,\n",
    "    y_hat_insample=y_hat_base_insample,\n",
    "    sigmah=sigmah,\n",
    "    level=[80, 90],\n",
    "    intervals_method=\"normality\",\n",
    "    num_samples=200,\n",
    "    seed=0,\n",
    "    tags=tags,\n",
    "    idx_bottom=idx_bottom,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test normality prediction intervals\n",
    "# we should recover the original sigmah\n",
    "# for the bottom time series\n",
    "cls_bottom_up = BottomUp()\n",
    "test_eq(cls_bottom_up(**reconciler_args)[\"sigmah\"][idx_bottom], sigmah[idx_bottom])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test normality interval's names\n",
    "cls_bottom_up = BottomUp()\n",
    "bu_bootstrap_intervals = cls_bottom_up(**reconciler_args)\n",
    "test_eq([\"mean\", \"sigmah\", \"quantiles\"], list(bu_bootstrap_intervals.keys()))\n",
    "\n",
    "# test PERMBU interval's names\n",
    "reconciler_args[\"intervals_method\"] = \"permbu\"\n",
    "bu_permbu_intervals = cls_bottom_up(**reconciler_args)\n",
    "test_eq([\"mean\", \"quantiles\"], list(bu_permbu_intervals.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test TopDown + intervals\n",
    "for method in [\"average_proportions\", \"proportion_averages\"]:\n",
    "    for intervals_method in [\"normality\", \"bootstrap\", \"permbu\"]:\n",
    "        cls_top_down = TopDown(method=method)\n",
    "        reconciler_args[\"intervals_method\"] = \"permbu\"\n",
    "        cls_top_down(**reconciler_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test MinTrace + intervals\n",
    "nonnegative = False\n",
    "for method in [\"ols\", \"wls_struct\", \"wls_var\", \"mint_shrink\"]:\n",
    "    for intervals_method in [\"normality\", \"bootstrap\", \"permbu\"]:\n",
    "        cls_min_trace = MinTrace(method=method, nonnegative=nonnegative)\n",
    "        reconciler_args[\"intervals_method\"] = intervals_method\n",
    "        cls_min_trace(**reconciler_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test OptimalCombination + intervals\n",
    "nonnegative = False\n",
    "for method in [\"ols\", \"wls_struct\"]:\n",
    "    for intervals_method in [\"normality\", \"bootstrap\", \"permbu\"]:\n",
    "        cls_optimal_combination = OptimalCombination(\n",
    "            method=method, nonnegative=nonnegative\n",
    "        )\n",
    "        reconciler_args[\"intervals_method\"] = intervals_method\n",
    "        if not nonnegative:\n",
    "            cls_optimal_combination(**reconciler_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test ERM + intervals\n",
    "for method in [\"reg_bu\"]:\n",
    "    for intervals_method in [\"normality\", \"bootstrap\", \"permbu\"]:\n",
    "        cls_erm = ERM(method=method, lambda_reg=None)\n",
    "        reconciler_args[\"intervals_method\"] = intervals_method\n",
    "        cls_erm(**reconciler_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test coherent sample's shape\n",
    "reconciler_args = dict(\n",
    "    S=S,\n",
    "    y_hat=y_hat_base,\n",
    "    y_insample=y_base,\n",
    "    y_hat_insample=y_hat_base_insample,\n",
    "    sigmah=sigmah,\n",
    "    intervals_method=\"bootstrap\",\n",
    "    tags=tags,\n",
    "    idx_bottom=idx_bottom,\n",
    ")\n",
    "\n",
    "cls_bottom_up = BottomUp()\n",
    "shapes = []\n",
    "for intervals_method in [\"normality\", \"bootstrap\", \"permbu\"]:\n",
    "    cls_bottom_up.fit(**reconciler_args)\n",
    "    coherent_samples = cls_bottom_up.sample(num_samples=100)\n",
    "    shapes.append(coherent_samples.shape)\n",
    "test_eq(shapes[0], shapes[1])\n",
    "test_eq(shapes[0], shapes[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "### General Reconciliation\n",
    "- [Orcutt, G.H., Watts, H.W., & Edwards, J.B.(1968). Data aggregation and information loss. The American \n",
    "Economic Review, 58 , 773(787).](http://www.jstor.org/stable/1815532)<br>\n",
    "- [Disaggregation methods to expedite product line forecasting. Journal of Forecasting, 9 , 233–254. \n",
    "doi:10.1002/for.3980090304](https://onlinelibrary.wiley.com/doi/abs/10.1002/for.3980090304).<br>\n",
    "- [An investigation of aggregate variable time series forecast strategies with specific subaggregate \n",
    "time series statistical correlation. Computers and Operations Research, 26 , 1133–1149. \n",
    "doi:10.1016/S0305-0548(99)00017-9.](https://doi.org/10.1016/S0305-0548(99)00017-9)<br>\n",
    "- [Hyndman, R.J., & Athanasopoulos, G. (2021). \"Forecasting: principles and practice, 3rd edition: \n",
    "Chapter 11: Forecasting hierarchical and grouped series.\". OTexts: Melbourne, Australia. OTexts.com/fpp3 \n",
    "Accessed on July 2022.](https://otexts.com/fpp3/hierarchical.html)\n",
    "\n",
    "### Optimal Reconciliation\n",
    "- [Rob J. Hyndman, Roman A. Ahmed, George Athanasopoulos, Han Lin Shang. \"Optimal Combination Forecasts for \n",
    "Hierarchical Time Series\" (2010).](https://robjhyndman.com/papers/Hierarchical6.pdf)<br>\n",
    "- [Shanika L. Wickramasuriya, George Athanasopoulos and Rob J. Hyndman. \"Optimal Combination Forecasts for \n",
    "Hierarchical Time Series\" (2010).](https://robjhyndman.com/papers/MinT.pdf)<br>\n",
    "- [Ben Taieb, S., & Koo, B. (2019). Regularized regression for hierarchical forecasting without \n",
    "unbiasedness conditions. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge \n",
    "Discovery & Data Mining KDD '19 (p. 1337-1347). New York, NY, USA: Association for Computing Machinery.](https://doi.org/10.1145/3292500.3330976)<br>\n",
    "\n",
    "### Hierarchical Probabilistic Coherent Predictions\n",
    "- [Puwasala Gamakumara Ph. D. dissertation. Monash University, Econometrics and Business Statistics. \"Probabilistic Forecast Reconciliation\".](https://bridges.monash.edu/articles/thesis/Probabilistic_Forecast_Reconciliation_Theory_and_Applications/11869533)<br>\n",
    "- [Taieb, Souhaib Ben and Taylor, James W and Hyndman, Rob J. (2017). Coherent probabilistic forecasts for hierarchical time series. International conference on machine learning ICML.](https://proceedings.mlr.press/v70/taieb17a.html)<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
