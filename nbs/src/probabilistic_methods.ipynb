{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp probabilistic_methods"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probabilistic Methods"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we provide a collection of methods designed to provide hierarchically coherent probabilistic distributions, \n",
    "which means that they generate samples of multivariate time series with hierarchical linear constraints.\n",
    "\n",
    "We designed these methods to extend the `core.HierarchicalForecast` capabilities class. Check their [usage example here](https://nixtla.github.io/hierarchicalforecast/examples/example.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import warnings\n",
    "from typing import Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from hierarchicalforecast.utils import is_strictly_hierarchical, cov2corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import add_docs, show_doc\n",
    "from fastcore.test import ExceptionExpected, test_close, test_eq, test_fail\n",
    "\n",
    "from hierarchicalforecast.methods import BottomUp, TopDown, MiddleOut, MinTrace, OptimalCombination, ERM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Normality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Normality:\n",
    "    \"\"\" Normality Probabilistic Reconciliation Class.\n",
    "\n",
    "    The Normality method leverages the Gaussian Distribution linearity, to\n",
    "    generate hierarchically coherent prediction distributions. This class is \n",
    "    meant to be used as the `sampler` input as other `HierarchicalForecast` [reconciliation classes](https://nixtla.github.io/hierarchicalforecast/methods.html).\n",
    "\n",
    "    Given base forecasts under a normal distribution:\n",
    "    $$\\hat{y}_{h} \\sim \\mathrm{N}(\\hat{\\\\boldsymbol{\\\\mu}}, \\hat{\\mathbf{W}}_{h})$$\n",
    "\n",
    "    The reconciled forecasts are also normally distributed:\n",
    "\n",
    "    $$\n",
    "    \\\\tilde{y}_{h} \\sim \\mathrm{N}(\\mathbf{S}\\mathbf{P}\\hat{\\\\boldsymbol{\\\\mu}}, \n",
    "    \\mathbf{S}\\mathbf{P}\\hat{\\mathbf{W}}_{h} \\mathbf{P}^{\\intercal} \\mathbf{S}^{\\intercal})\n",
    "    $$\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `S`: np.array, summing matrix of size (`base`, `bottom`).<br>\n",
    "    `P`: np.array, reconciliation matrix of size (`bottom`, `base`).<br>\n",
    "    `y_hat`: Point forecasts values of size (`base`, `horizon`).<br>\n",
    "    `W`: np.array, hierarchical covariance matrix of size (`base`, `base`).<br>\n",
    "    `sigmah`: np.array, forecast standard dev. of size (`base`, `horizon`).<br>\n",
    "    `num_samples`: int, number of bootstraped samples generated.<br>\n",
    "    `seed`: int, random seed for numpy generator's replicability.<br>    \n",
    "\n",
    "    **References:**<br>\n",
    "    - [Panagiotelis A., Gamakumara P. Athanasopoulos G., and Hyndman R. J. (2022).\n",
    "    \"Probabilistic forecast reconciliation: Properties, evaluation and score optimisation\". European Journal of Operational Research.](https://www.sciencedirect.com/science/article/pii/S0377221722006087)\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 S: np.ndarray,\n",
    "                 P: np.ndarray,\n",
    "                 y_hat: np.ndarray,\n",
    "                 sigmah: np.ndarray,\n",
    "                 W: np.ndarray,\n",
    "                 seed: int = 0):\n",
    "        self.S = S\n",
    "        self.P = P\n",
    "        self.y_hat = y_hat\n",
    "        self.SP = self.S @ self.P\n",
    "        self.W = W\n",
    "        self.sigmah = sigmah\n",
    "        self.seed = seed\n",
    "\n",
    "        # Base Normality Errors assume independence/diagonal covariance\n",
    "        # TODO: replace bilinearity with elementwise row multiplication\n",
    "        R1 = cov2corr(self.W)\n",
    "        Wh = [np.diag(sigma) @ R1 @ np.diag(sigma).T for sigma in self.sigmah.T]\n",
    "\n",
    "        # Reconciled covariances across forecast horizon\n",
    "        self.cov_rec = [(self.SP @ W @ self.SP.T) for W in Wh]\n",
    "        self.sigmah_rec = np.hstack([np.sqrt(np.diag(cov))[:, None] for cov in self.cov_rec])\n",
    "\n",
    "    def get_samples(self, num_samples: int):\n",
    "        \"\"\"Normality Coherent Samples.\n",
    "\n",
    "        Obtains coherent samples under the Normality assumptions.\n",
    "\n",
    "        **Parameters:**<br>\n",
    "        `num_samples`: int, number of samples generated from coherent distribution.<br>\n",
    "\n",
    "        **Returns:**<br>\n",
    "        `samples`: Coherent samples of size (`base`, `horizon`, `num_samples`).\n",
    "        \"\"\"\n",
    "        state = np.random.RandomState(self.seed)\n",
    "        n_series, n_horizon = self.y_hat.shape\n",
    "        samples = np.empty(shape=(num_samples, n_series, n_horizon))\n",
    "        for t in range(n_horizon):\n",
    "            with warnings.catch_warnings():\n",
    "                # Avoid 'RuntimeWarning: covariance is not positive-semidefinite.'\n",
    "                # By definition the multivariate distribution is not full-rank\n",
    "                partial_samples = state.multivariate_normal(mean=self.SP @ self.y_hat[:,t],\n",
    "                                                    cov=self.cov_rec[t], size=num_samples)\n",
    "            samples[:,:,t] = partial_samples\n",
    "\n",
    "        # [samples, N, H] -> [N, H, samples]\n",
    "        samples = samples.transpose((1, 2, 0))\n",
    "        return samples\n",
    "\n",
    "    def get_prediction_levels(self, res, level):\n",
    "        \"\"\" Adds reconciled forecast levels to results dictionary \"\"\"\n",
    "        res['sigmah'] = self.sigmah_rec\n",
    "        level = np.asarray(level)\n",
    "        z = norm.ppf(0.5 + level / 200)\n",
    "        for zs, lv in zip(z, level):\n",
    "            res[f'lo-{lv}'] = res['mean'] - zs * self.sigmah_rec\n",
    "            res[f'hi-{lv}'] = res['mean'] + zs * self.sigmah_rec\n",
    "        return res\n",
    "\n",
    "    def get_prediction_quantiles(self, res, quantiles):\n",
    "        \"\"\" Adds reconciled forecast quantiles to results dictionary \"\"\"\n",
    "        # [N,H,None] + [None None,Q] * [N,H,None] -> [N,H,Q]\n",
    "        z = norm.ppf(quantiles)\n",
    "        res['sigmah'] = self.sigmah_rec\n",
    "        res['quantiles'] = res['mean'][:,:,None] + z[None,None,:] * self.sigmah_rec[:,:,None]\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(Normality, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(Normality.get_samples, title_level=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "class Bootstrap:\n",
    "    \"\"\" Bootstrap Probabilistic Reconciliation Class.\n",
    "\n",
    "    This method goes beyond the normality assumption for the base forecasts,\n",
    "    the technique simulates future sample paths and uses them to generate\n",
    "    base sample paths that are latered reconciled. This clever idea and its\n",
    "    simplicity allows to generate coherent bootstraped prediction intervals\n",
    "    for any reconciliation strategy. This class is meant to be used as the `sampler` \n",
    "    input as other `HierarchicalForecast` [reconciliation classes](https://nixtla.github.io/hierarchicalforecast/methods.html).\n",
    "\n",
    "    Given a boostraped set of simulated sample paths:\n",
    "    $$(\\hat{\\mathbf{y}}^{[1]}_{\\\\tau}, \\dots ,\\hat{\\mathbf{y}}^{[B]}_{\\\\tau})$$\n",
    "\n",
    "    The reconciled sample paths allow for reconciled distributional forecasts:\n",
    "    $$(\\mathbf{S}\\mathbf{P}\\hat{\\mathbf{y}}^{[1]}_{\\\\tau}, \\dots ,\\mathbf{S}\\mathbf{P}\\hat{\\mathbf{y}}^{[B]}_{\\\\tau})$$\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `S`: np.array, summing matrix of size (`base`, `bottom`).<br>\n",
    "    `P`: np.array, reconciliation matrix of size (`bottom`, `base`).<br>\n",
    "    `y_hat`: Point forecasts values of size (`base`, `horizon`).<br>\n",
    "    `y_insample`: Insample values of size (`base`, `insample_size`).<br>\n",
    "    `y_hat_insample`: Insample point forecasts of size (`base`, `insample_size`).<br>\n",
    "    `num_samples`: int, number of bootstraped samples generated.<br>\n",
    "    `seed`: int, random seed for numpy generator's replicability.<br>\n",
    "\n",
    "    **References:**<br>\n",
    "    - [Puwasala Gamakumara Ph. D. dissertation. Monash University, Econometrics and Business Statistics (2020).\n",
    "    \"Probabilistic Forecast Reconciliation\"](https://bridges.monash.edu/articles/thesis/Probabilistic_Forecast_Reconciliation_Theory_and_Applications/11869533)\n",
    "    - [Panagiotelis A., Gamakumara P. Athanasopoulos G., and Hyndman R. J. (2022).\n",
    "    \"Probabilistic forecast reconciliation: Properties, evaluation and score optimisation\". European Journal of Operational Research.](https://www.sciencedirect.com/science/article/pii/S0377221722006087)\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 S: np.ndarray,\n",
    "                 P: np.ndarray,\n",
    "                 y_hat: np.ndarray,\n",
    "                 y_insample: np.ndarray,\n",
    "                 y_hat_insample: np.ndarray,\n",
    "                 num_samples: int=100,\n",
    "                 seed: int = 0,\n",
    "                 W: np.ndarray = None):\n",
    "        self.S = S\n",
    "        self.P = P\n",
    "        self.W = W\n",
    "        self.y_hat = y_hat\n",
    "        self.y_insample = y_insample\n",
    "        self.y_hat_insample = y_hat_insample\n",
    "        self.num_samples = num_samples\n",
    "        self.seed = seed\n",
    "\n",
    "    def get_samples(self, num_samples: int):\n",
    "        \"\"\"Bootstrap Sample Reconciliation Method.\n",
    "\n",
    "        Applies Bootstrap sample reconciliation method as defined by Gamakumara 2020.\n",
    "        Generating independent sample paths and reconciling them with Bootstrap.\n",
    "\n",
    "        **Parameters:**<br>\n",
    "        `num_samples`: int, number of samples generated from coherent distribution.<br>\n",
    "\n",
    "        **Returns:**<br>\n",
    "        `samples`: Coherent samples of size (`base`, `horizon`, `num_samples`).\n",
    "        \"\"\"\n",
    "        residuals = self.y_insample - self.y_hat_insample\n",
    "        h = self.y_hat.shape[1]\n",
    "\n",
    "        #removing nas from residuals\n",
    "        residuals = residuals[:, np.isnan(residuals).sum(axis=0) == 0]\n",
    "        sample_idx = np.arange(residuals.shape[1] - h)\n",
    "        state = np.random.RandomState(self.seed)\n",
    "        samples_idx = state.choice(sample_idx, size=num_samples)\n",
    "        samples = [self.y_hat + residuals[:, idx:(idx + h)] for idx in samples_idx]\n",
    "        SP = self.S @ self.P\n",
    "        samples = np.apply_along_axis(lambda path: np.matmul(SP, path),\n",
    "                                      axis=1, arr=samples)\n",
    "        samples_np = np.stack(samples)\n",
    "\n",
    "        # [samples, N, H] -> [N, H, samples]\n",
    "        samples_np = samples_np.transpose((1, 2, 0))\n",
    "        return samples_np\n",
    "\n",
    "    def get_prediction_levels(self, res, level):\n",
    "        \"\"\" Adds reconciled forecast levels to results dictionary \"\"\"\n",
    "        samples = self.get_samples(num_samples=self.num_samples)\n",
    "        for lv in level:\n",
    "            min_q = (100 - lv) / 200\n",
    "            max_q = min_q + lv / 100\n",
    "            res[f'lo-{lv}'] = np.quantile(samples, min_q, axis=2)\n",
    "            res[f'hi-{lv}'] = np.quantile(samples, max_q, axis=2)\n",
    "        return res\n",
    "\n",
    "    def get_prediction_quantiles(self, res, quantiles):\n",
    "        \"\"\" Adds reconciled forecast quantiles to results dictionary \"\"\"\n",
    "        samples = self.get_samples(num_samples=self.num_samples)\n",
    "\n",
    "        # [Q, N, H] -> [N, H, Q]\n",
    "        sample_quantiles = np.quantile(samples, quantiles, axis=2)\n",
    "        res['quantiles'] = sample_quantiles.transpose((1, 2, 0))\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(Bootstrap, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(Bootstrap.get_samples, title_level=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. PERMBU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "class PERMBU:\n",
    "    \"\"\" PERMBU Probabilistic Reconciliation Class.\n",
    "\n",
    "    The PERMBU method leverages empirical bottom-level marginal distributions \n",
    "    with empirical copula functions (describing bottom-level dependencies) to \n",
    "    generate the distribution of aggregate-level distributions using BottomUp \n",
    "    reconciliation. The sample reordering technique in the PERMBU method reinjects \n",
    "    multivariate dependencies into independent bottom-level samples.\n",
    "\n",
    "        Algorithm:\n",
    "        1.   For all series compute conditional marginals distributions.\n",
    "        2.   Compute residuals $\\hat{\\epsilon}_{i,t}$ and obtain rank permutations.\n",
    "        2.   Obtain K-sample from the bottom-level series predictions.\n",
    "        3.   Apply recursively through the hierarchical structure:<br>\n",
    "            3.1.   For a given aggregate series $i$ and its children series:<br>\n",
    "            3.2.   Obtain children's empirical joint using sample reordering copula.<br>\n",
    "            3.2.   From the children's joint obtain the aggregate series's samples.    \n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `S`: np.array, summing matrix of size (`base`, `bottom`).<br>\n",
    "    `tags`: Each key is a level and each value its `S` indices.<br>\n",
    "    `y_insample`: Insample values of size (`base`, `insample_size`).<br>\n",
    "    `y_hat_insample`: Insample point forecasts of size (`base`, `insample_size`).<br>\n",
    "    `sigmah`: np.array, forecast standard dev. of size (`base`, `horizon`).<br>\n",
    "    `num_samples`: int, number of normal prediction samples generated.<br>\n",
    "    `seed`: int, random seed for numpy generator's replicability.<br>\n",
    "\n",
    "    **References:**<br>\n",
    "    - [Taieb, Souhaib Ben and Taylor, James W and Hyndman, Rob J. (2017). \n",
    "    Coherent probabilistic forecasts for hierarchical time series. \n",
    "    International conference on machine learning ICML.](https://proceedings.mlr.press/v70/taieb17a.html)\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 S: np.ndarray,\n",
    "                 tags: Dict[str, np.ndarray],\n",
    "                 y_hat: np.ndarray,\n",
    "                 y_insample: np.ndarray,\n",
    "                 y_hat_insample: np.ndarray,\n",
    "                 sigmah: np.ndarray,\n",
    "                 num_samples: Optional[int] = None,\n",
    "                 seed: int=0,\n",
    "                 P: np.ndarray = None):\n",
    "        # PERMBU only works for strictly hierarchical structures\n",
    "        if not is_strictly_hierarchical(S, tags):\n",
    "            raise ValueError('PERMBU probabilistic reconciliation requires strictly hierarchical structures.')\n",
    "        self.S = S\n",
    "        self.P = P\n",
    "        self.y_hat = y_hat\n",
    "        self.y_insample = y_insample\n",
    "        self.y_hat_insample = y_hat_insample\n",
    "        self.sigmah = sigmah\n",
    "        self.num_samples = num_samples\n",
    "        self.seed = seed\n",
    "\n",
    "    def _obtain_ranks(self, array):\n",
    "        \"\"\" Vector ranks\n",
    "\n",
    "        Efficiently obtain vector ranks.\n",
    "        Example `array=[4,2,7,1]` -> `ranks=[2, 1, 3, 0]`.\n",
    "\n",
    "        **Parameters**<br>\n",
    "        `array`: np.array, matrix with floats or integers on which the \n",
    "                ranks will be computed on the second dimension.<br>\n",
    "\n",
    "        **Returns**<br>\n",
    "        `ranks`: np.array, matrix with ranks along the second dimension.<br>\n",
    "        \"\"\"\n",
    "        temp = array.argsort(axis=1)\n",
    "        ranks = np.empty_like(temp)\n",
    "        a_range = np.arange(temp.shape[1])\n",
    "        for i_row in range(temp.shape[0]):\n",
    "            ranks[i_row, temp[i_row,:]] = a_range\n",
    "        return ranks\n",
    "\n",
    "    def _permutate_samples(self, samples, permutations):\n",
    "        \"\"\" Permutate Samples\n",
    "\n",
    "        Applies efficient vectorized permutation on the samples.\n",
    "\n",
    "        **Parameters**<br>\n",
    "        `samples`: np.array [series,samples], independent base samples.<br>\n",
    "        `permutations`: np.array [series,samples], permutation ranks with wich\n",
    "                  which `samples` dependence will be restored see `_obtain_ranks`.<br>\n",
    "\n",
    "        **Returns**<br>\n",
    "        `permutated_samples`: np.array.<br>\n",
    "        \"\"\"\n",
    "        # Generate auxiliary and flat permutation indexes\n",
    "        n_rows, n_cols = permutations.shape\n",
    "        aux_row_idx = np.arange(n_rows)[:,None] * n_cols\n",
    "        aux_row_idx = np.repeat(aux_row_idx, repeats=n_cols, axis=1)\n",
    "        permutate_idxs = permutations.flatten() + aux_row_idx.flatten()\n",
    "\n",
    "        # Apply flat permutation indexes and recover original shape\n",
    "        permutated_samples = samples.flatten()\n",
    "        permutated_samples = permutated_samples[permutate_idxs]\n",
    "        permutated_samples = permutated_samples.reshape(n_rows, n_cols)\n",
    "        return permutated_samples\n",
    "    \n",
    "    def _permutate_predictions(self, prediction_samples, permutations):\n",
    "        \"\"\" Permutate Prediction Samples\n",
    "\n",
    "        Applies permutations to prediction_samples across the horizon.\n",
    "\n",
    "        **Parameters**<br>\n",
    "        `prediction_samples`: np.array [series,horizon,samples], independent \n",
    "                  base prediction samples.<br>\n",
    "        `permutations`: np.array [series, samples], permutation ranks with which\n",
    "                  `samples` dependence will be restored see `_obtain_ranks`.\n",
    "                  it can also apply a random permutation.<br>\n",
    "\n",
    "        **Returns**<br>\n",
    "        `permutated_prediction_samples`: np.array.<br>\n",
    "        \"\"\"\n",
    "        # Apply permutation throughout forecast horizon\n",
    "        permutated_prediction_samples = prediction_samples.copy()\n",
    "\n",
    "        _, n_horizon, _ = prediction_samples.shape\n",
    "        for t in range(n_horizon):\n",
    "            permutated_prediction_samples[:,t,:] = \\\n",
    "                              self._permutate_samples(prediction_samples[:,t,:],\n",
    "                                                      permutations)\n",
    "        return permutated_prediction_samples\n",
    "\n",
    "    def _nonzero_indexes_by_row(self, M):\n",
    "        return [np.nonzero(M[row,:])[0] for row in range(len(M))]\n",
    "\n",
    "    def get_samples(self, num_samples: Optional[int] = None):\n",
    "        \"\"\"PERMBU Sample Reconciliation Method.\n",
    "\n",
    "        Applies PERMBU reconciliation method as defined by Taieb et. al 2017.\n",
    "        Generating independent base prediction samples, restoring its multivariate\n",
    "        dependence using estimated copula with reordering and applying the BottomUp\n",
    "        aggregation to the new samples.\n",
    "\n",
    "        **Parameters:**<br>\n",
    "        `num_samples`: int, number of samples generated from coherent distribution.<br>\n",
    "\n",
    "        **Returns:**<br>\n",
    "        `samples`: Coherent samples of size (`base`, `horizon`, `num_samples`).\n",
    "        \"\"\"\n",
    "        # Compute residuals and rank permutations\n",
    "        residuals = self.y_insample - self.y_hat_insample\n",
    "        residuals = residuals[:, np.isnan(residuals).sum(axis=0) == 0]\n",
    "\n",
    "        # Sample h step-ahead base marginal distributions\n",
    "        if num_samples is None:\n",
    "            num_samples = residuals.shape[1]\n",
    "\n",
    "        # Expand residuals to match num_samples [(a,b),T] -> [(a,b),num_samples]\n",
    "        if num_samples > residuals.shape[1]:\n",
    "            residuals_idxs = np.random.choice(residuals.shape[1], size=num_samples)\n",
    "        else:\n",
    "            residuals_idxs = np.random.choice(residuals.shape[1], size=num_samples, \n",
    "                                              replace=False)\n",
    "        residuals = residuals[:,residuals_idxs]\n",
    "        rank_permutations = self._obtain_ranks(residuals)\n",
    "\n",
    "        state = np.random.RandomState(self.seed)\n",
    "        n_series, n_horizon = self.y_hat.shape\n",
    "\n",
    "        base_samples = np.array([\n",
    "            state.normal(loc=m, scale=s, size=num_samples) for m, s in \\\n",
    "            zip(self.y_hat.flatten(), self.sigmah.flatten())\n",
    "        ])\n",
    "        base_samples = base_samples.reshape(n_series, n_horizon, num_samples)\n",
    "\n",
    "        # Initialize PERMBU utility\n",
    "        rec_samples = base_samples.copy()\n",
    "        try:\n",
    "            encoder = OneHotEncoder(sparse_output=False, dtype=np.float64)\n",
    "        except TypeError:\n",
    "            encoder = OneHotEncoder(sparse=False, dtype=np.float64)\n",
    "        hier_links = np.vstack(self._nonzero_indexes_by_row(self.S.T))\n",
    "\n",
    "        # BottomUp hierarchy traversing\n",
    "        hier_levels = hier_links.shape[1]-1\n",
    "        for level_idx in reversed(range(hier_levels)):\n",
    "            # Obtain aggregation matrix from parent/children links\n",
    "            children_links = np.unique(hier_links[:,level_idx:level_idx+2], \n",
    "                                       axis=0)\n",
    "            children_idxs = np.unique(children_links[:,1])\n",
    "            parent_idxs = np.unique(children_links[:,0])\n",
    "            Agg = encoder.fit_transform(children_links).T\n",
    "            Agg = Agg[:len(parent_idxs),:]\n",
    "\n",
    "            # Permute children_samples for each prediction step\n",
    "            children_permutations = rank_permutations[children_idxs, :]\n",
    "            children_samples = rec_samples[children_idxs,:,:]\n",
    "            children_samples = self._permutate_predictions(\n",
    "                prediction_samples=children_samples,\n",
    "                permutations=children_permutations\n",
    "            )\n",
    "\n",
    "            # Overwrite hier_samples with BottomUp aggregation\n",
    "            # and randomly shuffle parent predictions after aggregation\n",
    "            parent_samples = np.einsum('ab,bhs->ahs', Agg, children_samples)\n",
    "            random_permutation = np.array([\n",
    "                np.random.permutation(np.arange(num_samples)) \\\n",
    "                for serie in range(len(parent_samples))\n",
    "            ])\n",
    "            parent_samples = self._permutate_predictions(\n",
    "                prediction_samples=parent_samples,\n",
    "                permutations=random_permutation\n",
    "            )\n",
    "\n",
    "            rec_samples[parent_idxs,:,:] = parent_samples\n",
    "        return rec_samples\n",
    "\n",
    "    def get_prediction_levels(self, res, level):\n",
    "        \"\"\" Adds reconciled forecast levels to results dictionary \"\"\"\n",
    "        samples = self.get_samples(num_samples=self.num_samples)\n",
    "        for lv in level:\n",
    "            min_q = (100 - lv) / 200\n",
    "            max_q = min_q + lv / 100\n",
    "            res[f'lo-{lv}'] = np.quantile(samples, min_q, axis=2)\n",
    "            res[f'hi-{lv}'] = np.quantile(samples, max_q, axis=2)\n",
    "        return res\n",
    "\n",
    "    def get_prediction_quantiles(self, res, quantiles):\n",
    "        \"\"\" Adds reconciled forecast quantiles to results dictionary \"\"\"\n",
    "        samples = self.get_samples(num_samples=self.num_samples)\n",
    "\n",
    "        # [Q, N, H] -> [N, H, Q]\n",
    "        sample_quantiles = np.quantile(samples, quantiles, axis=2)\n",
    "        res['quantiles'] = sample_quantiles.transpose((1, 2, 0))\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(PERMBU, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(PERMBU.get_samples, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from hierarchicalforecast.evaluation import (\n",
    "        rel_mse,\n",
    "        msse, \n",
    "        energy_score, \n",
    "        scaled_crps, \n",
    "        log_score\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "S = np.array([[1., 1., 1., 1.],\n",
    "              [1., 1., 0., 0.],\n",
    "              [0., 0., 1., 1.],\n",
    "              [0., 1., 0., 0.],\n",
    "              [1., 0., 0., 0.],\n",
    "              [0., 0., 1., 0.],\n",
    "              [0., 0., 0., 1.]])\n",
    "h = 2\n",
    "_y = np.array([10., 5., 4., 2., 1.])\n",
    "y_bottom = np.vstack([i * _y for i in range(1, 5)])\n",
    "y_hat_bottom_insample = np.roll(y_bottom, 1)\n",
    "y_hat_bottom_insample[:, 0] = np.nan\n",
    "y_hat_bottom = np.vstack([i * np.ones(h) for i in range(1, 5)])\n",
    "idx_bottom = [4, 3, 5, 6]\n",
    "tags = {'level1': np.array([0]),\n",
    "        'level2': np.array([1, 2]),\n",
    "        'level3': idx_bottom}\n",
    "\n",
    "# sigmah for all levels in the hierarchy\n",
    "# sigmah for Naive method\n",
    "# as calculated here:\n",
    "#https://otexts.com/fpp3/prediction-intervals.html\n",
    "y_base = S @ y_bottom\n",
    "y_hat_base = S @ y_hat_bottom\n",
    "y_hat_base_insample = S @ y_hat_bottom_insample\n",
    "sigma = np.nansum((y_base - y_hat_base_insample) ** 2, axis=1) / (y_base.shape[1] - 1)\n",
    "sigma = np.sqrt(sigma)\n",
    "sigmah = sigma[:, None] * np.sqrt(np.vstack([np.arange(1, h + 1) for _ in range(y_base.shape[0])]))\n",
    "noise = np.random.normal(scale=sigmah)\n",
    "y_test = y_hat_base + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# samplers for tests\n",
    "cls_bottom_up = BottomUp()\n",
    "P, W = cls_bottom_up._get_PW_matrices(S=S, idx_bottom=idx_bottom)\n",
    "\n",
    "normality_sampler = Normality(S=S, P=P, W=W,\n",
    "                              y_hat=y_hat_base,\n",
    "                              sigmah=sigmah)\n",
    "bootstrap_sampler = Bootstrap(S=S, P=P, W=W,\n",
    "                              y_hat=y_hat_base,\n",
    "                              y_insample=y_base, \n",
    "                              y_hat_insample=y_hat_base_insample,\n",
    "                              num_samples=1_000)\n",
    "empty_bootstrap_sampler = Bootstrap(S=S, P=P, W=W,\n",
    "                                    y_hat=y_hat_base,\n",
    "                                    y_insample=y_base, \n",
    "                                    y_hat_insample=y_base,\n",
    "                                    num_samples=1_000)\n",
    "permbu_sampler = PERMBU(S=S, P=P,\n",
    "                        tags=tags,\n",
    "                        y_hat=y_hat_base,\n",
    "                        y_insample=y_base, \n",
    "                        y_hat_insample=y_hat_base_insample,\n",
    "                        sigmah=sigmah)\n",
    "empty_permbu_sampler = PERMBU(S=S, P=P,\n",
    "                              tags=tags,\n",
    "                              y_hat=y_hat_base,\n",
    "                              y_insample=y_base,\n",
    "                              y_hat_insample=y_base,\n",
    "                              sigmah=sigmah)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test coherent sample's shape\n",
    "normality_samples = normality_sampler.get_samples(num_samples=100)\n",
    "bootstrap_samples = bootstrap_sampler.get_samples(num_samples=100)\n",
    "permbu_samples    = permbu_sampler.get_samples(num_samples=100)\n",
    "test_eq(bootstrap_samples.shape, normality_samples.shape)\n",
    "test_eq(bootstrap_samples.shape, permbu_samples.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test RelMSE's execution\n",
    "rel_mse(y=y_test, y_hat=y_hat_base, y_train=y_base)\n",
    "\n",
    "# test MSSE's execution\n",
    "msse(y=y_test, y_hat=y_hat_base, y_train=y_base)\n",
    "\n",
    "# test energy score's execution\n",
    "energy_score(y=y_test,\n",
    "             y_sample1=bootstrap_samples, y_sample2=permbu_samples)\n",
    "\n",
    "# test scaled CRPS' execution\n",
    "quantiles = np.array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])\n",
    "bootstrap_quantiles = np.quantile(bootstrap_samples, q=quantiles, axis=2)\n",
    "bootstrap_quantiles = bootstrap_quantiles.transpose((1,2,0)) # [Q,N,H] -> [N,H,Q]\n",
    "scaled_crps(y=y_test, y_hat=bootstrap_quantiles, quantiles=quantiles)\n",
    "\n",
    "# test log score's execution\n",
    "cov = np.concatenate([cov[:,:,None] for cov in normality_sampler.cov_rec], axis=2)\n",
    "log_score(y=y_test, y_hat=y_hat_base, cov=cov, allow_singular=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test quantile loss protections\n",
    "quantiles = np.array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.2])\n",
    "test_fail(\n",
    "    scaled_crps,\n",
    "    contains='between 0 and 1',\n",
    "    args=(y_test, bootstrap_quantiles, quantiles),   \n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "- [Rob J. Hyndman and George Athanasopoulos (2018). \n",
    "\"Forecasting principles and practice, Reconciled distributional forecasts\".](https://otexts.com/fpp3/rec-prob.html)<br>\n",
    "- [Puwasala Gamakumara Ph. D. dissertation. Monash University, Econometrics and Business Statistics (2020).\n",
    "\"Probabilistic Forecast Reconciliation\"](https://bridges.monash.edu/articles/thesis/Probabilistic_Forecast_Reconciliation_Theory_and_Applications/11869533)<br>\n",
    "- [Panagiotelis A., Gamakumara P. Athanasopoulos G., and Hyndman R. J. (2022). \"Probabilistic forecast reconciliation: Properties, evaluation and score optimisation\". European Journal of Operational Research.](https://www.sciencedirect.com/science/article/pii/S0377221722006087)<br>\n",
    "- [Taieb, Souhaib Ben and Taylor, James W and Hyndman, Rob J. (2017). Coherent probabilistic forecasts for hierarchical time series. \n",
    "International conference on machine learning ICML.](https://proceedings.mlr.press/v70/taieb17a.html)<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
