{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HierarchicalForecast contains pure Python implementations of hierarchical reconciliation methods as well as a `core.HierarchicalReconciliation` wrapper class that enables easy interaction with these methods through pandas DataFrames containing the hierarchical time series and the base predictions.\n",
    "\n",
    "The `core.HierarchicalReconciliation` reconciliation class operates with the hierarchical time series pd.DataFrame `Y_df`, the base predictions pd.DataFrame `Y_hat_df`, the aggregation constraints matrix `S`. For more information on the creation of aggregation constraints matrix see the utils [aggregation method](https://nixtla.github.io/hierarchicalforecast/utils.html#aggregate).<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import re\n",
    "import gc\n",
    "import time\n",
    "import copy\n",
    "from hierarchicalforecast.methods import HReconciler\n",
    "from inspect import signature\n",
    "from scipy.stats import norm\n",
    "from scipy import sparse\n",
    "from typing import Dict, List, Optional\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from fastcore.test import test_close, test_eq, test_fail\n",
    "from nbdev.showdoc import add_docs, show_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _build_fn_name(fn) -> str:\n",
    "    fn_name = type(fn).__name__\n",
    "    func_params = fn.__dict__\n",
    "\n",
    "    # Take default parameter out of names\n",
    "    args_to_remove = ['insample', 'num_threads']\n",
    "    if not func_params.get('nonnegative', False):\n",
    "        args_to_remove.append('nonnegative')\n",
    "\n",
    "    if fn_name == 'MinTrace' and \\\n",
    "        func_params['method']=='mint_shrink':\n",
    "        if func_params['mint_shr_ridge'] == 2e-8:\n",
    "            args_to_remove += ['mint_shr_ridge']\n",
    "\n",
    "    func_params = [f'{name}-{value}' for name, value in func_params.items() if name not in args_to_remove]\n",
    "    if func_params:\n",
    "        fn_name += '_' + '_'.join(func_params)\n",
    "    return fn_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test fn name\n",
    "from hierarchicalforecast.methods import BottomUp, MinTrace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "test_eq(_build_fn_name(BottomUp()), 'BottomUp')\n",
    "test_eq(\n",
    "    _build_fn_name(MinTrace(method='ols')), \n",
    "    'MinTrace_method-ols'\n",
    ")\n",
    "test_eq(\n",
    "    _build_fn_name(MinTrace(method='ols', nonnegative=True)), \n",
    "    'MinTrace_method-ols_nonnegative-True'\n",
    ")\n",
    "test_eq(\n",
    "    _build_fn_name(MinTrace(method='mint_shr')), \n",
    "    'MinTrace_method-mint_shr'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core.HierarchicalReconciliation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _reverse_engineer_sigmah(Y_hat_df, y_hat, model_name):\n",
    "    \"\"\"\n",
    "    This function assumes that the model creates prediction intervals\n",
    "    under a normality with the following the Equation:\n",
    "    $\\hat{y}_{t+h} + c \\hat{sigma}_{h}$\n",
    "\n",
    "    In the future, we might deprecate this function in favor of a \n",
    "    direct usage of an estimated $\\hat{sigma}_{h}$\n",
    "    \"\"\"\n",
    "\n",
    "    drop_cols = ['ds']\n",
    "    if 'y' in Y_hat_df.columns:\n",
    "        drop_cols.append('y')\n",
    "    if model_name+'-median' in Y_hat_df.columns:\n",
    "        drop_cols.append(model_name+'-median')\n",
    "    model_names = Y_hat_df.drop(columns=drop_cols, axis=1).columns.to_list()\n",
    "    pi_model_names = [name for name in model_names if ('-lo' in name or '-hi' in name)]\n",
    "    pi_model_name = [pi_name for pi_name in pi_model_names if model_name in pi_name]\n",
    "    pi = len(pi_model_name) > 0\n",
    "\n",
    "    n_series = len(Y_hat_df.index.unique())\n",
    "\n",
    "    if not pi:\n",
    "        raise Exception(f'Please include `{model_name}` prediction intervals in `Y_hat_df`')\n",
    "\n",
    "    pi_col = pi_model_name[0]\n",
    "    sign = -1 if 'lo' in pi_col else 1\n",
    "    level_col = re.findall('[\\d]+[.,\\d]+|[\\d]*[.][\\d]+|[\\d]+', pi_col)\n",
    "    level_col = float(level_col[-1])\n",
    "    z = norm.ppf(0.5 + level_col / 200)\n",
    "    sigmah = Y_hat_df[pi_col].values.reshape(n_series,-1)\n",
    "    sigmah = sign * (sigmah - y_hat) / z\n",
    "\n",
    "    return sigmah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class HierarchicalReconciliation:\n",
    "    \"\"\"Hierarchical Reconciliation Class.\n",
    "\n",
    "    The `core.HierarchicalReconciliation` class allows you to efficiently fit multiple \n",
    "    HierarchicaForecast methods for a collection of time series and base predictions stored in \n",
    "    pandas DataFrames. The `Y_df` dataframe identifies series and datestamps with the unique_id and ds columns while the\n",
    "    y column denotes the target time series variable. The `Y_h` dataframe stores the base predictions, \n",
    "    example ([AutoARIMA](https://nixtla.github.io/statsforecast/models.html#autoarima), [ETS](https://nixtla.github.io/statsforecast/models.html#autoets), etc.).\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `reconcilers`: A list of instantiated classes of the [reconciliation methods](https://nixtla.github.io/hierarchicalforecast/methods.html) module .<br>\n",
    "\n",
    "    **References:**<br>\n",
    "    [Rob J. Hyndman and George Athanasopoulos (2018). \\\"Forecasting principles and practice, Hierarchical and Grouped Series\\\".](https://otexts.com/fpp3/hierarchical.html)\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 reconcilers: List[HReconciler]):\n",
    "        self.reconcilers = reconcilers\n",
    "        self.orig_reconcilers = copy.deepcopy(reconcilers) # TODO: elegant solution\n",
    "        self.insample = any([method.insample for method in reconcilers])\n",
    "    \n",
    "    def _prepare_fit(self,\n",
    "                     Y_hat_df: pd.DataFrame,\n",
    "                     S_df: pd.DataFrame,\n",
    "                     Y_df: Optional[pd.DataFrame],\n",
    "                     tags: Dict[str, np.ndarray],\n",
    "                     level: Optional[List[int]] = None,\n",
    "                     intervals_method: str = 'normality',\n",
    "                     sort_df: bool = True):\n",
    "        \"\"\"\n",
    "        Performs preliminary wrangling and protections\n",
    "        \"\"\"\n",
    "        #-------------------------------- Match Y_hat/Y/S index order --------------------------------#\n",
    "        if sort_df:\n",
    "            Y_hat_df = Y_hat_df.reset_index()\n",
    "            Y_hat_df.unique_id = Y_hat_df.unique_id.astype('category')\n",
    "            Y_hat_df.unique_id = Y_hat_df.unique_id.cat.set_categories(S_df.index)\n",
    "            Y_hat_df = Y_hat_df.sort_values(by=['unique_id', 'ds'])\n",
    "            Y_hat_df = Y_hat_df.set_index('unique_id')\n",
    "\n",
    "            if Y_df is not None:\n",
    "                Y_df = Y_df.reset_index()\n",
    "                Y_df.unique_id = Y_df.unique_id.astype('category')\n",
    "                Y_df.unique_id = Y_df.unique_id.cat.set_categories(S_df.index)\n",
    "                Y_df = Y_df.sort_values(by=['unique_id', 'ds'])\n",
    "                Y_df = Y_df.set_index('unique_id')\n",
    "\n",
    "            S_df.index = pd.CategoricalIndex(S_df.index, categories=S_df.index)\n",
    "\n",
    "        #----------------------------------- Check Input's Validity ----------------------------------#\n",
    "        # Check input's validity\n",
    "        if intervals_method not in ['normality', 'bootstrap', 'permbu']:\n",
    "            raise ValueError(f'Unkwon interval method: {intervals_method}')\n",
    "\n",
    "        if self.insample or (intervals_method in ['bootstrap', 'permbu']):\n",
    "            if Y_df is None:\n",
    "                raise Exception('you need to pass `Y_df`')\n",
    "        \n",
    "        # Protect level list\n",
    "        if (level is not None):\n",
    "            level_outside_domain = np.any((np.array(level) < 0)|(np.array(level) >= 100 ))\n",
    "            if level_outside_domain and (intervals_method in ['normality', 'permbu']):\n",
    "                raise Exception('Level outside domain, send `level` list in [0,100)')\n",
    "\n",
    "        # Declare output names\n",
    "        drop_cols = ['ds', 'y'] if 'y' in Y_hat_df.columns else ['ds']\n",
    "        model_names = Y_hat_df.drop(columns=drop_cols, axis=1).columns.to_list()\n",
    "\n",
    "        # Ensure numeric columns\n",
    "        if not len(Y_hat_df[model_names].select_dtypes(include='number').columns) == len(Y_hat_df[model_names].columns):\n",
    "            raise Exception('`Y_hat_df`s columns contain non numeric types')\n",
    "            \n",
    "        #Ensure no null values\n",
    "        if Y_hat_df[model_names].isnull().values.any():\n",
    "            raise Exception('`Y_hat_df` contains null values')\n",
    "        \n",
    "        pi_model_names = [name for name in model_names if ('-lo' in name or '-hi' in name or '-median' in name)]\n",
    "        model_names = [name for name in model_names if name not in pi_model_names]\n",
    "        \n",
    "        # TODO: Complete y_hat_insample protection\n",
    "        if intervals_method in ['bootstrap', 'permbu'] and Y_df is not None:\n",
    "            if not (set(model_names) <= set(Y_df.columns)):\n",
    "                raise Exception('Check `Y_hat_df`s models are included in `Y_df` columns')\n",
    "\n",
    "        uids = Y_hat_df.index.unique()\n",
    "\n",
    "        # Check Y_hat_df\\S_df series difference\n",
    "        S_diff  = len(S_df.index.difference(uids))\n",
    "        Y_hat_diff = len(Y_hat_df.index.difference(S_df.index.unique()))\n",
    "        if S_diff > 0 or Y_hat_diff > 0:\n",
    "            raise Exception(f'Check `S_df`, `Y_hat_df` series difference, S\\Y_hat={S_diff}, Y_hat\\S={Y_hat_diff}')\n",
    "\n",
    "        if Y_df is not None:\n",
    "            # Check Y_hat_df\\Y_df series difference\n",
    "            Y_diff  = len(Y_df.index.difference(uids))\n",
    "            Y_hat_diff = len(Y_hat_df.index.difference(Y_df.index.unique()))\n",
    "            if Y_diff > 0 or Y_hat_diff > 0:\n",
    "                raise Exception(f'Check `Y_hat_df`, `Y_df` series difference, Y_hat\\Y={Y_hat_diff}, Y\\Y_hat={Y_diff}')\n",
    "\n",
    "        # Same Y_hat_df/S_df/Y_df's unique_id order to prevent errors\n",
    "        S_df = S_df.loc[uids]\n",
    "\n",
    "        return Y_hat_df, S_df, Y_df, model_names\n",
    "\n",
    "    def reconcile(self, \n",
    "                  Y_hat_df: pd.DataFrame,\n",
    "                  S: pd.DataFrame,\n",
    "                  tags: Dict[str, np.ndarray],\n",
    "                  Y_df: Optional[pd.DataFrame] = None,\n",
    "                  level: Optional[List[int]] = None,\n",
    "                  intervals_method: str = 'normality',\n",
    "                  num_samples: int = -1,\n",
    "                  seed: int = 0,\n",
    "                  sort_df: bool = True,\n",
    "                  is_balanced: bool = False,\n",
    "        ):\n",
    "        \"\"\"Hierarchical Reconciliation Method.\n",
    "\n",
    "        The `reconcile` method is analogous to SKLearn `fit_predict` method, it \n",
    "        applies different reconciliation techniques instantiated in the `reconcilers` list.\n",
    "\n",
    "        Most reconciliation methods can be described by the following convenient \n",
    "        linear algebra notation:\n",
    "\n",
    "        $$\\\\tilde{\\mathbf{y}}_{[a,b],\\\\tau} = \\mathbf{S}_{[a,b][b]} \\mathbf{P}_{[b][a,b]} \\hat{\\mathbf{y}}_{[a,b],\\\\tau}$$\n",
    "\n",
    "        where $a, b$ represent the aggregate and bottom levels, $\\mathbf{S}_{[a,b][b]}$ contains\n",
    "        the hierarchical aggregation constraints, and $\\mathbf{P}_{[b][a,b]}$ varies across \n",
    "        reconciliation methods. The reconciled predictions are $\\\\tilde{\\mathbf{y}}_{[a,b],\\\\tau}$, and the \n",
    "        base predictions $\\hat{\\mathbf{y}}_{[a,b],\\\\tau}$.\n",
    "\n",
    "        **Parameters:**<br>\n",
    "        `Y_hat_df`: pd.DataFrame, base forecasts with columns `ds` and models to reconcile indexed by `unique_id`.<br>\n",
    "        `Y_df`: pd.DataFrame, training set of base time series with columns `['ds', 'y']` indexed by `unique_id`.<br>\n",
    "        If a class of `self.reconciles` receives `y_hat_insample`, `Y_df` must include them as columns.<br>\n",
    "        `S`: pd.DataFrame with summing matrix of size `(base, bottom)`, see [aggregate method](https://nixtla.github.io/hierarchicalforecast/utils.html#aggregate).<br>\n",
    "        `tags`: Each key is a level and its value contains tags associated to that level.<br>\n",
    "        `level`: positive float list [0,100), confidence levels for prediction intervals.<br>\n",
    "        `intervals_method`: str, method used to calculate prediction intervals, one of `normality`, `bootstrap`, `permbu`.<br>\n",
    "        `num_samples`: int=-1, if positive return that many probabilistic coherent samples.\n",
    "        `seed`: int=0, random seed for numpy generator's replicability.<br>\n",
    "        `sort_df` : bool (default=True), if True, sort `df` by [`unique_id`,`ds`].<br>\n",
    "        `is_balanced`: bool=False, wether `Y_df` is balanced, set it to True to speed things up if `Y_df` is balanced.<br>\n",
    "\n",
    "        **Returns:**<br>\n",
    "        `Y_tilde_df`: pd.DataFrame, with reconciled predictions.\n",
    "        \"\"\"\n",
    "        # Check input's validity and sort dataframes\n",
    "        Y_hat_df, S_df, Y_df, self.model_names = \\\n",
    "                    self._prepare_fit(Y_hat_df=Y_hat_df,\n",
    "                                      S_df=S,\n",
    "                                      Y_df=Y_df,\n",
    "                                      tags=tags,\n",
    "                                      level=level,\n",
    "                                      intervals_method=intervals_method,\n",
    "                                      sort_df=sort_df)\n",
    "\n",
    "        # Initialize reconciler arguments\n",
    "        reconciler_args = dict(\n",
    "            idx_bottom=S_df.index.get_indexer(S.columns),\n",
    "            tags={key: S_df.index.get_indexer(val) for key, val in tags.items()}\n",
    "        )\n",
    "\n",
    "        any_sparse = any([method.is_sparse_method for method in self.reconcilers])\n",
    "        if any_sparse:\n",
    "            try:\n",
    "                S_for_sparse = sparse.csr_matrix(S_df.sparse.to_coo())\n",
    "            except AttributeError:\n",
    "                warnings.warn('Using dense S matrix for sparse reconciliation method.')\n",
    "                S_for_sparse = S_df.values.astype(np.float32)\n",
    "\n",
    "        if Y_df is not None:\n",
    "            if is_balanced:\n",
    "                y_insample = Y_df['y'].values.reshape(len(S_df), -1).astype(np.float32)\n",
    "            else:\n",
    "                y_insample = Y_df.pivot(columns='ds', values='y').loc[S_df.index].values.astype(np.float32)\n",
    "            reconciler_args['y_insample'] = y_insample\n",
    "\n",
    "        Y_tilde_df= Y_hat_df.copy()\n",
    "        start = time.time()\n",
    "        self.execution_times = {}\n",
    "        self.level_names = {}\n",
    "        self.sample_names = {}\n",
    "        for reconciler, name_copy in zip(self.reconcilers, self.orig_reconcilers):\n",
    "            reconcile_fn_name = _build_fn_name(name_copy)\n",
    "\n",
    "            if reconciler.is_sparse_method:\n",
    "                reconciler_args[\"S\"] = S_for_sparse\n",
    "            else:\n",
    "                reconciler_args[\"S\"] = S_df.values.astype(np.float32)\n",
    "\n",
    "            has_fitted = 'y_hat_insample' in signature(reconciler.fit_predict).parameters\n",
    "            has_level = 'level' in signature(reconciler.fit_predict).parameters\n",
    "\n",
    "            for model_name in self.model_names:\n",
    "                recmodel_name = f'{model_name}/{reconcile_fn_name}'\n",
    "                y_hat = Y_hat_df[model_name].values.reshape(len(S_df), -1).astype(np.float32)\n",
    "                reconciler_args['y_hat'] = y_hat\n",
    "\n",
    "                if (self.insample and has_fitted) or intervals_method in ['bootstrap', 'permbu']:\n",
    "                    if is_balanced:\n",
    "                        y_hat_insample = Y_df[model_name].values.reshape(len(S_df), -1).astype(np.float32)\n",
    "                    else:\n",
    "                        y_hat_insample = Y_df.pivot(columns='ds', values=model_name).loc[S_df.index].values.astype(np.float32)\n",
    "                    reconciler_args['y_hat_insample'] = y_hat_insample\n",
    "\n",
    "                if has_level and (level is not None):\n",
    "                    if intervals_method in ['normality', 'permbu']:\n",
    "                        sigmah = _reverse_engineer_sigmah(Y_hat_df=Y_hat_df,\n",
    "                                    y_hat=y_hat, model_name=model_name)\n",
    "                        reconciler_args['sigmah'] = sigmah\n",
    "\n",
    "                    reconciler_args['intervals_method'] = intervals_method\n",
    "                    reconciler_args['num_samples'] = 200 # TODO: solve duplicated num_samples\n",
    "                    reconciler_args['seed'] = seed\n",
    "\n",
    "                # Mean and Probabilistic reconciliation\n",
    "                kwargs_ls = [key for key in signature(reconciler.fit_predict).parameters if key in reconciler_args.keys()]\n",
    "                kwargs = {key: reconciler_args[key] for key in kwargs_ls}\n",
    "                \n",
    "                if (level is not None) and (num_samples > 0):\n",
    "                    # Store reconciler's memory to generate samples\n",
    "                    reconciler = reconciler.fit(**kwargs)\n",
    "                    fcsts_model = reconciler.predict(S=reconciler_args['S'], \n",
    "                                                     y_hat=reconciler_args['y_hat'], level=level)\n",
    "                else:\n",
    "                    # Memory efficient reconciler's fit_predict\n",
    "                    fcsts_model = reconciler(**kwargs, level=level)\n",
    "\n",
    "                # Parse final outputs\n",
    "                Y_tilde_df[recmodel_name] = fcsts_model['mean'].flatten()\n",
    "                if intervals_method in ['bootstrap', 'normality', 'permbu'] and level is not None:\n",
    "                    level.sort()\n",
    "                    lo_names = [f'{recmodel_name}-lo-{lv}' for lv in reversed(level)]\n",
    "                    hi_names = [f'{recmodel_name}-hi-{lv}' for lv in level]\n",
    "                    self.level_names[recmodel_name] = lo_names + hi_names\n",
    "                    sorted_quantiles = np.reshape(fcsts_model['quantiles'], (len(Y_tilde_df),-1))\n",
    "                    intervals_df = pd.DataFrame(sorted_quantiles, index=Y_tilde_df.index,\n",
    "                                                columns=self.level_names[recmodel_name])\n",
    "                    Y_tilde_df= pd.concat([Y_tilde_df, intervals_df], axis=1)\n",
    "\n",
    "                    if num_samples > 0:\n",
    "                        samples = reconciler.sample(num_samples=num_samples)\n",
    "                        self.sample_names[recmodel_name] = [f'{recmodel_name}-sample-{i}' for i in range(num_samples)]\n",
    "                        samples = np.reshape(samples, (len(Y_tilde_df),-1))\n",
    "                        samples_df = pd.DataFrame(samples, index=Y_tilde_df.index,\n",
    "                                                  columns=self.sample_names[recmodel_name])\n",
    "                        Y_tilde_df= pd.concat([Y_tilde_df, samples_df], axis=1)\n",
    "\n",
    "                    del sorted_quantiles\n",
    "                    del intervals_df\n",
    "                if self.insample and has_fitted:\n",
    "                    del y_hat_insample\n",
    "                gc.collect()\n",
    "\n",
    "                end = time.time()\n",
    "                self.execution_times[f'{model_name}/{reconcile_fn_name}'] = (end - start)\n",
    "\n",
    "        return Y_tilde_df\n",
    "\n",
    "    def bootstrap_reconcile(self,\n",
    "                            Y_hat_df: pd.DataFrame,\n",
    "                            S_df: pd.DataFrame,\n",
    "                            tags: Dict[str, np.ndarray],\n",
    "                            Y_df: Optional[pd.DataFrame] = None,\n",
    "                            level: Optional[List[int]] = None,\n",
    "                            intervals_method: str = 'normality',\n",
    "                            num_samples: int = -1,\n",
    "                            num_seeds: int = 1,\n",
    "                            sort_df: bool = True):\n",
    "        \"\"\"Bootstraped Hierarchical Reconciliation Method.\n",
    "\n",
    "        Applies N times, based on different random seeds, the `reconcile` method \n",
    "        for the different reconciliation techniques instantiated in the `reconcilers` list. \n",
    "\n",
    "        **Parameters:**<br>\n",
    "        `Y_hat_df`: pd.DataFrame, base forecasts with columns `ds` and models to reconcile indexed by `unique_id`.<br>\n",
    "        `Y_df`: pd.DataFrame, training set of base time series with columns `['ds', 'y']` indexed by `unique_id`.<br>\n",
    "        If a class of `self.reconciles` receives `y_hat_insample`, `Y_df` must include them as columns.<br>\n",
    "        `S`: pd.DataFrame with summing matrix of size `(base, bottom)`, see [aggregate method](https://nixtla.github.io/hierarchicalforecast/utils.html#aggregate).<br>\n",
    "        `tags`: Each key is a level and its value contains tags associated to that level.<br>\n",
    "        `level`: positive float list [0,100), confidence levels for prediction intervals.<br>\n",
    "        `intervals_method`: str, method used to calculate prediction intervals, one of `normality`, `bootstrap`, `permbu`.<br>\n",
    "        `num_samples`: int=-1, if positive return that many probabilistic coherent samples.\n",
    "        `num_seeds`: int=1, random seed for numpy generator's replicability.<br>\n",
    "        `sort_df` : bool (default=True), if True, sort `df` by [`unique_id`,`ds`].<br>\n",
    "\n",
    "        **Returns:**<br>\n",
    "        `Y_bootstrap_df`: pd.DataFrame, with bootstraped reconciled predictions.\n",
    "        \"\"\"\n",
    "\n",
    "        # Check input's validity and sort dataframes\n",
    "        Y_hat_df, S_df, Y_df, self.model_names = \\\n",
    "                    self._prepare_fit(Y_hat_df=Y_hat_df,\n",
    "                                      S_df=S_df,\n",
    "                                      Y_df=Y_df,\n",
    "                                      tags=tags,\n",
    "                                      intervals_method=intervals_method,\n",
    "                                      sort_df=sort_df)\n",
    "\n",
    "        # Bootstrap reconciled predictions\n",
    "        Y_tilde_list = []\n",
    "        for seed in range(num_seeds):\n",
    "            Y_tilde_df = self.reconcile(Y_hat_df=Y_hat_df,\n",
    "                                        S=S_df,\n",
    "                                        tags=tags,\n",
    "                                        Y_df=Y_df,\n",
    "                                        level=level,\n",
    "                                        intervals_method=intervals_method,\n",
    "                                        num_samples=num_samples,\n",
    "                                        seed=seed,\n",
    "                                        sort_df=False)\n",
    "            Y_tilde_df['seed'] = seed\n",
    "            # TODO: fix broken recmodel_names\n",
    "            if seed==0:\n",
    "                first_columns = Y_tilde_df.columns\n",
    "            Y_tilde_df.columns = first_columns\n",
    "            Y_tilde_list.append(Y_tilde_df)\n",
    "\n",
    "        Y_bootstrap_df = pd.concat(Y_tilde_list, axis=0)\n",
    "        del Y_tilde_list\n",
    "        gc.collect()\n",
    "\n",
    "        return Y_bootstrap_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(HierarchicalReconciliation, \n",
    "         name='init', title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(HierarchicalReconciliation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(HierarchicalReconciliation.reconcile, name='reconcile', title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(HierarchicalReconciliation.bootstrap_reconcile, name='bootstrap_reconcile', title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from hierarchicalforecast.methods import (\n",
    "    BottomUp, TopDown, MiddleOut, MinTrace, ERM,\n",
    ")\n",
    "from hierarchicalforecast.utils import aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/Nixtla/transfer-learning-time-series/main/datasets/tourism.csv')\n",
    "df = df.rename({'Trips': 'y', 'Quarter': 'ds'}, axis=1)\n",
    "df.insert(0, 'Country', 'Australia')\n",
    "\n",
    "# non strictly hierarchical structure\n",
    "hierS_grouped_df = [\n",
    "    ['Country'],\n",
    "    ['Country', 'State'], \n",
    "    ['Country', 'Purpose'], \n",
    "    ['Country', 'State', 'Region'], \n",
    "    ['Country', 'State', 'Purpose'], \n",
    "    ['Country', 'State', 'Region', 'Purpose']\n",
    "]\n",
    "# strictly hierarchical structure\n",
    "hiers_strictly = [\n",
    "    ['Country'],\n",
    "    ['Country', 'State'], \n",
    "    ['Country', 'State', 'Region'], \n",
    "]\n",
    "\n",
    "# getting df\n",
    "hier_grouped_df, S_grouped_df, tags_grouped = aggregate(df, hierS_grouped_df)\n",
    "hier_strict_df, S_strict, tags_strict = aggregate(df, hiers_strictly)\n",
    "\n",
    "# check categorical input produces same output\n",
    "df2 = df.copy()\n",
    "for col in ['Country', 'State', 'Purpose', 'Region']:\n",
    "    df2[col] = df2[col].astype('category')\n",
    "\n",
    "for spec in [hierS_grouped_df, hiers_strictly]:\n",
    "    Y_orig, S_orig, tags_orig = aggregate(df, spec)\n",
    "    Y_cat, S_cat, tags_cat = aggregate(df2, spec)\n",
    "    pd.testing.assert_frame_equal(Y_cat, Y_orig)\n",
    "    pd.testing.assert_frame_equal(S_cat, S_orig)\n",
    "    assert all(np.array_equal(tags_orig[k], tags_cat[k]) for k in tags_orig.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "hier_grouped_df['y_model'] = hier_grouped_df['y']\n",
    "# we should be able to recover y using the methods\n",
    "hier_grouped_hat_df = hier_grouped_df.groupby('unique_id').tail(12)\n",
    "ds_h = hier_grouped_hat_df['ds'].unique()\n",
    "hier_grouped_df = hier_grouped_df.query('~(ds in @ds_h)')\n",
    "#adding noise to `y_model` to avoid perfect fited values\n",
    "hier_grouped_df['y_model'] += np.random.uniform(-1, 1, len(hier_grouped_df))\n",
    "\n",
    "#hierachical reconciliation\n",
    "hrec = HierarchicalReconciliation(reconcilers=[\n",
    "    #these methods should reconstruct the original y\n",
    "    BottomUp(),\n",
    "    MinTrace(method='ols'),\n",
    "    MinTrace(method='wls_struct'),\n",
    "    MinTrace(method='wls_var'),\n",
    "    MinTrace(method='mint_shrink'),\n",
    "    MinTrace(method='ols', nonnegative=True),\n",
    "    MinTrace(method='wls_struct', nonnegative=True),\n",
    "    MinTrace(method='wls_var', nonnegative=True),\n",
    "    MinTrace(method='mint_shrink', nonnegative=True),\n",
    "    # ERM recovers but needs bigger eps\n",
    "    #ERM(method='reg_bu', lambda_reg=None),\n",
    "])\n",
    "reconciled = hrec.reconcile(Y_hat_df=hier_grouped_hat_df, Y_df=hier_grouped_df, \n",
    "                            S=S_grouped_df, tags=tags_grouped)\n",
    "for model in reconciled.drop(columns=['ds', 'y']).columns:\n",
    "    if 'ERM' in model:\n",
    "        eps = 3\n",
    "    elif 'nonnegative' in model:\n",
    "        eps = 1e-1\n",
    "    else:\n",
    "        eps = 1e-1\n",
    "    test_close(reconciled['y'], reconciled[model], eps=eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test incorrect Y_hat_df datatypes\n",
    "hier_grouped_hat_df_nan = hier_grouped_hat_df.copy()\n",
    "hier_grouped_hat_df_nan.loc['Australia', 'y_model'] = float('nan')\n",
    "test_fail(\n",
    "    hrec.reconcile,\n",
    "    contains='null values',\n",
    "    args=(hier_grouped_hat_df_nan, S_grouped_df, tags_grouped, hier_grouped_df),\n",
    ")\n",
    "\n",
    "hier_grouped_hat_df_none = hier_grouped_hat_df.copy()\n",
    "hier_grouped_hat_df_none.loc['Australia', 'y_model'] = None\n",
    "test_fail(\n",
    "    hrec.reconcile,\n",
    "    contains='null values',\n",
    "    args=(hier_grouped_hat_df_none, S_grouped_df, tags_grouped, hier_grouped_df),\n",
    ")\n",
    "\n",
    "hier_grouped_hat_df_str = hier_grouped_hat_df.copy()\n",
    "hier_grouped_hat_df_str['y_model'] = hier_grouped_hat_df_str['y_model'].astype(str)\n",
    "test_fail(\n",
    "    hrec.reconcile,\n",
    "    contains='numeric types',\n",
    "    args=(hier_grouped_hat_df_str, S_grouped_df, tags_grouped, hier_grouped_df),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test expected error\n",
    "# different series S and Y_hat_df\n",
    "test_fail(\n",
    "    hrec.reconcile,\n",
    "    contains='series difference',\n",
    "    args=(hier_grouped_hat_df.drop('Australia'), S_grouped_df, tags_grouped, hier_grouped_df),\n",
    "    \n",
    ")\n",
    "test_fail(\n",
    "    hrec.reconcile,\n",
    "    contains='series difference',\n",
    "    args=(hier_grouped_hat_df, S_grouped_df.drop('Australia'), tags_grouped, hier_grouped_df),\n",
    "    \n",
    ")\n",
    "test_fail(\n",
    "    hrec.reconcile,\n",
    "    contains='series difference',\n",
    "    args=(hier_grouped_hat_df, S_grouped_df, tags_grouped, hier_grouped_df.drop('Australia')),\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "# test expected error\n",
    "# different columns Y_df and Y_hat_df\n",
    "hrec = HierarchicalReconciliation(\n",
    "            reconcilers=[ERM(method='reg_bu', lambda_reg=100)])\n",
    "test_fail(\n",
    "    hrec.reconcile,\n",
    "    contains='Please include ',\n",
    "    args=(hier_grouped_hat_df, S_grouped_df, tags_grouped, \n",
    "          hier_grouped_df, [80], 'permbu'), # permbu needs y_hat_insample\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test reconcile method without insample\n",
    "hrec = HierarchicalReconciliation(reconcilers=[\n",
    "    #these methods should reconstruct the original y\n",
    "    BottomUp(),\n",
    "    MinTrace(method='ols'),\n",
    "    MinTrace(method='wls_struct'),\n",
    "    MinTrace(method='ols', nonnegative=True),\n",
    "    MinTrace(method='wls_struct', nonnegative=True),\n",
    "])\n",
    "reconciled = hrec.reconcile(Y_hat_df=hier_grouped_hat_df,\n",
    "                            S=S_grouped_df, tags=tags_grouped)\n",
    "for model in reconciled.drop(columns=['ds', 'y']).columns:\n",
    "    if 'ERM' in model:\n",
    "        eps = 3\n",
    "    elif 'nonnegative' in model:\n",
    "        eps = 1e-1\n",
    "    else:\n",
    "        eps = 1e-1\n",
    "    test_close(reconciled['y'], reconciled[model], eps=eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# top down should break\n",
    "# with non strictly hierarchical structures\n",
    "hrec = HierarchicalReconciliation([TopDown(method='average_proportions')])\n",
    "test_fail(\n",
    "    hrec.reconcile,\n",
    "    contains='requires strictly hierarchical structures',\n",
    "    args=(hier_grouped_hat_df, S_grouped_df, tags_grouped,  hier_grouped_df,)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# methods should work with\n",
    "# srtictly hierarchical structures\n",
    "#| hide\n",
    "hier_strict_df['y_model'] = hier_strict_df['y']\n",
    "# we should be able to recover y using the methods\n",
    "hier_strict_df_h = hier_strict_df.groupby('unique_id').tail(12)\n",
    "ds_h = hier_strict_df_h['ds'].unique()\n",
    "hier_strict_df = hier_strict_df.query('~(ds in @ds_h)')\n",
    "#adding noise to `y_model` to avoid perfect fited values\n",
    "hier_strict_df['y_model'] += np.random.uniform(-1, 1, len(hier_strict_df))\n",
    "\n",
    "middle_out_level = 'Country/State'\n",
    "# hierarchical reconciliation\n",
    "hrec = HierarchicalReconciliation(reconcilers=[\n",
    "    #these methods should reconstruct the original y\n",
    "    BottomUp(),\n",
    "    MinTrace(method='ols'),\n",
    "    MinTrace(method='wls_struct'),\n",
    "    MinTrace(method='wls_var'),\n",
    "    MinTrace(method='mint_shrink'),\n",
    "    MinTrace(method='ols', nonnegative=True),\n",
    "    MinTrace(method='wls_struct', nonnegative=True),\n",
    "    MinTrace(method='wls_var', nonnegative=True),\n",
    "    MinTrace(method='mint_shrink', nonnegative=True),\n",
    "    # top down doesnt recover the original y\n",
    "    # but it should recover the total level\n",
    "    TopDown(method='forecast_proportions'),\n",
    "    TopDown(method='average_proportions'),\n",
    "    TopDown(method='proportion_averages'),\n",
    "    # middle out doesnt recover the original y\n",
    "    # but it should recover the total level\n",
    "    MiddleOut(middle_level=middle_out_level, top_down_method='forecast_proportions'),\n",
    "    MiddleOut(middle_level=middle_out_level, top_down_method='average_proportions'),\n",
    "    MiddleOut(middle_level=middle_out_level, top_down_method='proportion_averages'),\n",
    "    # ERM recovers but needs bigger eps\n",
    "    #ERM(method='reg_bu', lambda_reg=None),\n",
    "])\n",
    "reconciled = hrec.reconcile(\n",
    "    Y_hat_df=hier_strict_df_h, \n",
    "    Y_df=hier_strict_df, \n",
    "    S=S_strict, \n",
    "    tags=tags_strict\n",
    ")\n",
    "for model in reconciled.drop(columns=['ds', 'y']).columns:\n",
    "    if 'ERM' in model:\n",
    "        eps = 3\n",
    "    elif 'nonnegative' in model:\n",
    "        eps = 1e-1\n",
    "    else:\n",
    "        eps = 1e-1\n",
    "    if 'TopDown' in model:\n",
    "        if 'forecast_proportions' in model:\n",
    "            test_close(reconciled['y'], reconciled[model], eps)\n",
    "        else:\n",
    "            # top down doesnt recover the original y\n",
    "            test_fail(\n",
    "                test_close,\n",
    "                args=(reconciled['y'], reconciled[model], eps),\n",
    "            )\n",
    "        # but it should recover the total level\n",
    "        total_tag = tags_strict['Country']\n",
    "        test_close(reconciled['y'].loc[total_tag], \n",
    "                   reconciled[model].loc[total_tag], 1e-2)\n",
    "    elif 'MiddleOut' in model:\n",
    "        if 'forecast_proportions' in model:\n",
    "            test_close(reconciled['y'], reconciled[model], eps)\n",
    "        else:\n",
    "            # top down doesnt recover the original y\n",
    "            test_fail(\n",
    "                test_close,\n",
    "                args=(reconciled['y'], reconciled[model], eps),\n",
    "            )\n",
    "        # but it should recover the total level\n",
    "        total_tag = tags_strict[middle_out_level]\n",
    "        test_close(reconciled['y'].loc[total_tag], \n",
    "                   reconciled[model].loc[total_tag], 1e-2)\n",
    "    else:\n",
    "        test_close(reconciled['y'], reconciled[model], eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test is_balanced behaviour\n",
    "reconciled_balanced = hrec.reconcile(\n",
    "    Y_hat_df=hier_strict_df_h, \n",
    "    Y_df=hier_strict_df, \n",
    "    S=S_strict, \n",
    "    tags=tags_strict,\n",
    "    is_balanced=True,\n",
    ")\n",
    "test_close(reconciled.drop(columns='ds').values, reconciled_balanced.drop(columns='ds').values, eps=1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from statsforecast import StatsForecast\n",
    "from statsforecast.utils import generate_series\n",
    "from statsforecast.models import RandomWalkWithDrift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test unbalanced dataset\n",
    "max_tenure = 24\n",
    "dates = pd.date_range(start='2019-01-31', freq='M', periods=max_tenure)\n",
    "cohort_tenure = [24, 23, 22, 21]\n",
    "\n",
    "ts_list = []\n",
    "\n",
    "# Create ts for each cohort\n",
    "for i in range(len(cohort_tenure)):\n",
    "    ts_list.append(\n",
    "        generate_series(n_series=1, freq='M', min_length=cohort_tenure[i], max_length=cohort_tenure[i]).reset_index() \\\n",
    "            .assign(ult=i) \\\n",
    "            .assign(ds=dates[-cohort_tenure[i]:]) \\\n",
    "            .drop(columns=['unique_id'])\n",
    "    )\n",
    "df = pd.concat(ts_list, ignore_index=True)\n",
    "\n",
    "# Create categories\n",
    "df.loc[df['ult'] < 2, 'pen'] = 'a'\n",
    "df.loc[df['ult'] >= 2, 'pen'] = 'b'\n",
    "# Note that unique id requires strings\n",
    "df['ult'] = df['ult'].astype(str)\n",
    "\n",
    "hier_levels = [\n",
    "    ['pen'],\n",
    "    ['pen', 'ult'],\n",
    "]\n",
    "hier_df, S_df, tags = aggregate(df=df, spec=hier_levels)\n",
    "\n",
    "train_df = hier_df.query(\"ds <= @pd.to_datetime('2019-12-31')\")\n",
    "test_df = hier_df.query(\"ds > @pd.to_datetime('2019-12-31')\")\n",
    "\n",
    "fcst = StatsForecast(\n",
    "    models=[\n",
    "        RandomWalkWithDrift(),\n",
    "    ],\n",
    "    freq='M',\n",
    "    n_jobs=1,\n",
    ")\n",
    "\n",
    "hrec = HierarchicalReconciliation(\n",
    "    reconcilers=[\n",
    "        BottomUp(),\n",
    "        MinTrace(method='mint_shrink'),\n",
    "    ]\n",
    ")\n",
    "\n",
    "fcst_df = fcst.forecast(df=train_df, h=12, fitted=True)\n",
    "fitted_df = fcst.forecast_fitted_values()\n",
    "\n",
    "fcst_df = hrec.reconcile(\n",
    "    Y_hat_df=fcst_df,\n",
    "    Y_df=fitted_df,\n",
    "    S=S_df,\n",
    "    tags=tags,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# MinTrace should break\n",
    "# with extremely overfitted model, y_model==y\n",
    "\n",
    "zero_df = hier_grouped_df.copy()\n",
    "zero_df['y'] = 0\n",
    "zero_df['y_model'] = 0\n",
    "hrec = HierarchicalReconciliation([MinTrace(method='mint_shrink')])\n",
    "test_fail(\n",
    "    hrec.reconcile,\n",
    "    contains='Insample residuals',\n",
    "    args=(hier_grouped_hat_df, S_grouped_df, tags_grouped,  zero_df)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#test methods that dont use residuals\n",
    "#even if their signature includes\n",
    "#that argument\n",
    "hrec = HierarchicalReconciliation([MinTrace(method='ols')])\n",
    "reconciled = hrec.reconcile(\n",
    "    Y_hat_df=hier_grouped_hat_df, \n",
    "    Y_df=hier_grouped_df.drop(columns=['y_model']), \n",
    "    S=S_grouped_df, \n",
    "    tags=tags_grouped\n",
    ")\n",
    "for model in reconciled.drop(columns=['ds', 'y']).columns:\n",
    "    test_close(reconciled['y'], reconciled[model], eps=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "reconciled.loc[tags_grouped['Country/State']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test methods with bootstrap prediction intervals\n",
    "hrec = HierarchicalReconciliation([BottomUp()])\n",
    "reconciled = hrec.reconcile(Y_hat_df=hier_grouped_hat_df, \n",
    "                            Y_df=hier_grouped_df, S=S_grouped_df, tags=tags_grouped,\n",
    "                            level=[80, 90], \n",
    "                            intervals_method='bootstrap')\n",
    "total = reconciled.loc[tags_grouped['Country/State/Region/Purpose']].groupby('ds').sum().reset_index()\n",
    "pd.testing.assert_frame_equal(\n",
    "    total[['ds', 'y_model/BottomUp']],\n",
    "    reconciled.loc['Australia'][['ds', 'y_model/BottomUp']].reset_index(drop=True)\n",
    ")\n",
    "assert 'y_model/BottomUp-lo-80' in reconciled.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test methods with  normality prediction intervals\n",
    "hier_grouped_hat_df['y_model-lo-80'] = hier_grouped_hat_df['y_model'] - 1.96\n",
    "hier_grouped_hat_df['y_model-hi-80'] = hier_grouped_hat_df['y_model'] + 1.96\n",
    "hrec = HierarchicalReconciliation([BottomUp()])\n",
    "reconciled = hrec.reconcile(Y_hat_df=hier_grouped_hat_df,\n",
    "                            Y_df=hier_grouped_df, S=S_grouped_df, tags=tags_grouped,\n",
    "                            level=[80, 90], \n",
    "                            intervals_method='normality')\n",
    "total = reconciled.loc[tags_grouped['Country/State/Region/Purpose']].groupby('ds').sum().reset_index()\n",
    "pd.testing.assert_frame_equal(\n",
    "    total[['ds', 'y_model/BottomUp']],\n",
    "    reconciled.loc['Australia'][['ds', 'y_model/BottomUp']].reset_index(drop=True)\n",
    ")\n",
    "assert 'y_model/BottomUp-lo-80' in reconciled.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test methods with PERMBU prediction intervals\n",
    "\n",
    "# test expect error with grouped structure\n",
    "# (non strictly hierarchical)\n",
    "hier_grouped_hat_df['y_model-lo-80'] = hier_grouped_hat_df['y_model'] - 1.96\n",
    "hier_grouped_hat_df['y_model-hi-80'] = hier_grouped_hat_df['y_model'] + 1.96\n",
    "hrec = HierarchicalReconciliation([BottomUp()])\n",
    "test_fail(\n",
    "    hrec.reconcile,\n",
    "    contains='requires strictly hierarchical structures',\n",
    "    args=(hier_grouped_hat_df, S_grouped_df, tags_grouped, hier_grouped_df, [80, 90], 'permbu',)\n",
    ")\n",
    "\n",
    "# test PERMBU\n",
    "hier_strict_df_h['y_model-lo-80'] = hier_strict_df_h['y_model'] - 1.96\n",
    "hier_strict_df_h['y_model-hi-80'] = hier_strict_df_h['y_model'] + 1.96\n",
    "hrec = HierarchicalReconciliation([BottomUp()])\n",
    "reconciled = hrec.reconcile(Y_hat_df=hier_strict_df_h,\n",
    "                            Y_df=hier_strict_df, S=S_strict, \n",
    "                            tags=tags_grouped,\n",
    "                            level=[80, 90], \n",
    "                            intervals_method='permbu')\n",
    "total = reconciled.loc[tags_grouped['Country/State/Region']].groupby('ds').sum().reset_index()\n",
    "pd.testing.assert_frame_equal(\n",
    "    total[['ds', 'y_model/BottomUp']],\n",
    "    reconciled.loc['Australia'][['ds', 'y_model/BottomUp']].reset_index(drop=True)\n",
    ")\n",
    "assert 'y_model/BottomUp-lo-80' in reconciled.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test methods with Bootraped Bootstap prediction intervals\n",
    "hrec = HierarchicalReconciliation([BottomUp()])\n",
    "bootstrap_df = hrec.bootstrap_reconcile(Y_hat_df=hier_grouped_hat_df,\n",
    "                                        Y_df=hier_grouped_df, S_df=S_grouped_df, tags=tags_grouped,\n",
    "                                        level=[80, 90],\n",
    "                                        intervals_method='bootstrap',\n",
    "                                        num_seeds=2)\n",
    "assert 'y_model/BottomUp-lo-80' in bootstrap_df.columns\n",
    "assert 'seed' in bootstrap_df.columns\n",
    "assert len(bootstrap_df.seed.unique())==2\n",
    "bootstrap_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test level protection for PERMBU and Normality probabilistic methods\n",
    "hrec = HierarchicalReconciliation([BottomUp()])\n",
    "test_fail(\n",
    "    hrec.reconcile,\n",
    "    contains='Level outside domain',\n",
    "    args=(hier_grouped_hat_df, S_grouped_df, tags_grouped, hier_grouped_df, [-1, 80, 90], 'permbu',)\n",
    ")\n",
    "test_fail(\n",
    "    hrec.reconcile,\n",
    "    contains='Level outside domain',\n",
    "    args=(hier_grouped_hat_df, S_grouped_df, tags_grouped, hier_grouped_df, [80, 90, 101], 'normality',)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from statsforecast.core import StatsForecast\n",
    "from statsforecast.models import ETS, Naive\n",
    "\n",
    "from hierarchicalforecast.utils import aggregate\n",
    "from hierarchicalforecast.core import HierarchicalReconciliation\n",
    "from hierarchicalforecast.methods import BottomUp, MinTrace\n",
    "\n",
    "# Load TourismSmall dataset\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/Nixtla/transfer-learning-time-series/main/datasets/tourism.csv')\n",
    "df = df.rename({'Trips': 'y', 'Quarter': 'ds'}, axis=1)\n",
    "df.insert(0, 'Country', 'Australia')\n",
    "\n",
    "# Create hierarchical seires based on geographic levels and purpose\n",
    "# And Convert quarterly ds string to pd.datetime format\n",
    "hierarchy_levels = [['Country'],\n",
    "                    ['Country', 'State'], \n",
    "                    ['Country', 'Purpose'], \n",
    "                    ['Country', 'State', 'Region'], \n",
    "                    ['Country', 'State', 'Purpose'], \n",
    "                    ['Country', 'State', 'Region', 'Purpose']]\n",
    "\n",
    "Y_df, S_df, tags = aggregate(df=df, spec=hierarchy_levels)\n",
    "qs = Y_df['ds'].str.replace(r'(\\d+) (Q\\d)', r'\\1-\\2', regex=True)\n",
    "Y_df['ds'] = pd.PeriodIndex(qs, freq='Q').to_timestamp()\n",
    "Y_df = Y_df.reset_index()\n",
    "\n",
    "# Split train/test sets\n",
    "Y_test_df  = Y_df.groupby('unique_id').tail(4)\n",
    "Y_train_df = Y_df.drop(Y_test_df.index)\n",
    "\n",
    "# Compute base auto-ETS predictions\n",
    "# Careful identifying correct data freq, this data quarterly 'Q'\n",
    "fcst = StatsForecast(models=[Naive()], freq='Q', n_jobs=-1)\n",
    "Y_hat_df = fcst.forecast(df=Y_train_df, h=4, fitted=True)\n",
    "Y_fitted_df = fcst.forecast_fitted_values()\n",
    "\n",
    "# Reconcile the base predictions\n",
    "Y_train_df = Y_train_df.reset_index().set_index('unique_id')\n",
    "Y_hat_df = Y_hat_df.reset_index().set_index('unique_id')\n",
    "reconcilers = [BottomUp(),\n",
    "               MinTrace(method='mint_shrink')]\n",
    "hrec = HierarchicalReconciliation(reconcilers=reconcilers)\n",
    "Y_rec_df = hrec.reconcile(Y_hat_df=Y_hat_df, \n",
    "                          Y_df=Y_fitted_df,\n",
    "                          S=S_df, tags=tags)\n",
    "Y_rec_df.groupby('unique_id', observed=True).head(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
