{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Evaluation \n",
    "\n",
    "> Module for Hierarchical Evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from inspect import signature\n",
    "from typing import Callable, Dict, List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from fastcore.test import test_close, test_fail\n",
    "from nbdev.showdoc import add_docs, show_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class HierarchicalEvaluation:\n",
    "    \"\"\"Hierarchical Evaluation Class.\n",
    "    [Source code](https://github.com/dluuo/hierarchicalforecast/blob/main/hierarchicalforecast/evaluation.py).\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `evaluators`: functions with arguments `y`, `y_hat`.<br>\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self, \n",
    "            evaluators: List[Callable] \n",
    "        ):\n",
    "        self.evaluators = evaluators\n",
    "        \n",
    "    def evaluate(\n",
    "            self, \n",
    "            Y_h: pd.DataFrame, \n",
    "            Y_test: pd.DataFrame, \n",
    "            tags: Dict[str, np.ndarray], \n",
    "            Y_df: Optional[pd.DataFrame] = None,\n",
    "            benchmark: Optional[str] = None \n",
    "        ):\n",
    "        \"\"\"Hierarchical Evaluation Method.\n",
    "        [Source code](https://github.com/dluuo/hierarchicalforecast/blob/main/hierarchicalforecast/evaluation.py).\n",
    "\n",
    "        **Parameters:**<br>\n",
    "        `Y_h`: Forecasts with columns `['ds']` and models to evaluate.<br>\n",
    "        `Y_test`: True values with columns `['ds', 'y']`.<br>\n",
    "        `tags`: Each key is a level and its value contains tags associated to that level.<br>\n",
    "        `Y_df`: Training set of base time series with columns `['ds', 'y']` indexed by `unique_id`.<br>\n",
    "        `benchmark`: If passed, evaluators are scaled by the error of this benchark.<br>\n",
    "        \n",
    "        **Returns:**<br>\n",
    "        `evaluation`: pd.DataFrame with accuracy measurements across hierarchical levels.\n",
    "        \"\"\"\n",
    "        drop_cols = ['ds', 'y'] if 'y' in Y_h.columns else ['ds']\n",
    "        h = len(Y_h.loc[Y_h.index[0]])\n",
    "        model_names = Y_h.drop(columns=drop_cols, axis=1).columns.to_list()\n",
    "        fn_names = [fn.__name__ for fn in self.evaluators]\n",
    "        has_y_insample = any(['y_insample' in signature(fn).parameters for fn in self.evaluators])\n",
    "        if has_y_insample and Y_df is None:\n",
    "            raise Exception('At least one evaluator needs y insample, please pass `Y_df`')\n",
    "        if benchmark is not None:\n",
    "            fn_names = [f'{fn_name}-scaled' for fn_name in fn_names]\n",
    "        tags_ = {'Overall': np.concatenate(list(tags.values()))}\n",
    "        tags_ = {**tags_, **tags}\n",
    "        index = pd.MultiIndex.from_product([tags_.keys(), fn_names], names=['level', 'metric'])\n",
    "        evaluation = pd.DataFrame(columns=model_names, index=index)\n",
    "        for level, cats in tags_.items():\n",
    "            Y_h_cats = Y_h.loc[cats]\n",
    "            y_test_cats = Y_test.loc[cats, 'y'].values.reshape(-1, h)\n",
    "            if has_y_insample:\n",
    "                y_insample = Y_df.pivot(columns='ds', values='y').loc[cats].values\n",
    "            for i_fn, fn in enumerate(self.evaluators):\n",
    "                if 'y_insample' in signature(fn).parameters:\n",
    "                    kwargs = {'y_insample': y_insample}\n",
    "                else:\n",
    "                    kwargs = {}\n",
    "                fn_name = fn_names[i_fn]\n",
    "                for model in model_names:\n",
    "                    loss = fn(y_test_cats, Y_h_cats[model].values.reshape(-1, h), **kwargs)\n",
    "                    if benchmark is not None:\n",
    "                        scale = fn(y_test_cats, Y_h_cats[benchmark].values.reshape(-1, h), **kwargs)\n",
    "                        if np.isclose(scale, 0., atol=np.finfo(float).eps):\n",
    "                            scale += np.finfo(float).eps\n",
    "                            if np.isclose(scale, loss, atol=1e-8):\n",
    "                                scale = 1.\n",
    "                        loss /= scale\n",
    "                    evaluation.loc[(level, fn_name), model] = loss\n",
    "        return evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "add_docs(HierarchicalEvaluation, \"Evaluate reconciliation methods.\",\n",
    "         evaluate=\"Evaluate reconciliation methods for distinct levels.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(HierarchicalEvaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(HierarchicalEvaluation.evaluate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use your own metrics to evaluate the performance of each level in the structure. The metrics receive `y` and `y_hat` as arguments and they are numpy arrays of size `(series, horizon)`. Consider, for example, the function `rmse` that calculates the root mean squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y, y_hat):\n",
    "    return np.mean(np.sqrt(np.mean((y-y_hat)**2, axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, you can use functions based on insample values, such as `mase` (mean absolute scaled error). In this case you have to include the argument `y_insample` (of size `(series, insample_size)`) in your function. For example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mase(y, y_hat, y_insample, seasonality=4):\n",
    "    errors = np.mean(np.abs(y - y_hat), axis=1)\n",
    "    scale = np.mean(np.abs(y_insample[:, seasonality:] - y_insample[:, :-seasonality]), axis=1)\n",
    "    return np.mean(errors / scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from hierarchicalforecast.core import HierarchicalReconciliation\n",
    "from hierarchicalforecast.methods import BottomUp, MinTrace, ERM\n",
    "from hierarchicalforecast.utils import aggregate\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/Nixtla/transfer-learning-time-series/main/datasets/tourism.csv')\n",
    "df = df.rename({'Trips': 'y', 'Quarter': 'ds'}, axis=1)\n",
    "df.insert(0, 'Country', 'Australia')\n",
    "\n",
    "# non strictly hierarchical structure\n",
    "hiers_grouped = [\n",
    "    ['Country'],\n",
    "    ['Country', 'State'], \n",
    "    ['Country', 'Purpose'], \n",
    "    ['Country', 'State', 'Region'], \n",
    "    ['Country', 'State', 'Purpose'], \n",
    "    ['Country', 'State', 'Region', 'Purpose']\n",
    "]\n",
    "# strictly hierarchical structure\n",
    "hiers_strictly = [\n",
    "    ['Country'],\n",
    "    ['Country', 'State'], \n",
    "    ['Country', 'State', 'Region'], \n",
    "]\n",
    "\n",
    "# getting df\n",
    "hier_grouped_df, S_grouped, tags_grouped = aggregate(df, hiers_grouped)\n",
    "\n",
    "#split train/test\n",
    "hier_grouped_df['y_model'] = hier_grouped_df['y']\n",
    "# we should be able to recover y using the methods\n",
    "hier_grouped_df_h = hier_grouped_df.groupby('unique_id').tail(12)\n",
    "ds_h = hier_grouped_df_h['ds'].unique()\n",
    "hier_grouped_df = hier_grouped_df.query('~(ds in @ds_h)')\n",
    "#adding noise to `y_model` to avoid perfect fited values\n",
    "hier_grouped_df['y_model'] += np.random.uniform(-1, 1, len(hier_grouped_df))\n",
    "\n",
    "#hierachical reconciliation\n",
    "hrec = HierarchicalReconciliation(reconcilers=[\n",
    "    #these methods should reconstruct the original y\n",
    "    BottomUp(),\n",
    "    MinTrace(method='ols'),\n",
    "    MinTrace(method='wls_struct'),\n",
    "    MinTrace(method='wls_var'),\n",
    "    MinTrace(method='mint_shrink'),\n",
    "    # ERM recovers but needs bigger eps\n",
    "    ERM(method='reg_bu', lambda_reg=None),\n",
    "])\n",
    "reconciled = hrec.reconcile(hier_grouped_df_h, hier_grouped_df, S_grouped, tags_grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "def mse(y, y_hat):\n",
    "    return np.mean((y-y_hat)**2)\n",
    "def rmse(y, y_hat):\n",
    "    return np.sqrt(mse(y, y_hat))\n",
    "evaluator = HierarchicalEvaluation([mse, rmse])\n",
    "evaluator.evaluate(Y_h=reconciled.drop(columns='y'), \n",
    "                   Y_test=reconciled[['ds', 'y']], \n",
    "                   tags=tags_grouped,\n",
    "                   benchmark='y_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "def mase(y, y_hat, y_insample, seasonality=4):\n",
    "    errors = np.mean(np.abs(y - y_hat), axis=1)\n",
    "    scale = np.mean(np.abs(y_insample[:, seasonality:] - y_insample[:, :-seasonality]), axis=1)\n",
    "    return np.mean(errors / scale)\n",
    "evaluator = HierarchicalEvaluation([mase])\n",
    "evaluator.evaluate(Y_h=reconciled.drop(columns='y'), \n",
    "                   Y_test=reconciled[['ds', 'y']], \n",
    "                   tags=tags_grouped,\n",
    "                   Y_df=hier_grouped_df,\n",
    "                   benchmark='y_model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
