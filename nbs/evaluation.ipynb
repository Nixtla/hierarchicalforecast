{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Evaluation \n",
    "\n",
    "> Module for Hierarchical Evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from typing import Callable, Dict, List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from fastcore.test import test_close, test_fail\n",
    "from nbdev.showdoc import add_docs, show_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class HierarchicalEvaluation:\n",
    "    \n",
    "    def __init__(\n",
    "            self, \n",
    "            evaluators: List[Callable] # functions with arguments `y`, `y_hat` \n",
    "        ):\n",
    "        self.evaluators = evaluators\n",
    "        \n",
    "    def evaluate(\n",
    "            self, \n",
    "            Y_h: pd.DataFrame, # Forecasts with columns `['ds']` and models to evaluate.\n",
    "            Y_test: pd.DataFrame, # True values with columns `['ds', 'y']`\n",
    "            tags: Dict[str, np.ndarray], # Each key is a level and its value contains tags associated to that level.\n",
    "            benchmark: Optional[str] = None # If passed, evaluators are scaled by the error of this benchark.\n",
    "        ):\n",
    "        drop_cols = ['ds', 'y'] if 'y' in Y_h.columns else ['ds']\n",
    "        model_names = Y_h.drop(columns=drop_cols, axis=1).columns.to_list()\n",
    "        fn_names = [fn.__name__ for fn in self.evaluators]\n",
    "        if benchmark is not None:\n",
    "            fn_names = [f'{fn_name}-scaled' for fn_name in fn_names]\n",
    "        tags_ = {'Overall': np.concatenate(list(tags.values()))}\n",
    "        tags_ = {**tags_, **tags}\n",
    "        index = pd.MultiIndex.from_product([tags_.keys(), fn_names], names=['level', 'metric'])\n",
    "        evaluation = pd.DataFrame(columns=model_names, index=index)\n",
    "        for level, cats in tags_.items():\n",
    "            Y_h_cats = Y_h.loc[cats]\n",
    "            y_test_cats = Y_test.loc[cats, 'y'].values\n",
    "            for i_fn, fn in enumerate(self.evaluators):\n",
    "                fn_name = fn_names[i_fn]\n",
    "                for model in model_names:\n",
    "                    loss = fn(y_test_cats, Y_h_cats[model].values)\n",
    "                    if benchmark is not None:\n",
    "                        scale = fn(y_test_cats, Y_h_cats[benchmark].values)\n",
    "                        if np.isclose(scale, 0., atol=np.finfo(float).eps):\n",
    "                            scale += np.finfo(float).eps\n",
    "                            if np.isclose(scale, loss, atol=1e-8):\n",
    "                                scale = 1.\n",
    "                        loss /= scale\n",
    "                    evaluation.loc[(level, fn_name), model] = loss\n",
    "        return evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "add_docs(HierarchicalEvaluation, \"Evaluate reconciliation methods.\",\n",
    "         evaluate=\"Evaluate reconciliation methods for distinct levels.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(HierarchicalEvaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(HierarchicalEvaluation.evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from hierarchicalforecast.core import HierarchicalReconciliation\n",
    "from hierarchicalforecast.methods import BottomUp, MinTrace, ERM\n",
    "from hierarchicalforecast.utils import hierarchize\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/Nixtla/transfer-learning-time-series/main/datasets/tourism.csv')\n",
    "df = df.rename({'Trips': 'y', 'Quarter': 'ds'}, axis=1)\n",
    "df.insert(0, 'Country', 'Australia')\n",
    "\n",
    "# non strictly hierarchical structure\n",
    "hiers_grouped = [\n",
    "    ['Country'],\n",
    "    ['Country', 'State'], \n",
    "    ['Country', 'Purpose'], \n",
    "    ['Country', 'State', 'Region'], \n",
    "    ['Country', 'State', 'Purpose'], \n",
    "    ['Country', 'State', 'Region', 'Purpose']\n",
    "]\n",
    "# strictly hierarchical structure\n",
    "hiers_strictly = [\n",
    "    ['Country'],\n",
    "    ['Country', 'State'], \n",
    "    ['Country', 'State', 'Region'], \n",
    "]\n",
    "\n",
    "# getting df\n",
    "hier_grouped_df, S_grouped, tags_grouped = hierarchize(df, hiers_grouped)\n",
    "\n",
    "#split train/test\n",
    "hier_grouped_df['y_model'] = hier_grouped_df['y']\n",
    "# we should be able to recover y using the methods\n",
    "hier_grouped_df_h = hier_grouped_df.groupby('unique_id').tail(12)\n",
    "ds_h = hier_grouped_df_h['ds'].unique()\n",
    "hier_grouped_df = hier_grouped_df.query('~(ds in @ds_h)')\n",
    "#adding noise to `y_model` to avoid perfect fited values\n",
    "hier_grouped_df['y_model'] += np.random.uniform(-1, 1, len(hier_grouped_df))\n",
    "\n",
    "#hierachical reconciliation\n",
    "hrec = HierarchicalReconciliation(reconcilers=[\n",
    "    #these methods should reconstruct the original y\n",
    "    BottomUp(),\n",
    "    MinTrace(method='ols'),\n",
    "    MinTrace(method='wls_struct'),\n",
    "    MinTrace(method='wls_var'),\n",
    "    MinTrace(method='mint_shrink'),\n",
    "    # ERM recovers but needs bigger eps\n",
    "    ERM(method='reg_bu', lambda_reg=None),\n",
    "])\n",
    "reconciled = hrec.reconcile(hier_grouped_df_h, hier_grouped_df, S_grouped, tags_grouped)\n",
    "\n",
    "#hide\n",
    "def mse(y, y_hat):\n",
    "    return np.mean((y-y_hat)**2)\n",
    "def rmse(y, y_hat):\n",
    "    return np.sqrt(mse(y, y_hat))\n",
    "evaluator = HierarchicalEvaluation([mse, rmse])\n",
    "evaluator.evaluate(Y_h=reconciled.drop(columns='y'), \n",
    "                   Y_test=reconciled[['ds', 'y']], \n",
    "                   tags=tags_grouped,\n",
    "                   benchmark='y_model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
