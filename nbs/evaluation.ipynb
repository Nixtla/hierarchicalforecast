{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp evaluation\n",
    "# - [Anastasios Panagiotelis et. al. \"Probabilistic Forecast Reconciliation: Properties, Evaluation and Score Optimisation\"](https://www.monash.edu/business/ebs/research/publications/ebs/wp26-2020.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Evaluation \n",
    "\n",
    "> To assist on the evaluation of hierarchical systems we make available the `HierarchicalEvaluation` module\n",
    "that facilitates the measurement of prediction's accuracy through the levels of a hierarchy.<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from inspect import signature\n",
    "from typing import Callable, Dict, List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from fastcore.test import test_close, test_fail\n",
    "from nbdev.showdoc import add_docs, show_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:DarkBlue\"> Hierarchical Evaluation </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class HierarchicalEvaluation:    \n",
    "    \"\"\"Hierarchical Evaluation Class.\n",
    "    \n",
    "    You can use your own metrics to evaluate the performance of each level in the structure.\n",
    "    The metrics receive `y` and `y_hat` as arguments and they are numpy arrays of size `(series, horizon)`.\n",
    "    Consider, for example, the function `rmse` that calculates the root mean squared error.\n",
    "\n",
    "    This class facilitates measurements across the hierarchy, defined by the `tags` list.\n",
    "    See also the [aggregate method](https://nixtla.github.io/hierarchicalforecast/utils.html#aggregate).\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `evaluators`: functions with arguments `y`, `y_hat` (numpy arrays).<br>\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 evaluators: List[Callable]):\n",
    "        self.evaluators = evaluators\n",
    "\n",
    "    def evaluate(self, \n",
    "                 Y_hat_df: pd.DataFrame,\n",
    "                 Y_test_df: pd.DataFrame,\n",
    "                 tags: Dict[str, np.ndarray],\n",
    "                 Y_df: Optional[pd.DataFrame] = None,\n",
    "                 benchmark: Optional[str] = None):\n",
    "        \"\"\"Hierarchical Evaluation Method.\n",
    "\n",
    "        **Parameters:**<br>\n",
    "        `Y_hat_df`: pd.DataFrame, Forecasts indexed by `'unique_id'` with column `'ds'` and models to evaluate.<br>\n",
    "        `Y_test_df`:  pd.DataFrame, True values with columns `['ds', 'y']`.<br>\n",
    "        `tags`: np.array, each str key is a level and its value contains tags associated to that level.<br>\n",
    "        `Y_df`: pd.DataFrame, Training set of base time series with columns `['ds', 'y']` indexed by `unique_id`.<br>\n",
    "        `benchmark`: str, If passed, evaluators are scaled by the error of this benchark.<br>\n",
    "\n",
    "        **Returns:**<br>\n",
    "        `evaluation`: pd.DataFrame with accuracy measurements across hierarchical levels.\n",
    "        \"\"\"\n",
    "        drop_cols = ['ds', 'y'] if 'y' in Y_hat_df.columns else ['ds']\n",
    "        h = len(Y_hat_df.loc[[Y_hat_df.index[0]]])\n",
    "        model_names = Y_hat_df.drop(columns=drop_cols, axis=1).columns.to_list()\n",
    "        fn_names = [fn.__name__ for fn in self.evaluators]\n",
    "        has_y_insample = any(['y_insample' in signature(fn).parameters for fn in self.evaluators])\n",
    "        if has_y_insample and Y_df is None:\n",
    "            raise Exception('At least one evaluator needs y insample, please pass `Y_df`')\n",
    "        if benchmark is not None:\n",
    "            fn_names = [f'{fn_name}-scaled' for fn_name in fn_names]\n",
    "        tags_ = {'Overall': np.concatenate(list(tags.values()))}\n",
    "        tags_ = {**tags_, **tags}\n",
    "        index = pd.MultiIndex.from_product([tags_.keys(), fn_names], names=['level', 'metric'])\n",
    "        evaluation = pd.DataFrame(columns=model_names, index=index)\n",
    "        for level, cats in tags_.items():\n",
    "            Y_h_cats = Y_hat_df.loc[cats]\n",
    "            y_test_cats = Y_test_df.loc[cats, 'y'].values.reshape(-1, h)\n",
    "            if has_y_insample:\n",
    "                y_insample = Y_df.pivot(columns='ds', values='y').loc[cats].values\n",
    "            for i_fn, fn in enumerate(self.evaluators):\n",
    "                if 'y_insample' in signature(fn).parameters:\n",
    "                    kwargs = {'y_insample': y_insample}\n",
    "                else:\n",
    "                    kwargs = {}\n",
    "                fn_name = fn_names[i_fn]\n",
    "                for model in model_names:\n",
    "                    loss = fn(y_test_cats, Y_h_cats[model].values.reshape(-1, h), **kwargs)\n",
    "                    if benchmark is not None:\n",
    "                        scale = fn(y_test_cats, Y_h_cats[benchmark].values.reshape(-1, h), **kwargs)\n",
    "                        if np.isclose(scale, 0., atol=np.finfo(float).eps):\n",
    "                            scale += np.finfo(float).eps\n",
    "                            if np.isclose(scale, loss, atol=1e-8):\n",
    "                                scale = 1.\n",
    "                        loss /= scale\n",
    "                    evaluation.loc[(level, fn_name), model] = loss\n",
    "        return evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(HierarchicalEvaluation, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(HierarchicalEvaluation.evaluate, title_level=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:DarkBlue\"> Example </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "def rmse(y, y_hat):\n",
    "    return np.mean(np.sqrt(np.mean((y-y_hat)**2, axis=1)))\n",
    "\n",
    "def mase(y, y_hat, y_insample, seasonality=4):\n",
    "    errors = np.mean(np.abs(y - y_hat), axis=1)\n",
    "    scale = np.mean(np.abs(y_insample[:, seasonality:] - y_insample[:, :-seasonality]), axis=1)\n",
    "    return np.mean(errors / scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from hierarchicalforecast.core import HierarchicalReconciliation\n",
    "from hierarchicalforecast.methods import BottomUp, MinTrace, ERM\n",
    "from hierarchicalforecast.utils import aggregate\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/Nixtla/transfer-learning-time-series/main/datasets/tourism.csv')\n",
    "df = df.rename({'Trips': 'y', 'Quarter': 'ds'}, axis=1)\n",
    "df.insert(0, 'Country', 'Australia')\n",
    "\n",
    "# non strictly hierarchical structure\n",
    "hiers_grouped = [\n",
    "    ['Country'],\n",
    "    ['Country', 'State'], \n",
    "    ['Country', 'Purpose'], \n",
    "    ['Country', 'State', 'Region'], \n",
    "    ['Country', 'State', 'Purpose'], \n",
    "    ['Country', 'State', 'Region', 'Purpose']\n",
    "]\n",
    "# strictly hierarchical structure\n",
    "hiers_strictly = [\n",
    "    ['Country'],\n",
    "    ['Country', 'State'], \n",
    "    ['Country', 'State', 'Region'], \n",
    "]\n",
    "\n",
    "# getting df\n",
    "hier_grouped_df, S_grouped, tags_grouped = aggregate(df, hiers_grouped)\n",
    "\n",
    "#split train/test\n",
    "hier_grouped_df['y_model'] = hier_grouped_df['y']\n",
    "# we should be able to recover y using the methods\n",
    "hier_grouped_df_h = hier_grouped_df.groupby('unique_id').tail(12)\n",
    "ds_h = hier_grouped_df_h['ds'].unique()\n",
    "hier_grouped_df = hier_grouped_df.query('~(ds in @ds_h)')\n",
    "#adding noise to `y_model` to avoid perfect fited values\n",
    "hier_grouped_df['y_model'] += np.random.uniform(-1, 1, len(hier_grouped_df))\n",
    "\n",
    "#hierachical reconciliation\n",
    "hrec = HierarchicalReconciliation(reconcilers=[\n",
    "    #these methods should reconstruct the original y\n",
    "    BottomUp(),\n",
    "    MinTrace(method='ols'),\n",
    "    MinTrace(method='wls_struct'),\n",
    "    MinTrace(method='wls_var'),\n",
    "    MinTrace(method='mint_shrink'),\n",
    "    # ERM recovers but needs bigger eps\n",
    "    ERM(method='reg_bu', lambda_reg=None),\n",
    "])\n",
    "reconciled = hrec.reconcile(hier_grouped_df_h, hier_grouped_df, S_grouped, tags_grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "def mse(y, y_hat):\n",
    "    return np.mean((y-y_hat)**2)\n",
    "def rmse(y, y_hat):\n",
    "    return np.sqrt(mse(y, y_hat))\n",
    "\n",
    "evaluator = HierarchicalEvaluation([mse, rmse])\n",
    "evaluator.evaluate(Y_hat_df=reconciled.drop(columns='y'), \n",
    "                   Y_test_df=reconciled[['ds', 'y']], \n",
    "                   tags=tags_grouped,\n",
    "                   benchmark='y_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "def mase(y, y_hat, y_insample, seasonality=4):\n",
    "    errors = np.mean(np.abs(y - y_hat), axis=1)\n",
    "    scale = np.mean(np.abs(y_insample[:, seasonality:] - y_insample[:, :-seasonality]), axis=1)\n",
    "    return np.mean(errors / scale)\n",
    "\n",
    "evaluator = HierarchicalEvaluation([mase])\n",
    "evaluator.evaluate(Y_hat_df=reconciled.drop(columns='y'), \n",
    "                   Y_test_df=reconciled[['ds', 'y']], \n",
    "                   tags=tags_grouped,\n",
    "                   Y_df=hier_grouped_df,\n",
    "                   benchmark='y_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test work for h=1\n",
    "evaluator = HierarchicalEvaluation([mase])\n",
    "evaluator.evaluate(Y_hat_df=reconciled.groupby('unique_id').tail(1).drop(columns='y'), \n",
    "                   Y_test_df=reconciled.groupby('unique_id').tail(1)[['ds', 'y']], \n",
    "                   tags=tags_grouped,\n",
    "                   Y_df=hier_grouped_df,\n",
    "                   benchmark='y_model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
