{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3226c32-707e-45a6-ab7f-9d8f33924670",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5955e6c8-f4cd-49a6-b6c8-b91c4392a6d3",
   "metadata": {},
   "source": [
    "# Aggregation/Visualization Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc33c1e",
   "metadata": {},
   "source": [
    "The `HierarchicalForecast` package contains utility functions to wrangle and visualize \n",
    "hierarchical series datasets. The `aggregate` function of the module allows you to create\n",
    "a hierarchy from categorical variables representing the structure levels, returning also\n",
    "the aggregation contraints matrix $\\mathbf{S}$.\n",
    "\n",
    "In addition, `HierarchicalForecast` ensures compatibility of its reconciliation methods with other popular machine-learning libraries via its external forecast adapters that transform output base forecasts from external libraries into a compatible data frame format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74967eef-4a18-433e-9dc8-d5e8e6d7dd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import sys\n",
    "import timeit\n",
    "import warnings\n",
    "from itertools import chain\n",
    "from typing import Callable, Dict, List, Optional, Iterable\n",
    "from collections.abc import Sequence\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "plt.rcParams['font.family'] = 'serif'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a501424-ebfd-403c-982e-280cb859bd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import add_docs, show_doc\n",
    "from fastcore.test import test_eq, test_close, test_fail\n",
    "\n",
    "from statsforecast.utils import generate_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ba2391",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "class CodeTimer:\n",
    "    def __init__(self, name=None, verbose=True):\n",
    "        self.name = \" '\"  + name + \"'\" if name else ''\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.start = timeit.default_timer()\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        self.took = (timeit.default_timer() - self.start)\n",
    "        if self.verbose:\n",
    "            print('Code block' + self.name + \\\n",
    "                  ' took:\\t{0:.5f}'.format(self.took) + ' seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e585c763-cba5-48f5-a021-822622f3b4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def is_strictly_hierarchical(S: np.ndarray, \n",
    "                             tags: Dict[str, np.ndarray]):\n",
    "    # main idea:\n",
    "    # if S represents a strictly hierarchical structure\n",
    "    # the number of paths before the bottom level\n",
    "    # should be equal to the number of nodes\n",
    "    # of the previuos level\n",
    "    levels_ = dict(sorted(tags.items(), key=lambda x: len(x[1])))\n",
    "    # removing bottom level\n",
    "    levels_.popitem()\n",
    "    # making S categorical\n",
    "    hiers = [np.argmax(S[idx], axis=0) + 1 for _, idx in levels_.items()]\n",
    "    hiers = np.vstack(hiers)\n",
    "    paths = np.unique(hiers, axis=1).shape[1] \n",
    "    nodes = levels_.popitem()[1].size\n",
    "    return paths == nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5727ad1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def cov2corr(cov, return_std=False):\n",
    "    \"\"\" convert covariance matrix to correlation matrix\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `cov`: array_like, 2d covariance matrix.<br>\n",
    "    `return_std`: bool=False, if True returned std.<br>\n",
    "\n",
    "    **Returns:**<br>\n",
    "    `corr`: ndarray (subclass) correlation matrix\n",
    "    \"\"\"\n",
    "    cov = np.asanyarray(cov)\n",
    "    std_ = np.sqrt(np.diag(cov))\n",
    "    corr = cov / np.outer(std_, std_)\n",
    "    if return_std:\n",
    "        return corr, std_\n",
    "    else:\n",
    "        return corr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1f4267",
   "metadata": {},
   "source": [
    "# Aggregate Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1620096",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _to_summing_matrix(S_df: pd.DataFrame, sparse_s: bool = False):\n",
    "    \"\"\"Transforms the DataFrame `df` of hierarchies to a summing matrix S.\"\"\"\n",
    "    categories = [S_df[col].unique() for col in S_df.columns]\n",
    "    cat_sizes = [len(cats) for cats in categories]\n",
    "    idx_bottom = np.argmax(cat_sizes)\n",
    "    cats_bottom = categories[idx_bottom]\n",
    "\n",
    "    try:\n",
    "        encoder = OneHotEncoder(categories=categories, sparse_output=sparse_s, dtype=np.float32)\n",
    "    except TypeError:  # sklearn < 1.2\n",
    "        encoder = OneHotEncoder(categories=categories, sparse=sparse_s, dtype=np.float32)\n",
    "\n",
    "    S = encoder.fit_transform(S_df).T\n",
    "\n",
    "    if sparse_s:\n",
    "        df_constructor = pd.DataFrame.sparse.from_spmatrix\n",
    "    else:\n",
    "        df_constructor = pd.DataFrame\n",
    "    S = df_constructor(S, index=chain(*categories), columns=cats_bottom)\n",
    "\n",
    "    tags = dict(zip(S_df.columns, categories))\n",
    "    return S, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40374c36-a0f0-4539-92af-1bcbdfe26714",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def aggregate_before(df: pd.DataFrame,\n",
    "              spec: List[List[str]],\n",
    "              agg_fn: Callable = np.sum,\n",
    "              sparse_s: bool = False):\n",
    "    \"\"\"Utils Aggregation Function.\n",
    "\n",
    "    Aggregates bottom level series contained in the pd.DataFrame `df` according\n",
    "    to levels defined in the `spec` list applying the `agg_fn` (sum, mean).<br>\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `df`: pd.DataFrame with columns `['ds', 'y']` and columns to aggregate.<br>\n",
    "    `spec`: List of levels. Each element of the list contains a list of columns of `df` to aggregate.<br>\n",
    "    `agg_fn`: Function used to aggregate `'y'`.<br>\n",
    "    `sparse_s`: bool=False, whether the returned S should be a sparse DataFrame.<br>\n",
    "\n",
    "\n",
    "    **Returns:**<br>\n",
    "    `Y_df, S, tags`: tuple with hierarchically structured series `Y_df` ($\\mathbf{y}_{[a,b]}$),\n",
    "    summing matrix `S`, and hierarchical aggregation indexes `tags`.\n",
    "    \"\"\"\n",
    "    max_len_idx = np.argmax([len(hier) for hier in spec])\n",
    "    bottom_comb = spec[max_len_idx]\n",
    "    hiers = []\n",
    "    for hier in spec:\n",
    "        df_hier = df.groupby(hier + ['ds'])['y'].apply(agg_fn).reset_index()\n",
    "        df_hier['unique_id'] = df_hier[hier].agg('/'.join, axis=1)\n",
    "        if hier == bottom_comb:\n",
    "            bottom_hier = df_hier['unique_id'].unique()\n",
    "        hiers.append(df_hier)\n",
    "    df_hiers = pd.concat(hiers)\n",
    "    S_df = df_hiers[['unique_id'] + bottom_comb].drop_duplicates().reset_index(drop=True)\n",
    "    S_df = S_df.set_index('unique_id')\n",
    "    S_df = S_df.fillna('agg')\n",
    "    hiers_cols = []\n",
    "    for hier in spec:\n",
    "        hier_col = '/'.join(hier)\n",
    "        S_df[hier_col] = S_df[hier].agg('/'.join, axis=1)\n",
    "        hiers_cols.append(hier_col)\n",
    "    Y_df = df_hiers[['unique_id', 'ds', 'y']].set_index('unique_id')\n",
    "    \n",
    "    # Aggregations constraints S definition\n",
    "    S, tags = _to_summing_matrix(S_df.loc[bottom_hier, hiers_cols], sparse_s)\n",
    "    return Y_df, S, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a233df7-50d1-4a70-9f07-afb8bff2aa6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _to_upper_hierarchy(bottom_split, bottom_values, upper_key):\n",
    "    upper_split = upper_key.split('/')\n",
    "    upper_idxs = [bottom_split.index(i) for i in upper_split]\n",
    "\n",
    "    def join_upper(bottom_value):\n",
    "        bottom_parts = bottom_value.split('/')\n",
    "        return '/'.join(bottom_parts[i] for i in upper_idxs)\n",
    "\n",
    "    return [join_upper(val) for val in bottom_values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73d1052",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def aggregate(\n",
    "    df: pd.DataFrame,\n",
    "    spec: List[List[str]],\n",
    "    is_balanced: bool = False,\n",
    "    sparse_s: bool = False,\n",
    "):\n",
    "    \"\"\"Utils Aggregation Function.\n",
    "    Aggregates bottom level series contained in the pandas DataFrame `df` according\n",
    "    to levels defined in the `spec` list.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas DataFrame\n",
    "        Dataframe with columns `['ds', 'y']` and columns to aggregate.\n",
    "    spec : list of list of str\n",
    "        List of levels. Each element of the list should contain a list of columns of `df` to aggregate.\n",
    "    is_balanced : bool (default=False)\n",
    "        Deprecated.\n",
    "    sparse_s : bool (default=False)\n",
    "        Return `S_df` as a sparse dataframe.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Y_df : pandas DataFrame\n",
    "        Hierarchically structured series.\n",
    "    S_df : pandas DataFrame\n",
    "        Summing dataframe.\n",
    "    tags : dict\n",
    "        Aggregation indices.\n",
    "    \"\"\"\n",
    "    # Checks\n",
    "    if df.isnull().values.any():\n",
    "        raise ValueError('`df` contains null values')\n",
    "    if is_balanced:\n",
    "        warnings.warn(\n",
    "            \"`is_balanced` is deprecated and will be removed in a future version. \"\n",
    "            \"Don't set this argument to suppress this warning.\",\n",
    "            category=DeprecationWarning,\n",
    "        )\n",
    "            \n",
    "    # compute aggregations and tags\n",
    "    spec = sorted(spec, key=len)\n",
    "    bottom = spec[-1]\n",
    "    aggs = []\n",
    "    tags = {}\n",
    "    for levels in spec:\n",
    "        agg = df.groupby(levels + ['ds'], observed=True)['y'].sum()\n",
    "        if not agg.index.is_monotonic_increasing:\n",
    "            agg = agg.sort_index()\n",
    "        agg = agg.reset_index('ds')\n",
    "        group = agg.index.get_level_values(0)\n",
    "        if not pd.api.types.is_string_dtype(group.dtype):\n",
    "            group = group.astype(str)\n",
    "        for level in levels[1:]:\n",
    "            group = group + '/' + agg.index.get_level_values(level).str.replace('/', '_')\n",
    "        agg.index = group\n",
    "        agg.index.name = 'unique_id'\n",
    "        tags['/'.join(levels)] = group.unique().values\n",
    "        aggs.append(agg)\n",
    "    Y_df = pd.concat(aggs)\n",
    "\n",
    "    # construct S\n",
    "    bottom_key = '/'.join(bottom)\n",
    "    bottom_levels = tags[bottom_key]\n",
    "    S = np.empty((len(bottom_levels), len(spec)), dtype=object)\n",
    "    for j, levels in enumerate(spec[:-1]):\n",
    "        S[:, j] = _to_upper_hierarchy(bottom, bottom_levels, '/'.join(levels))\n",
    "    S[:, -1] = tags[bottom_key]\n",
    "    categories = list(tags.values())\n",
    "    try:\n",
    "        encoder = OneHotEncoder(categories=categories, sparse_output=sparse_s, dtype=np.float32)\n",
    "    except TypeError:  # sklearn < 1.2\n",
    "        encoder = OneHotEncoder(categories=categories, sparse=sparse_s, dtype=np.float32)    \n",
    "    S = encoder.fit_transform(S).T\n",
    "    if sparse_s:\n",
    "        df_constructor = pd.DataFrame.sparse.from_spmatrix\n",
    "    else:\n",
    "        df_constructor = pd.DataFrame\n",
    "    S_df = df_constructor(S, index=np.hstack(categories), columns=bottom_levels)\n",
    "    return Y_df, S_df, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cea2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(aggregate, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e70572-9c01-466d-a3e9-7667b92def2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# simple case\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        'cat1': ['a', 'a', 'a', 'b'],\n",
    "        'cat2': ['1', '2', '3', '2'],\n",
    "        'y': [10, 20, 30, 40],\n",
    "        'ds': ['2020-01-01', '2020-02-01', '2020-03-01', '2020-02-01']\n",
    "    }\n",
    ")\n",
    "df['country'] = 'COUNTRY'\n",
    "spec = [['country'], ['country', 'cat1'], ['country','cat1', 'cat2']]\n",
    "Y_df, S_df, tags = aggregate(df, spec)\n",
    "test_eq(\n",
    "    Y_df.index.tolist(), \n",
    "    3 * ['COUNTRY'] +\n",
    "    3 * ['COUNTRY/a'] +\n",
    "    ['COUNTRY/b'] +\n",
    "    ['COUNTRY/a/1', 'COUNTRY/a/2', 'COUNTRY/a/3'] +\n",
    "    ['COUNTRY/b/2']\n",
    ")\n",
    "test_eq(Y_df.loc['COUNTRY', 'y'].values, [10, 60, 30])\n",
    "test_eq(\n",
    "    S_df.index,\n",
    "    ['COUNTRY', 'COUNTRY/a', 'COUNTRY/b', 'COUNTRY/a/1', 'COUNTRY/a/2', 'COUNTRY/a/3', 'COUNTRY/b/2'],\n",
    ")\n",
    "test_eq(\n",
    "    S_df.columns,\n",
    "    ['COUNTRY/a/1', 'COUNTRY/a/2', 'COUNTRY/a/3', 'COUNTRY/b/2'],\n",
    ")\n",
    "expected_tags = {\n",
    "    'country': ['COUNTRY'],\n",
    "    'country/cat1': ['COUNTRY/a', 'COUNTRY/b'],\n",
    "    'country/cat1/cat2': ['COUNTRY/a/1', 'COUNTRY/a/2', 'COUNTRY/a/3','COUNTRY/b/2'],\n",
    "}\n",
    "for k, actual in tags.items():\n",
    "    test_eq(actual, expected_tags[k])\n",
    "\n",
    "# test categoricals don't produce all combinations\n",
    "df2 = df.copy()\n",
    "for col in ('country', 'cat1', 'cat2'):\n",
    "    df2[col] = df2[col].astype('category')\n",
    "\n",
    "Y_df2, *_ = aggregate(df2, spec)\n",
    "assert Y_df.shape[0] == Y_df2.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae76480-44d9-45ec-b50a-f8b666cc0200",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test unbalanced dataset\n",
    "max_tenure = 24\n",
    "dates = pd.date_range(start='2019-01-31', freq='M', periods=max_tenure)\n",
    "cohort_tenure = [24, 23, 22, 21]\n",
    "\n",
    "ts_list = []\n",
    "\n",
    "# Create ts for each cohort\n",
    "for i in range(len(cohort_tenure)):\n",
    "    ts_list.append(\n",
    "        generate_series(n_series=1, freq='M', min_length=cohort_tenure[i], max_length=cohort_tenure[i]).reset_index() \\\n",
    "            .assign(ult=i) \\\n",
    "            .assign(ds=dates[-cohort_tenure[i]:]) \\\n",
    "            .drop(columns=['unique_id'])\n",
    "    )\n",
    "df = pd.concat(ts_list, ignore_index=True)\n",
    "\n",
    "# Create categories\n",
    "df['pen'] = np.where(df['ult'] < 2, 'a', 'b')\n",
    "# Note that unique id requires strings\n",
    "df['ult'] = df['ult'].astype(str)\n",
    "\n",
    "hier_levels = [\n",
    "    ['pen'],\n",
    "    ['pen', 'ult'],\n",
    "]\n",
    "\n",
    "hier_df, S_df, tags = aggregate(df=df, spec=hier_levels)\n",
    "hier_df_before, S_df_before, _ = aggregate_before(df=df, spec=hier_levels)\n",
    "test_eq(S_df, S_df_before)\n",
    "test_eq(hier_df, hier_df_before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fd8bd9-d7e8-4602-a1ad-021f404532f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/Nixtla/transfer-learning-time-series/main/datasets/tourism.csv')\n",
    "df = df.rename({'Trips': 'y', 'Quarter': 'ds'}, axis=1)\n",
    "df.insert(0, 'Country', 'Australia')\n",
    "\n",
    "# grouped structure\n",
    "hiers_grouped = [['Country'],\n",
    "                 ['Country', 'State'], \n",
    "                 ['Country', 'Purpose'], \n",
    "                 ['Country', 'State', 'Region'], \n",
    "                 ['Country', 'State', 'Purpose'], \n",
    "                 ['Country', 'State', 'Region', 'Purpose']]\n",
    "\n",
    "# strictly hierarchical structure\n",
    "hiers_strictly = [['Country'],\n",
    "                  ['Country', 'State'], \n",
    "                  ['Country', 'State', 'Region']]\n",
    "\n",
    "# test strict\n",
    "hier_df, S_df, tags = aggregate(df=df, spec=hiers_strictly)\n",
    "test_eq(len(hier_df), 6800)\n",
    "test_eq(hier_df.index.nunique(), 85)\n",
    "test_eq(S_df.shape, (85, 76))\n",
    "test_eq(hier_df.index.unique(), S_df.index)\n",
    "test_eq(len(tags), len(hiers_strictly))                  \n",
    "\n",
    "# test grouped\n",
    "hier_df, S_df, tags = aggregate(df=df, spec=hiers_grouped)\n",
    "test_eq(len(hier_df), 34_000)\n",
    "test_eq(hier_df.index.nunique(), 425)\n",
    "test_eq(S_df.shape, (425, 304))\n",
    "test_eq(hier_df.index.unique(), S_df.index)\n",
    "test_eq(len(tags), len(hiers_grouped))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1cb923",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/Nixtla/transfer-learning-time-series/main/datasets/tourism.csv')\n",
    "df = df.rename({'Trips': 'y', 'Quarter': 'ds'}, axis=1)\n",
    "df.insert(0, 'Country', 'Australia')\n",
    "\n",
    "#Unit Test NaN Values\n",
    "df_nan = df.copy()\n",
    "df_nan.loc[0, 'Region'] = float('nan')\n",
    "test_fail(\n",
    "    aggregate,\n",
    "    contains='null values',\n",
    "    args=(df_nan, hiers_strictly),\n",
    ")\n",
    "\n",
    "#Unit Test None Values\n",
    "df_none = df.copy()\n",
    "df_none.loc[0, 'Region'] = None\n",
    "test_fail(\n",
    "    aggregate,\n",
    "    contains='null values',\n",
    "    args=(df_none, hiers_strictly),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b3828f-bbcc-4116-a969-49c78c33bf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# Test equality of aggregation and aggregation_before\n",
    "for name, spec in zip(['strict', 'grouped'], [hiers_strictly, hiers_grouped]):\n",
    "    with CodeTimer(f'{name} aggregation before'):\n",
    "        Y_df_before, S_df_before, tags_before = aggregate_before(df=df, spec=spec)\n",
    "    \n",
    "    with CodeTimer(f'{name} aggregation now'):\n",
    "        Y_df, S_df, tags = aggregate(df=df, spec=spec)\n",
    "    \n",
    "    np.testing.assert_allclose(\n",
    "        Y_df['y'].values,\n",
    "        Y_df_before['y'].values,\n",
    "    )\n",
    "    np.testing.assert_equal(S_df.values, S_df_before.values)\n",
    "    \n",
    "    test_eq(S_df.columns, S_df_before.columns)\n",
    "    test_eq(S_df.index, S_df_before.index)\n",
    "    \n",
    "    test_eq(Y_df.columns, Y_df_before.columns)\n",
    "    test_eq(Y_df.index, Y_df_before.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7688d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# Test equality of sparse and non-sparse aggregation\n",
    "with CodeTimer('strict non-sparse aggregate'):\n",
    "    Y_df, S_df, tags = aggregate(df=df, sparse_s=False, spec=hiers_strictly)\n",
    "\n",
    "with CodeTimer('strict sparse aggregate'):\n",
    "    Y_df_sparse, S_df_sparse, tags_sparse = aggregate(df=df, sparse_s=True, spec=hiers_strictly)\n",
    "\n",
    "test_close(Y_df.y.values, Y_df_sparse.y.values)\n",
    "test_eq(S_df.values, S_df_sparse.values)\n",
    "\n",
    "test_eq(S_df.columns, S_df_sparse.columns)\n",
    "test_eq(S_df.index, S_df_sparse.index)\n",
    "\n",
    "test_eq(Y_df.columns, Y_df_sparse.columns)\n",
    "test_eq(Y_df.index, Y_df_sparse.index)\n",
    "\n",
    "with CodeTimer('grouped non-sparse aggregate'):\n",
    "    Y_df, S_df, tags = aggregate(df=df, sparse_s=False, spec=hiers_grouped)\n",
    "\n",
    "with CodeTimer('grouped sparse aggregate'):\n",
    "    Y_df_sparse, S_df_sparse, tags_sparse = aggregate(df=df, sparse_s=True, spec=hiers_grouped)\n",
    "\n",
    "test_close(Y_df.y.values, Y_df_sparse.y.values)\n",
    "test_eq(S_df.values, S_df_sparse.values)\n",
    "\n",
    "test_eq(S_df.columns, S_df_sparse.columns)\n",
    "test_eq(S_df.index, S_df_sparse.index)\n",
    "\n",
    "test_eq(Y_df.columns, Y_df_sparse.columns)\n",
    "test_eq(Y_df.index, Y_df_sparse.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22febc26-1901-4bef-a181-09ae2f52453b",
   "metadata": {},
   "source": [
    "# Hierarchical Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b125e256-c210-4776-ac55-9841acee583b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class HierarchicalPlot:\n",
    "    \"\"\" Hierarchical Plot\n",
    "\n",
    "    This class contains a collection of matplotlib visualization methods, suited for small\n",
    "    to medium sized hierarchical series.\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `S`: pd.DataFrame with summing matrix of size `(base, bottom)`, see [aggregate function](https://nixtla.github.io/hierarchicalforecast/utils.html#aggregate).<br>\n",
    "    `tags`: np.ndarray, with hierarchical aggregation indexes, where \n",
    "        each key is a level and its value contains tags associated to that level.<br><br>\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 S: pd.DataFrame,\n",
    "                 tags: Dict[str, np.ndarray]):\n",
    "        self.S = S\n",
    "        self.tags = tags\n",
    "\n",
    "    def plot_summing_matrix(self):\n",
    "        \"\"\" Summation Constraints plot\n",
    "        \n",
    "        This method simply plots the hierarchical aggregation\n",
    "        constraints matrix $\\mathbf{S}$.\n",
    "        \"\"\"\n",
    "        plt.figure(num=1, figsize=(4, 6), dpi=80, facecolor='w')\n",
    "        plt.spy(self.S)\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "    def plot_series(self,\n",
    "                    series: str,\n",
    "                    Y_df: pd.DataFrame,\n",
    "                    models: Optional[List[str]] = None,\n",
    "                    level: Optional[List[int]] = None):\n",
    "        \"\"\" Single Series plot\n",
    "\n",
    "        **Parameters:**<br>\n",
    "        `series`: str, string identifying the `'unique_id'` any-level series to plot.<br>\n",
    "        `Y_df`: pd.DataFrame, hierarchically structured series ($\\mathbf{y}_{[a,b]}$). \n",
    "                It contains columns `['unique_id', 'ds', 'y']`, it may have `'models'`.<br>\n",
    "        `models`: List[str], string identifying filtering model columns.\n",
    "        `level`: float list 0-100, confidence levels for prediction intervals available in `Y_df`.<br>\n",
    "\n",
    "        **Returns:**<br>\n",
    "        Single series plot with filtered models and prediction interval level.<br><br>\n",
    "        \"\"\"\n",
    "        if series not in self.S.index:\n",
    "            raise Exception(f'time series {series} not found')\n",
    "        fig, ax = plt.subplots(1, 1, figsize = (20, 7))\n",
    "        df_plot = Y_df.loc[series].set_index('ds')\n",
    "        cols = models if models is not None else df_plot.columns\n",
    "        cols_wo_levels = [col for col in cols if ('-lo-' not in col and '-hi-' not in col)]\n",
    "        try:\n",
    "            cmap = plt.get_cmap(\"tab10\", 10)\n",
    "        except AttributeError:\n",
    "            cmap = plt.cm.get_cmap(\"tab10\", 10)\n",
    "        cmap = [cmap(i) for i in range(10)][:len(cols_wo_levels)]\n",
    "        cmap_dict = dict(zip(cols_wo_levels, cmap))\n",
    "        for col in cols_wo_levels:\n",
    "            ax.plot(df_plot[col], linewidth=2, label=col, color=cmap_dict[col])\n",
    "            if level is not None and col != 'y':\n",
    "                for lv in level:\n",
    "                    if f'{col}-lo-{lv}' not in df_plot.columns:\n",
    "                        # if model\n",
    "                        # doesnt have levels\n",
    "                        continue\n",
    "                    ax.fill_between(\n",
    "                        df_plot.dropna().index, \n",
    "                        df_plot[f'{col}-lo-{lv}'].dropna().values, \n",
    "                        df_plot[f'{col}-hi-{lv}'].dropna().values,\n",
    "                        alpha=-lv/100 + 1,\n",
    "                        color=cmap_dict[col],\n",
    "                        label=f'{col}_level_{lv}'\n",
    "                    )\n",
    "        ax.set_title(f'{series} Forecast', fontsize=22)\n",
    "        ax.set_xlabel('Timestamp [t]', fontsize=20)\n",
    "        ax.legend(prop={'size': 15})\n",
    "        ax.grid()\n",
    "        ax.xaxis.set_major_locator(\n",
    "            plt.MaxNLocator(min(max(len(df_plot) // 10, 1), 10))\n",
    "        )\n",
    "        for label in (ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "            label.set_fontsize(20)\n",
    "                    \n",
    "    def plot_hierarchically_linked_series(self,\n",
    "                                          bottom_series: str,\n",
    "                                          Y_df: pd.DataFrame,\n",
    "                                          models: Optional[List[str]] = None,\n",
    "                                          level: Optional[List[int]] = None):\n",
    "        \"\"\" Hierarchically Linked Series plot\n",
    "\n",
    "        **Parameters:**<br>\n",
    "        `bottom_series`: str, string identifying the `'unique_id'` bottom-level series to plot.<br>\n",
    "        `Y_df`: pd.DataFrame, hierarchically structured series ($\\mathbf{y}_{[a,b]}$). \n",
    "                It contains columns ['unique_id', 'ds', 'y'] and models. <br>\n",
    "        `models`: List[str], string identifying filtering model columns.\n",
    "        `level`: float list 0-100, confidence levels for prediction intervals available in `Y_df`.<br>\n",
    "\n",
    "        **Returns:**<br>\n",
    "        Collection of hierarchilly linked series plots associated with the `bottom_series`\n",
    "        and filtered models and prediction interval level.<br><br>\n",
    "        \"\"\"\n",
    "        if bottom_series not in self.S.columns:\n",
    "            raise Exception(f'bottom time series {bottom_series} not found')\n",
    "        linked_series = self.S[bottom_series].loc[lambda x: x == 1.].index\n",
    "        fig, axs = plt.subplots(len(linked_series), 1, figsize=(20, 2 * len(linked_series)))\n",
    "        cols = models if models is not None else Y_df.drop(['ds'], axis=1)\n",
    "        cols_wo_levels = [col for col in cols if ('-lo-' not in col and '-hi-' not in col)]\n",
    "        cmap = plt.cm.get_cmap(\"tab10\", 10)\n",
    "        cmap = [cmap(i) for i in range(10)][:len(cols_wo_levels)]\n",
    "        cmap_dict = dict(zip(cols_wo_levels, cmap))\n",
    "        for idx, series in enumerate(linked_series):\n",
    "            df_plot = Y_df.loc[[series]].set_index('ds')\n",
    "            for col in cols_wo_levels:\n",
    "                axs[idx].plot(df_plot[col], linewidth=2, label=col, color=cmap_dict[col])\n",
    "                if level is not None and col != 'y':\n",
    "                    for lv in level:\n",
    "                        if f'{col}-lo-{lv}' not in df_plot.columns:\n",
    "                            # if model\n",
    "                            # doesnt have levels\n",
    "                            continue\n",
    "                        axs[idx].fill_between(\n",
    "                            df_plot.dropna().index, \n",
    "                            df_plot[f'{col}-lo-{lv}'].dropna().values, \n",
    "                            df_plot[f'{col}-hi-{lv}'].dropna().values,\n",
    "                            alpha=-lv/100 + 1,\n",
    "                            color=cmap_dict[col],\n",
    "                            label=f'{col}_level_{lv}'\n",
    "                        )\n",
    "            axs[idx].set_title(f'{series}', fontsize=10)\n",
    "            axs[idx].grid()\n",
    "            axs[idx].get_xaxis().label.set_visible(False)\n",
    "            axs[idx].legend().set_visible(False)\n",
    "            axs[idx].xaxis.set_major_locator(\n",
    "                plt.MaxNLocator(min(max(len(df_plot) // 10, 1), 10))\n",
    "            )\n",
    "            for label in (axs[idx].get_xticklabels() + axs[idx].get_yticklabels()):\n",
    "                label.set_fontsize(10)\n",
    "        plt.subplots_adjust(hspace=0.4)\n",
    "        handles, labels = axs[0].get_legend_handles_labels()\n",
    "        kwargs = dict(loc='lower center', \n",
    "                      prop={'size': 10}, \n",
    "                      bbox_to_anchor=(0, 0.05, 1, 1))\n",
    "        if sys.version_info.minor > 7:\n",
    "            kwargs['ncols'] = np.max([2, np.ceil(len(labels) / 2)])\n",
    "        fig.legend(handles, labels, **kwargs)\n",
    "\n",
    "    def plot_hierarchical_predictions_gap(self,\n",
    "                                          Y_df: pd.DataFrame,\n",
    "                                          models: Optional[List[str]] = None,\n",
    "                                          xlabel: Optional[str] = None,\n",
    "                                          ylabel: Optional[str] = None,\n",
    "                                          ):\n",
    "        \"\"\" Hierarchically Predictions Gap plot\n",
    "\n",
    "        **Parameters:**<br>\n",
    "        `Y_df`: pd.DataFrame, hierarchically structured series ($\\mathbf{y}_{[a,b]}$). \n",
    "                It contains columns ['unique_id', 'ds', 'y'] and models. <br>\n",
    "        `models`: List[str], string identifying filtering model columns.\n",
    "        `xlabel`: str, string for the plot's x axis label.\n",
    "        `ylable`: str, string for the plot's y axis label.\n",
    "\n",
    "        **Returns:**<br>\n",
    "        Plots of aggregated predictions at different levels of the hierarchical structure.\n",
    "        The aggregation is performed according to the tag levels see \n",
    "        [aggregate function](https://nixtla.github.io/hierarchicalforecast/utils.html).<br><br>\n",
    "        \"\"\"\n",
    "        # Parse predictions dataframe\n",
    "        horizon_dates = Y_df['ds'].unique()\n",
    "        cols = models if models is not None else Y_df.drop(['ds', 'y'], axis=1).columns\n",
    "        \n",
    "        # Plot predictions across tag levels\n",
    "        fig, ax = plt.subplots(figsize=(8, 5))\n",
    "        \n",
    "        if 'y' in Y_df.columns:\n",
    "            idx_top = self.S.sum(axis=1).idxmax()\n",
    "            y_plot = Y_df.loc[idx_top].y.values\n",
    "            plt.plot(horizon_dates, y_plot, label='True')\n",
    "\n",
    "        ys = []\n",
    "        for tag in self.tags:\n",
    "            y_plot = sum([Y_df[cols].loc[Y_df.index == idx].values \\\n",
    "                          for idx in self.tags[tag]])\n",
    "            plt.plot(horizon_dates, y_plot, label=f'Level: {tag}')\n",
    "            \n",
    "            ys.append(y_plot[:,None])\n",
    "\n",
    "        plt.title('Predictions Accumulated Difference')\n",
    "        if ylabel is not None:\n",
    "            plt.ylabel(ylabel)\n",
    "        if xlabel is not None:\n",
    "            plt.xlabel(xlabel)\n",
    "\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27c4cff-ebbe-41f9-920b-7bbc997d0adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(HierarchicalPlot, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d1872c-7979-44f7-972b-2031729b04b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(HierarchicalPlot.plot_summing_matrix, \n",
    "         name='plot_summing_matrix', title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920de36f-e7fe-4ea4-81bb-d0f897e1f469",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(HierarchicalPlot.plot_series, \n",
    "         name='plot_series', title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa265cd-0c05-4617-a40a-f2c62513f7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(HierarchicalPlot.plot_hierarchically_linked_series, \n",
    "         name='plot_hierarchically_linked_series', title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8621cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(HierarchicalPlot.plot_hierarchical_predictions_gap,\n",
    "         name='plot_hierarchical_predictions_gap', title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662bdbce-cb13-4ba2-a794-1cd1bc3b96a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "hplots = HierarchicalPlot(S=S_df, tags=tags)\n",
    "hplots.plot_summing_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdaa2909-6344-4b6c-a4d9-6d4f903ce408",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "hier_df['Model'] = hier_df['y'] * 1.1\n",
    "hier_df['Model-lo-80'] = hier_df['Model'] - 0.1 * hier_df['Model']\n",
    "hier_df['Model-hi-80'] = hier_df['Model'] + 0.1 * hier_df['Model']\n",
    "hier_df['Model-lo-90'] = hier_df['Model'] - 0.2 * hier_df['Model']\n",
    "hier_df['Model-hi-90'] = hier_df['Model'] + 0.2 * hier_df['Model']\n",
    "hplots.plot_series(\n",
    "    series='Australia', \n",
    "    Y_df=hier_df,\n",
    "    level=[80, 90]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88068d1a-b670-410a-975e-a92e22ea9948",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "hplots.plot_series(series='Australia', \n",
    "                   Y_df=hier_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b90ed3-e1c3-47da-850b-ccda3319f630",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "hplots.plot_hierarchically_linked_series(\n",
    "    bottom_series='Australia/Western Australia/Experience Perth/Visiting', \n",
    "    Y_df=hier_df,\n",
    "    level=[80, 90]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45855eb-e800-40db-a00b-5ddb956ae348",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "hplots.plot_hierarchically_linked_series(\n",
    "    bottom_series='Australia/Western Australia/Experience Perth/Visiting', \n",
    "    Y_df=hier_df,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c9263e-6d07-4527-88ea-40153435f44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test series with just one value\n",
    "hplots.plot_hierarchically_linked_series(\n",
    "    bottom_series='Australia/Western Australia/Experience Perth/Visiting', \n",
    "    Y_df=hier_df.groupby('unique_id').tail(1),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7cc841-73db-4e00-99e4-b123b9d09db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "hplots.plot_hierarchical_predictions_gap(Y_df=hier_df.drop(columns='y'), models=['Model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c2b84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "from statsforecast.core import StatsForecast\n",
    "from statsforecast.models import AutoARIMA, ETS, Naive\n",
    "from datasetsforecast.hierarchical import HierarchicalData\n",
    "\n",
    "Y_df, S, tags = HierarchicalData.load('./data', 'Labour')\n",
    "Y_df['ds'] = pd.to_datetime(Y_df['ds'])\n",
    "\n",
    "Y_test_df  = Y_df.groupby('unique_id').tail(24)\n",
    "Y_train_df = Y_df.drop(Y_test_df.index)\n",
    "Y_test_df  = Y_test_df.set_index('unique_id')\n",
    "Y_train_df = Y_train_df.set_index('unique_id')\n",
    "\n",
    "fcst = StatsForecast(\n",
    "    df=Y_train_df, \n",
    "    #models=[AutoARIMA(season_length=12), Naive()], \n",
    "    models=[ETS(season_length=12, model='AAZ')],\n",
    "    freq='MS', \n",
    "    n_jobs=-1\n",
    ")\n",
    "Y_hat_df = fcst.forecast(h=24)\n",
    "\n",
    "# Plot prediction difference of different aggregation\n",
    "# Levels Country, Country/Region, Country/Gender/Region ...\n",
    "hplots = HierarchicalPlot(S=S, tags=tags)\n",
    "\n",
    "hplots.plot_hierarchical_predictions_gap(\n",
    "    Y_df=Y_hat_df, models='ETS',\n",
    "    xlabel='Month', ylabel='Predictions',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fcfbc2",
   "metadata": {},
   "source": [
    "# External Forecast Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c629ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "# convert levels to output quantile names\n",
    "def level_to_outputs(level:Iterable[int]):\n",
    "    \"\"\" Converts list of levels into output names matching StatsForecast and NeuralForecast methods.\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `level`: int list [0,100]. Probability levels for prediction intervals.<br>\n",
    "\n",
    "    **Returns:**<br>\n",
    "    `output_names`: str list. String list with output column names.\n",
    "    \"\"\"\n",
    "    qs = sum([[50-l/2, 50+l/2] for l in level], [])\n",
    "    output_names = sum([[f'-lo-{l}', f'-hi-{l}'] for l in level], [])\n",
    "\n",
    "    sort_idx = np.argsort(qs)\n",
    "    quantiles = np.array(qs)[sort_idx]\n",
    "\n",
    "    # Add default median\n",
    "    quantiles = np.concatenate([np.array([50]), quantiles]) / 100\n",
    "    output_names = list(np.array(output_names)[sort_idx])\n",
    "    output_names.insert(0, '-median')\n",
    "    \n",
    "    return quantiles, output_names\n",
    "\n",
    "# convert quantiles to output quantile names\n",
    "def quantiles_to_outputs(quantiles:Iterable[float]):\n",
    "    \"\"\"Converts list of quantiles into output names matching StatsForecast and NeuralForecast methods.\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `quantiles`: float list [0., 1.]. Alternative to level, quantiles to estimate from y distribution.<br>\n",
    "\n",
    "    **Returns:**<br>\n",
    "    `output_names`: str list. String list with output column names.\n",
    "    \"\"\"\n",
    "    output_names = []\n",
    "    for q in quantiles:\n",
    "        if q<.50:\n",
    "            output_names.append(f'-lo-{np.round(100-200*q,2)}')\n",
    "        elif q>.50:\n",
    "            output_names.append(f'-hi-{np.round(100-200*(1-q),2)}')\n",
    "        else:\n",
    "            output_names.append('-median')\n",
    "    return quantiles, output_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b52fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "# given input array of sample forecasts and inptut quantiles/levels, \n",
    "# output a Pandas Dataframe with columns of quantile predictions\n",
    "def samples_to_quantiles_df(samples: np.ndarray, \n",
    "                            unique_ids: Sequence[str], \n",
    "                            dates: List[str], \n",
    "                            quantiles: Optional[List[float]] = None,\n",
    "                            level: Optional[List[int]] = None, \n",
    "                            model_name: Optional[str] = \"model\"):\n",
    "    \"\"\" Transform Random Samples into HierarchicalForecast input.\n",
    "    Auxiliary function to create compatible HierarchicalForecast input `Y_hat_df` dataframe.\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `samples`: numpy array. Samples from forecast distribution of shape [n_series, n_samples, horizon].<br>\n",
    "    `unique_ids`: string list. Unique identifiers for each time series.<br>\n",
    "    `dates`: datetime list. List of forecast dates.<br>\n",
    "    `quantiles`: float list in [0., 1.]. Alternative to level, quantiles to estimate from y distribution.<br>\n",
    "    `level`: int list in [0,100]. Probability levels for prediction intervals.<br>\n",
    "    `model_name`: string. Name of forecasting model.<br>\n",
    "\n",
    "    **Returns:**<br>\n",
    "    `quantiles`: float list in [0., 1.]. quantiles to estimate from y distribution .<br>\n",
    "    `Y_hat_df`: pd.DataFrame. With base quantile forecasts with columns ds and models to reconcile indexed by unique_id.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the shape of the array\n",
    "    n_series, n_samples, horizon = samples.shape\n",
    "\n",
    "    assert n_series == len(unique_ids)\n",
    "    assert horizon == len(dates)\n",
    "    assert (quantiles is not None) ^ (level is not None)  #check exactly one of quantiles/levels has been input\n",
    "\n",
    "    #create initial dictionary\n",
    "    forecasts_mean = np.mean(samples, axis=1).flatten()\n",
    "    unique_ids = np.repeat(unique_ids, horizon)\n",
    "    ds = np.tile(dates, n_series)\n",
    "    data = pd.DataFrame({\"unique_id\":unique_ids, \"ds\":ds, model_name:forecasts_mean})\n",
    "\n",
    "    #create quantiles and quantile names\n",
    "    if level is not None:\n",
    "        _quantiles, quantile_names = level_to_outputs(level)\n",
    "    elif quantiles is not None:\n",
    "        _quantiles, quantile_names = quantiles_to_outputs(quantiles)\n",
    "\n",
    "    percentiles = [quantile * 100 for quantile in _quantiles]\n",
    "    col_names = np.array([model_name + quantile_name for quantile_name in quantile_names])\n",
    "    \n",
    "    #add quantiles to dataframe\n",
    "    forecasts_quantiles = np.percentile(samples, percentiles, axis=1)\n",
    "\n",
    "    forecasts_quantiles = np.transpose(forecasts_quantiles, (1,2,0)) # [Q,H,N] -> [N,H,Q]\n",
    "    forecasts_quantiles = forecasts_quantiles.reshape(-1,len(_quantiles))\n",
    "\n",
    "    df = pd.DataFrame(data=forecasts_quantiles, \n",
    "                      columns=col_names)\n",
    "    \n",
    "    return _quantiles, pd.concat([data,df], axis=1).set_index('unique_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e4fcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(samples_to_quantiles_df, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500ad055",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "#level_to_outputs unit tests\n",
    "test_eq(\n",
    "    level_to_outputs([80, 90]),\n",
    "    ([0.5 , 0.05, 0.1 , 0.9 , 0.95], ['-median', '-lo-90', '-lo-80', '-hi-80', '-hi-90'])\n",
    ")\n",
    "test_eq(\n",
    "    level_to_outputs([30]),\n",
    "    ([0.5 , 0.35, 0.65], ['-median', '-lo-30', '-hi-30'])\n",
    ")\n",
    "#quantiles_to_outputs unit tests\n",
    "test_eq(\n",
    "    quantiles_to_outputs([0.2, 0.4, 0.6, 0.8]),\n",
    "    ([0.2,0.4,0.6, 0.8], ['-lo-60.0', '-lo-20.0', '-hi-20.0', '-hi-60.0'])\n",
    ")\n",
    "test_eq(\n",
    "    quantiles_to_outputs([0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]),\n",
    "    ([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9], \n",
    "     ['-lo-80.0', '-lo-60.0', '-lo-40.0', '-lo-20.0', '-median', '-hi-20.0', '-hi-40.0', '-hi-60.0', '-hi-80.0'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f4f2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "#samples_to_quantiles_df unit tests\n",
    "start_date = pd.Timestamp(\"2023-06-01\")\n",
    "end_date = pd.Timestamp(\"2023-06-10\")\n",
    "frequency = \"D\"  # Daily frequency\n",
    "dates = pd.date_range(start=start_date, end=end_date, freq=frequency).tolist()\n",
    "samples = np.random.rand(3, 200, 10)\n",
    "unique_ids = ['id1', 'id2', 'id3']\n",
    "level = np.array([10, 50, 90])\n",
    "quantiles=np.array([0.5, 0.05, 0.25, 0.45, 0.55, 0.75, 0.95])\n",
    "\n",
    "ret_quantiles_1, ret_df_1 = samples_to_quantiles_df(samples, unique_ids, dates, level=level)\n",
    "ret_quantiles_2, ret_df_2 = samples_to_quantiles_df(samples, unique_ids, dates, quantiles=quantiles)\n",
    "\n",
    "test_eq(\n",
    "    ret_quantiles_1,\n",
    "    quantiles\n",
    ")\n",
    "test_eq(\n",
    "    ret_df_1.columns,\n",
    "    ['ds', 'model', 'model-median', 'model-lo-90', 'model-lo-50', 'model-lo-10', 'model-hi-10', 'model-hi-50', 'model-hi-90']\n",
    ")\n",
    "test_eq(\n",
    "    ret_df_1.index,\n",
    "    ['id1', 'id1', 'id1', 'id1', 'id1', 'id1', 'id1', 'id1', 'id1', 'id1',\n",
    "       'id2', 'id2', 'id2', 'id2', 'id2', 'id2', 'id2', 'id2', 'id2', 'id2',\n",
    "       'id3', 'id3', 'id3', 'id3', 'id3', 'id3', 'id3', 'id3', 'id3', 'id3']\n",
    ")\n",
    "test_eq(\n",
    "    ret_quantiles_1, ret_quantiles_2\n",
    ")\n",
    "test_eq(\n",
    "    ret_df_1.index, ret_df_2.index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc421ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_to_quantiles_df(samples, unique_ids, dates, level=level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b5ddbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_to_quantiles_df(samples, unique_ids, dates, quantiles=quantiles)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
