{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HierarchicalForecast with GluonTS Example Notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example notebook which shows how HierarchicalForecast's reconciliation capabilities can be integrated with other popular machine learning libraries, in this case GluonTS. \n",
    "\n",
    "It trains the GluonTS DeepAREstimator on the TourismLarge Hierarchical Dataset, then uses the `samples_to_quantiles_df` util function to transform the output forecasts into a dataframe compatible with HierarchicalForecast's reconciliation functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install gluonts\n",
    "!pip install pytorch_lightning\n",
    "!pip install datasetsforecast\n",
    "!pip install git+https://github.com/Nixtla/hierarchicalforecast.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mxnet-cu112\n",
    "import mxnet as mx\n",
    "mx.context.num_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasetsforecast.hierarchical import HierarchicalData\n",
    "from gluonts.dataset.pandas import PandasDataset\n",
    "from gluonts.mx.model.deepar import DeepAREstimator\n",
    "from gluonts.mx.trainer import Trainer\n",
    "from gluonts.evaluation import make_evaluation_predictions\n",
    "\n",
    "from hierarchicalforecast.methods import BottomUp, MinTrace\n",
    "from hierarchicalforecast.core import HierarchicalReconciliation\n",
    "from hierarchicalforecast.evaluation import scaled_crps\n",
    "from hierarchicalforecast.utils import samples_to_quantiles_df\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load hierarchical dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This detailed Australian Tourism Dataset comes from the National Visitor Survey, managed by the Tourism Research Australia, it is composed of 555 monthly series from 1998 to 2016, it is organized geographically, and purpose of travel. The natural geographical hierarchy comprises seven states, divided further in 27 zones and 76 regions. The purpose of travel categories are holiday, visiting friends and relatives (VFR), business and other. The MinT (Wickramasuriya et al., 2019), among other hierarchical forecasting studies has used the dataset it in the past. The dataset can be accessed in the [MinT reconciliation webpage](https://robjhyndman.com/publications/mint/), although other sources are available.\n",
    "\n",
    "| Geographical Division | Number of series per division | Number of series per purpose | Total |\n",
    "|          ---          |               ---             |              ---             |  ---  |\n",
    "|  Australia            |              1                |               4              |   5   |\n",
    "|  States               |              7                |              28              |  35   |\n",
    "|  Zones                |             27                |              108             |  135  |\n",
    "|  Regions              |             76                |              304             |  380  |\n",
    "|  Total                |            111                |              444             |  555  |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'TourismLarge'\n",
    "Y_df, S_df, tags = HierarchicalData.load(directory = \"./data\", group=dataset)\n",
    "Y_df['ds'] = pd.to_datetime(Y_df['ds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_hier_df(Y_df, S_df):\n",
    "    # sorts unique_id lexicographically\n",
    "    Y_df.unique_id = Y_df.unique_id.astype('category')\n",
    "    Y_df.unique_id = Y_df.unique_id.cat.set_categories(S_df.index)\n",
    "    Y_df = Y_df.sort_values(by=['unique_id', 'ds'])\n",
    "    return Y_df\n",
    "\n",
    "Y_df = sort_hier_df(Y_df, S_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon = 12\n",
    "\n",
    "Y_test_df = Y_df.groupby('unique_id').tail(horizon)\n",
    "Y_train_df = Y_df.drop(Y_test_df.index)\n",
    "Y_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = PandasDataset.from_long_dataframe(Y_train_df, target=\"y\", item_id=\"unique_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fit and Predict Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = DeepAREstimator(\n",
    "    freq=\"M\",\n",
    "    prediction_length=horizon,\n",
    "    trainer=Trainer(ctx = mx.context.gpu(),\n",
    "                    epochs=20),\n",
    ")\n",
    "predictor = estimator.train(ds)\n",
    "\n",
    "forecast_it = predictor.predict(ds, num_samples=1000)\n",
    "\n",
    "forecasts = list(forecast_it)\n",
    "forecasts = np.array([arr.samples for arr in forecasts])\n",
    "forecasts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Reconciliation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level = np.arange(1, 100, 2)\n",
    "\n",
    "#transform the output of DeepAREstimator to a form that is compatible with HierarchicalForecast\n",
    "quantiles, forecast_df = samples_to_quantiles_df(samples=forecasts, \n",
    "                               unique_ids=S_df.index, \n",
    "                               dates=Y_test_df['ds'].unique(), \n",
    "                               level=level,\n",
    "                               model_name='DeepAREstimator')\n",
    "\n",
    "#reconcile forecasts\n",
    "reconcilers = [\n",
    "    BottomUp(),\n",
    "    MinTrace('ols')\n",
    "]\n",
    "hrec = HierarchicalReconciliation(reconcilers=reconcilers)\n",
    "\n",
    "forecast_rec = hrec.reconcile(Y_hat_df=forecast_df, S=S_df, tags=tags, level=level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_rec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_model_names = ['DeepAREstimator/MinTrace_method-ols', 'DeepAREstimator/BottomUp']\n",
    "\n",
    "quantiles = np.array(quantiles[1:]) #remove first quantile (median)\n",
    "n_quantiles = len(quantiles)\n",
    "n_series = len(S_df)\n",
    "\n",
    "for name in rec_model_names:\n",
    "    quantile_columns = [col for col in forecast_rec.columns if (name+'-') in col]\n",
    "    y_rec  = forecast_rec[quantile_columns].values \n",
    "    y_test = Y_test_df['y'].values\n",
    "\n",
    "    y_rec  = y_rec.reshape(n_series, horizon, n_quantiles)\n",
    "    y_test = y_test.reshape(n_series, horizon)\n",
    "    scrps  = scaled_crps(y=y_test, y_hat=y_rec, quantiles=quantiles)\n",
    "    print(\"{:<40} {:.5f}\".format(name+\":\", scrps))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
