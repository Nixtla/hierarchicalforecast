{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9a2ffba7",
   "metadata": {},
   "source": [
    "# Probabilistic Forecast Evaluation\n",
    "\n",
    "> Hierarchical Forecast's reconciliation and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebf6eb9",
   "metadata": {},
   "source": [
    "This notebook offers a step to step guide to create a hierarchical forecasting pipeline.\n",
    "\n",
    "In the pipeline we will use `HierarchicalForecast` and `StatsForecast` core class, to create base predictions, reconcile and evaluate them. \n",
    "\n",
    "We will use the TourismL dataset that summarizes large Australian national visitor survey.\n",
    "\n",
    "Outline\n",
    "1. Installing Packages\n",
    "2. Prepare TourismL dataset\n",
    "    - Read and aggregate\n",
    "    - StatsForecast's Base Predictions\n",
    "3. Reconciliar\n",
    "4. Evaluar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08fca30",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/Nixtla/hierarchicalforecast/blob/main/nbs/examples/TourismLarge-Evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690aea05",
   "metadata": {},
   "source": [
    "## 1. Installing HierarchicalForecast\n",
    "\n",
    "We assume you have StatsForecast and HierarchicalForecast already installed, if not \n",
    "check this guide for instructions on how to install HierarchicalForecast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225fd9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install hierarchicalforecast statsforecast datasetsforecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1624c79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from statsforecast.core import StatsForecast\n",
    "from statsforecast.models import AutoARIMA, Naive\n",
    "\n",
    "from hierarchicalforecast.core import HierarchicalReconciliation\n",
    "from hierarchicalforecast.methods import BottomUp, TopDown, MinTrace, ERM\n",
    "\n",
    "from hierarchicalforecast.utils import is_strictly_hierarchical\n",
    "from hierarchicalforecast.utils import HierarchicalPlot, CodeTimer\n",
    "\n",
    "from datasetsforecast.hierarchical import HierarchicalData, HierarchicalInfo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58961ccd",
   "metadata": {},
   "source": [
    "## 2. Preparing TourismL Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d002ec0",
   "metadata": {},
   "source": [
    "### 2.1 Read Hierarchical Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e107596b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ['Labour', 'Traffic', 'TourismSmall', 'TourismLarge', 'Wiki2']\n",
    "dataset = 'TourismSmall' # 'TourismLarge'\n",
    "verbose = True\n",
    "intervals_method = 'bootstrap'\n",
    "LEVEL = np.arange(0, 100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0045589f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with CodeTimer('Read and Parse data   ', verbose):\n",
    "    print(f'{dataset}')\n",
    "    if not os.path.exists('./data'):\n",
    "        os.makedirs('./data')\n",
    "    \n",
    "    dataset_info = HierarchicalInfo[dataset]\n",
    "    Y_df, S_df, tags = HierarchicalData.load(directory=f'./data/{dataset}', group=dataset)\n",
    "    Y_df['ds'] = pd.to_datetime(Y_df['ds'])\n",
    "\n",
    "    # Train/Test Splits\n",
    "    horizon = dataset_info.horizon\n",
    "    seasonality = dataset_info.seasonality\n",
    "    Y_test_df = Y_df.groupby('unique_id', as_index=False).tail(horizon)\n",
    "    Y_train_df = Y_df.drop(Y_test_df.index)\n",
    "    S_df = S_df.reset_index(names=\"unique_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec911cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_info.seasonality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3958dc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "hplot = HierarchicalPlot(S=S_df, tags=tags)\n",
    "hplot.plot_summing_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56af1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fec0eb",
   "metadata": {},
   "source": [
    "### 2.2 StatsForecast's Base Predictions\n",
    "\n",
    "This cell computes the base predictions `Y_hat_df` for all the series in `Y_df` using StatsForecast's `AutoARIMA`.\n",
    "Additionally we obtain insample predictions `Y_fitted_df` for the methods that require them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76df4d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "with CodeTimer('Fit/Predict Model     ', verbose):\n",
    "    # Read to avoid unnecesary AutoARIMA computation\n",
    "    yhat_file = f'./data/{dataset}/Y_hat.csv'\n",
    "    yfitted_file = f'./data/{dataset}/Y_fitted.csv'\n",
    "\n",
    "    if os.path.exists(yhat_file):\n",
    "        Y_hat_df = pd.read_csv(yhat_file, parse_dates=['ds'])\n",
    "        Y_fitted_df = pd.read_csv(yfitted_file, parse_dates=['ds'])\n",
    "\n",
    "    else:\n",
    "        fcst = StatsForecast(\n",
    "            models=[AutoARIMA(season_length=seasonality)],\n",
    "            fallback_model=[Naive()],\n",
    "            freq=dataset_info.freq, \n",
    "            n_jobs=-1\n",
    "        )\n",
    "        Y_hat_df = fcst.forecast(df=Y_train_df, h=horizon, fitted=True, level=LEVEL)\n",
    "        Y_fitted_df = fcst.forecast_fitted_values()\n",
    "        Y_hat_df.to_csv(yhat_file, index=False)\n",
    "        Y_fitted_df.to_csv(yfitted_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e557adc7",
   "metadata": {},
   "source": [
    "## 3. Reconcile Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1982db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with CodeTimer('Reconcile Predictions ', verbose):\n",
    "    if is_strictly_hierarchical(S=S_df.drop(columns=\"unique_id\").values.astype(np.float32), tags={key: S_df[\"unique_id\"].isin(val).values.nonzero()[0] for key, val in tags.items()}):\n",
    "        reconcilers = [\n",
    "            BottomUp(),\n",
    "            TopDown(method='average_proportions'),\n",
    "            TopDown(method='proportion_averages'),\n",
    "            MinTrace(method='ols'),\n",
    "            MinTrace(method='wls_var'),\n",
    "            MinTrace(method='mint_shrink'),\n",
    "            ERM(method='closed'),\n",
    "        ]\n",
    "    else:\n",
    "        reconcilers = [\n",
    "            BottomUp(),\n",
    "            MinTrace(method='ols'),\n",
    "            MinTrace(method='wls_var'),\n",
    "            MinTrace(method='mint_shrink'),\n",
    "            ERM(method='closed'),\n",
    "        ]\n",
    "    \n",
    "    hrec = HierarchicalReconciliation(reconcilers=reconcilers)\n",
    "    Y_rec_df = hrec.bootstrap_reconcile(Y_hat_df=Y_hat_df,\n",
    "                                        Y_df=Y_fitted_df,\n",
    "                                        S_df=S_df, tags=tags,\n",
    "                                        level=LEVEL,\n",
    "                                        intervals_method=intervals_method,\n",
    "                                        num_samples=10, \n",
    "                                        num_seeds=10)\n",
    "    \n",
    "    Y_rec_df = Y_rec_df.merge(Y_test_df, on=['unique_id', 'ds'], how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893a7544",
   "metadata": {},
   "source": [
    "Qualitative evaluation, of parsed quantiles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46702fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_id = \"total\"\n",
    "plot_df = Y_rec_df.query(\"unique_id == @unique_id\").groupby([\"unique_id\", \"ds\"], as_index=False).mean()\n",
    "for col in hrec.level_names['AutoARIMA/BottomUp']:\n",
    "    plt.plot(plot_df[\"ds\"], plot_df[col], color=\"orange\")\n",
    "plt.plot(plot_df[\"ds\"], plot_df[\"y\"], label=\"True\")\n",
    "plt.title(f\"AutoARIMA/BottomUp - {unique_id}\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7109676b",
   "metadata": {},
   "source": [
    "## 4. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7212b919",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilsforecast.losses import scaled_crps, msse\n",
    "from hierarchicalforecast.evaluation import evaluate\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39986115",
   "metadata": {},
   "outputs": [],
   "source": [
    "with CodeTimer('Evaluate Models CRPS and MSSE ', verbose):\n",
    "    metrics_seeds = []\n",
    "    for seed in Y_rec_df.seed.unique():\n",
    "        df_seed = Y_rec_df.query(\"seed == @seed\")\n",
    "        metrics_seed = evaluate(df = df_seed,\n",
    "                            tags = tags,\n",
    "                            metrics = [scaled_crps, \n",
    "                                       partial(msse, seasonality=4)],\n",
    "                            models= hrec.level_names.keys(),\n",
    "                            level = LEVEL,\n",
    "                            train_df = Y_train_df,\n",
    "                            )\n",
    "        metrics_seed['seed'] = seed\n",
    "        metrics_seeds.append(metrics_seed)\n",
    "    metrics_seeds = pd.concat(metrics_seeds)\n",
    "\n",
    "    metrics_mean = metrics_seeds.groupby([\"level\", \"metric\"], as_index=False).mean()\n",
    "    metrics_std = metrics_seeds.groupby([\"level\", \"metric\"], as_index=False).std()\n",
    "\n",
    "    results = metrics_mean[hrec.level_names.keys()].round(3).astype(str) + \"Â±\" + metrics_std[hrec.level_names.keys()].round(4).astype(str)\n",
    "    results.insert(0, \"metric\", metrics_mean[\"metric\"])\n",
    "    results.insert(0, \"level\", metrics_mean[\"level\"])\n",
    "\n",
    "results.sort_values(by=[\"metric\", \"level\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0821f68d",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402077e0",
   "metadata": {},
   "source": [
    "- [Syama Sundar Rangapuram, Lucien D Werner, Konstantinos Benidis, Pedro Mercado, Jan Gasthaus, Tim Januschowski. (2021). \n",
    "\\\"End-to-End Learning of Coherent Probabilistic Forecasts for Hierarchical Time Series\\\". \n",
    "Proceedings of the 38th International Conference on Machine Learning (ICML).](https://proceedings.mlr.press/v139/rangapuram21a.html)\n",
    "- [Kin G. Olivares, O. Nganba Meetei, Ruijun Ma, Rohan Reddy, Mengfei Cao, Lee Dicker (2022). \n",
    "\"Probabilistic Hierarchical Forecasting with Deep Poisson Mixtures\". \n",
    "Submitted to the International Journal Forecasting, Working paper available at arxiv.](https://arxiv.org/pdf/2110.13179.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
