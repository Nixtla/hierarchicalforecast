{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probabilistic Reconciliation Methods Comparison\n",
    "\n",
    "This notebook compares the different probabilistic reconciliation methods available in HierarchicalForecast:\n",
    "- **Normality**: Gaussian-based, parametric approach\n",
    "- **Bootstrap**: Non-parametric residual resampling\n",
    "- **PERMBU**: Empirical copula-based with rank permutation\n",
    "- **Conformal**: Distribution-free with coverage guarantees under exchangeability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies if needed\n",
    "# !pip install hierarchicalforecast statsforecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from statsforecast.models import AutoARIMA\n",
    "from statsforecast.core import StatsForecast\n",
    "\n",
    "from hierarchicalforecast.utils import aggregate\n",
    "from hierarchicalforecast.core import HierarchicalReconciliation\n",
    "from hierarchicalforecast.methods import BottomUp, MinTrace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Data\n",
    "\n",
    "We use the Australian Tourism dataset for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tourism data\n",
    "Y_df = pd.read_csv('https://raw.githubusercontent.com/Nixtla/transfer-learning-time-series/main/datasets/tourism.csv')\n",
    "Y_df = Y_df.rename({'Trips': 'y', 'Quarter': 'ds'}, axis=1)\n",
    "Y_df = Y_df[['Country', 'Region', 'State', 'Purpose', 'ds', 'y']]\n",
    "Y_df['ds'] = Y_df['ds'].str.replace(r'(\\d+) (Q\\d)', r'\\1-\\2', regex=True)\n",
    "Y_df['ds'] = pd.PeriodIndex(Y_df[\"ds\"], freq='Q').to_timestamp()\n",
    "Y_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hierarchical structure\n",
    "spec = [\n",
    "    ['Country'],\n",
    "    ['Country', 'State'],\n",
    "    ['Country', 'State', 'Region'],\n",
    "]\n",
    "\n",
    "Y_df, S_df, tags = aggregate(df=Y_df, spec=spec)\n",
    "print(f\"Hierarchy has {S_df.shape[0]} series ({S_df.shape[1]} bottom-level)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split\n",
    "Y_test_df = Y_df.groupby('unique_id').tail(8)\n",
    "Y_train_df = Y_df.drop(Y_test_df.index)\n",
    "\n",
    "print(f\"Training: {len(Y_train_df)} observations\")\n",
    "print(f\"Testing: {len(Y_test_df)} observations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Compute Base Forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit base forecaster\n",
    "fcst = StatsForecast(\n",
    "    models=[AutoARIMA(season_length=4)],\n",
    "    freq='QE',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Get forecasts and fitted values for probabilistic methods\n",
    "Y_hat_df = fcst.forecast(df=Y_train_df, h=8, fitted=True)\n",
    "Y_fitted_df = fcst.forecast_fitted_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Probabilistic Reconciliation Methods\n",
    "\n",
    "### Method Comparison Table\n",
    "\n",
    "| Method | Distributional Assumptions | Residual Usage | Reconciliation Timing | Coverage Guarantee | Hierarchy Requirement |\n",
    "|--------|---------------------------|----------------|----------------------|-------------------|----------------------|\n",
    "| **Normality** | Gaussian errors | Computes covariance from residuals | N/A (analytical) | Asymptotic | Any |\n",
    "| **Bootstrap** | None | Block resamples raw residuals, then reconciles | After perturbation | Asymptotic | Any |\n",
    "| **PERMBU** | None (uses empirical marginals) | Rank permutations for copula | Bottom-up aggregation | Asymptotic | Strictly hierarchical only |\n",
    "| **Conformal** | Exchangeability | Scores from reconciled residuals | Before perturbation | Finite-sample: $(1-\\alpha) \\cdot n/(n+1)$ | Any |\n",
    "\n",
    "### Key Technical Differences\n",
    "\n",
    "**Bootstrap vs Conformal residual handling:**\n",
    "- **Bootstrap**: Adds residual blocks to *raw* forecasts, then applies reconciliation (`SP @ (y_hat + residuals)`)\n",
    "- **Conformal**: Computes scores from *reconciled* forecasts, adds to reconciled predictions (`y_rec + scores`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconcile with different probabilistic methods\n",
    "reconcilers = [\n",
    "    MinTrace(method='ols'),\n",
    "]\n",
    "\n",
    "hrec = HierarchicalReconciliation(reconcilers=reconcilers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normality-based intervals\n",
    "Y_rec_normality = hrec.reconcile(\n",
    "    Y_hat_df=Y_hat_df,\n",
    "    Y_df=Y_fitted_df,\n",
    "    S_df=S_df,\n",
    "    tags=tags,\n",
    "    level=[90],\n",
    "    intervals_method='normality'\n",
    ")\n",
    "print(\"Normality reconciliation complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstrap-based intervals\n",
    "Y_rec_bootstrap = hrec.reconcile(\n",
    "    Y_hat_df=Y_hat_df,\n",
    "    Y_df=Y_fitted_df,\n",
    "    S_df=S_df,\n",
    "    tags=tags,\n",
    "    level=[90],\n",
    "    intervals_method='bootstrap'\n",
    ")\n",
    "print(\"Bootstrap reconciliation complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conformal-based intervals\n",
    "Y_rec_conformal = hrec.reconcile(\n",
    "    Y_hat_df=Y_hat_df,\n",
    "    Y_df=Y_fitted_df,\n",
    "    S_df=S_df,\n",
    "    tags=tags,\n",
    "    level=[90],\n",
    "    intervals_method='conformal'\n",
    ")\n",
    "print(\"Conformal reconciliation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize Prediction Intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_intervals(series_id, ax, rec_df, method_name, color):\n",
    "    \"\"\"Helper to plot prediction intervals for a series.\"\"\"\n",
    "    df = rec_df[rec_df['unique_id'] == series_id].copy()\n",
    "    \n",
    "    col_mean = 'AutoARIMA/MinTrace_method-ols'\n",
    "    col_lo = f'{col_mean}-lo-90'\n",
    "    col_hi = f'{col_mean}-hi-90'\n",
    "    \n",
    "    ax.fill_between(df['ds'], df[col_lo], df[col_hi], alpha=0.3, color=color, label=f'{method_name} 90% PI')\n",
    "    ax.plot(df['ds'], df[col_mean], color=color, linewidth=2)\n",
    "\n",
    "# Plot comparison for top-level series\n",
    "series_id = 'Australia'\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "plot_intervals(series_id, axes[0], Y_rec_normality, 'Normality', 'blue')\n",
    "axes[0].set_title('Normality')\n",
    "axes[0].legend()\n",
    "\n",
    "plot_intervals(series_id, axes[1], Y_rec_bootstrap, 'Bootstrap', 'green')\n",
    "axes[1].set_title('Bootstrap')\n",
    "axes[1].legend()\n",
    "\n",
    "plot_intervals(series_id, axes[2], Y_rec_conformal, 'Conformal', 'orange')\n",
    "axes[2].set_title('Conformal')\n",
    "axes[2].legend()\n",
    "\n",
    "fig.suptitle(f'Prediction Intervals Comparison: {series_id}')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Method Pros and Cons\n",
    "\n",
    "### Normality\n",
    "**Pros:**\n",
    "- Fast computation using closed-form solutions\n",
    "- Well-understood theoretical properties\n",
    "- Works with any reconciliation method and hierarchy structure\n",
    "- Supports different covariance estimators (`ols`, `wls_var`, `mint_shrink`, etc.)\n",
    "\n",
    "**Cons:**\n",
    "- Assumes Gaussian distribution of errors\n",
    "- May underestimate uncertainty for heavy-tailed or skewed distributions\n",
    "- Covariance estimation can be unstable with limited data\n",
    "\n",
    "---\n",
    "\n",
    "### Bootstrap\n",
    "**Pros:**\n",
    "- Non-parametric: no distributional assumptions required\n",
    "- Captures empirical error distribution shape (skewness, heavy tails)\n",
    "- Preserves temporal correlation through block resampling\n",
    "- Works with any hierarchy structure\n",
    "\n",
    "**Cons:**\n",
    "- Requires sufficient historical residuals (at least horizon + some buffer)\n",
    "- Computationally more expensive than Normality\n",
    "- Coverage is asymptotic (no finite-sample guarantees)\n",
    "\n",
    "---\n",
    "\n",
    "### PERMBU\n",
    "**Pros:**\n",
    "- Preserves empirical dependencies using copula-based approach\n",
    "- Respects marginal distributions at each level\n",
    "- Captures complex cross-series dependencies\n",
    "\n",
    "**Cons:**\n",
    "- **Only works with strictly hierarchical structures** (not grouped hierarchies)\n",
    "- Computationally intensive for large hierarchies\n",
    "- Requires careful handling of the permutation ordering\n",
    "\n",
    "---\n",
    "\n",
    "### Conformal\n",
    "**Pros:**\n",
    "- Distribution-free: no parametric assumptions required\n",
    "- Finite-sample coverage guarantee under exchangeability\n",
    "- Simple interpretation: intervals based on empirical quantiles of scores\n",
    "- Works with any hierarchy structure\n",
    "\n",
    "**Cons:**\n",
    "- **Requires proper calibration set**: Coverage guarantees assume the calibration data is independent from training data. When using in-sample residuals (fitted values from the same data used to train the model), this assumption is violated, potentially leading to overly optimistic (narrow) intervals\n",
    "- Exchangeability assumption may not hold for time series with trends or structural breaks\n",
    "- Coverage guarantees are marginal (per-series), not simultaneous across the hierarchy\n",
    "- May produce wider intervals than well-specified parametric methods\n",
    "\n",
    "**⚠️ Important Caveat for Conformal:**\n",
    "In this example (and the default HierarchicalForecast API), we use in-sample fitted values as the calibration set. This is convenient but technically violates the split conformal prediction framework. For rigorous coverage guarantees, you should use a held-out validation set that was not used for model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Recommendations\n",
    "\n",
    "| Scenario | Recommended Method | Notes |\n",
    "|----------|-------------------|-------|\n",
    "| Quick analysis, errors appear Gaussian | **Normality** | Fastest, well-understood |\n",
    "| Unknown error distribution, general use | **Bootstrap** | Safe default, no assumptions |\n",
    "| Strict hierarchies, need correlation preservation | **PERMBU** | Best for capturing dependencies |\n",
    "| Have proper held-out calibration set | **Conformal** | Valid finite-sample guarantees |\n",
    "| Using in-sample residuals, want simplicity | **Bootstrap** or **Normality** | Conformal caveats apply with in-sample data |\n",
    "\n",
    "### Decision Flowchart\n",
    "\n",
    "1. **Is your hierarchy strictly hierarchical (no cross-classifications)?**\n",
    "   - Yes → Consider PERMBU if you need correlation preservation\n",
    "   - No → Use Normality, Bootstrap, or Conformal\n",
    "\n",
    "2. **Do you have a proper held-out calibration set?**\n",
    "   - Yes → Conformal provides finite-sample coverage guarantees\n",
    "   - No (using in-sample) → Bootstrap or Normality are more appropriate\n",
    "\n",
    "3. **Do your residuals appear Gaussian?**\n",
    "   - Yes → Normality is fast and efficient\n",
    "   - No / Unknown → Bootstrap adapts to any distribution"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
