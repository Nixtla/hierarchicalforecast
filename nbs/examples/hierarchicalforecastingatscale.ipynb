{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Forecasting at Scale\n",
    "\n",
    "> Practical tips to improve performance when reconciling large hierarchies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with hierarchies containing thousands or millions of time series, default settings can lead to slow reconciliation and high memory usage. This guide covers concrete steps to improve performance.\n",
    "\n",
    "We will cover:\n",
    "1. Using Polars instead of Pandas\n",
    "2. Telling the library your data is balanced\n",
    "3. Using sparse reconciliation methods\n",
    "4. Choosing the right reconciliation method for your scale\n",
    "5. Parallelizing non-negative reconciliation\n",
    "6. Avoiding unnecessary computation\n",
    "7. Profiling your pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install hierarchicalforecast statsforecast datasetsforecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "\n",
    "from datasetsforecast.hierarchical import HierarchicalData, HierarchicalInfo\n",
    "\n",
    "from statsforecast.core import StatsForecast\n",
    "from statsforecast.models import AutoARIMA, Naive\n",
    "\n",
    "from hierarchicalforecast.core import HierarchicalReconciliation\n",
    "from hierarchicalforecast.methods import (\n",
    "    BottomUp,\n",
    "    BottomUpSparse,\n",
    "    MinTrace,\n",
    "    MinTraceSparse,\n",
    ")\n",
    "from hierarchicalforecast.utils import CodeTimer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data\n",
    "\n",
    "We use the `TourismSmall` dataset for demonstration. The same principles apply to much larger hierarchies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_name = 'TourismSmall'\n",
    "group = HierarchicalInfo.get_group(group_name)\n",
    "Y_df, S_df, tags = HierarchicalData.load('./data', group_name)\n",
    "\n",
    "# Convert to Polars (see Tip 1 below)\n",
    "Y_df = pl.from_pandas(Y_df).with_columns(pl.col('ds').cast(pl.Date))\n",
    "S_df = pl.from_pandas(S_df.reset_index(names='unique_id'))\n",
    "\n",
    "# Train/test split\n",
    "Y_test_df = Y_df.group_by('unique_id').tail(group.horizon)\n",
    "Y_train_df = Y_df.filter(pl.col('ds') < Y_test_df['ds'].min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Base Forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst = StatsForecast(\n",
    "    models=[AutoARIMA(season_length=group.seasonality), Naive()],\n",
    "    freq='1q',\n",
    "    n_jobs=-1,\n",
    ")\n",
    "Y_hat_df = fcst.forecast(df=Y_train_df, h=group.horizon, fitted=True)\n",
    "Y_fitted_df = fcst.forecast_fitted_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Performance Tips\n",
    "\n",
    "### Tip 1: Use Polars instead of Pandas\n",
    "\n",
    "`HierarchicalForecast` supports both Pandas and Polars DataFrames transparently via [Narwhals](https://narwhals-dev.github.io/narwhals/). Polars is generally faster for the DataFrame operations used internally (sorting, joining, pivoting) and uses less memory due to Apache Arrow columnar storage.\n",
    "\n",
    "Simply pass Polars DataFrames to `reconcile()` — no code changes needed beyond the data loading step:\n",
    "\n",
    "```python\n",
    "# Instead of pandas DataFrames, convert to Polars:\n",
    "import polars as pl\n",
    "\n",
    "Y_df = pl.from_pandas(Y_df)\n",
    "S_df = pl.from_pandas(S_df)\n",
    "```\n",
    "\n",
    "If you use `aggregate()` to build your hierarchy, pass a Polars DataFrame directly:\n",
    "\n",
    "```python\n",
    "from hierarchicalforecast.utils import aggregate\n",
    "\n",
    "Y_df, S_df, tags = aggregate(df=pl.from_pandas(df), spec=[...]) \n",
    "```\n",
    "\n",
    "**Note**: The `sparse_s=True` option in `aggregate()` is currently only available for Pandas DataFrames. If you need sparse S matrices with Polars, pass a Polars DataFrame without `sparse_s` — the core reconciliation will still construct sparse matrices internally when using sparse methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tip 2: Set `is_balanced=True`\n",
    "\n",
    "If all your time series have the same number of observations (same start and end dates), set `is_balanced=True` in `reconcile()`. This skips an expensive `pivot()` operation and uses a fast `reshape` instead.\n",
    "\n",
    "```python\n",
    "Y_rec_df = hrec.reconcile(\n",
    "    Y_hat_df=Y_hat_df, S_df=S_df, tags=tags,\n",
    "    is_balanced=True,  # <-- skip pivot, use fast reshape\n",
    ")\n",
    "```\n",
    "\n",
    "**When can you use this?** Most hierarchical datasets are balanced — every series has observations at every time step. If you built your hierarchy with `aggregate()`, the result is always balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the speedup from is_balanced\n",
    "reconcilers = [BottomUp()]\n",
    "hrec = HierarchicalReconciliation(reconcilers=reconcilers)\n",
    "\n",
    "with CodeTimer('is_balanced=False (default)', verbose=True):\n",
    "    _ = hrec.reconcile(\n",
    "        Y_hat_df=Y_hat_df, S_df=S_df, tags=tags,\n",
    "        is_balanced=False,\n",
    "    )\n",
    "\n",
    "with CodeTimer('is_balanced=True', verbose=True):\n",
    "    _ = hrec.reconcile(\n",
    "        Y_hat_df=Y_hat_df, S_df=S_df, tags=tags,\n",
    "        is_balanced=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tip 3: Use Sparse Reconciliation Methods\n",
    "\n",
    "For large hierarchies, the dense summing matrix **S** (shape: `n_hiers × n_bottom`) can consume significant memory and slow down matrix operations. Sparse methods use `scipy.sparse` matrices and iterative solvers instead of dense linear algebra.\n",
    "\n",
    "The library provides sparse variants of the main methods:\n",
    "\n",
    "| Dense Method | Sparse Variant | Notes |\n",
    "|---|---|---|\n",
    "| `BottomUp()` | `BottomUpSparse()` | Drop-in replacement |\n",
    "| `TopDown(...)` | `TopDownSparse(...)` | Strictly hierarchical data only |\n",
    "| `MiddleOut(...)` | `MiddleOutSparse(...)` | Strictly hierarchical data only |\n",
    "| `MinTrace(method='ols')` | `MinTraceSparse(method='ols')` | Uses iterative `bicgstab` solver |\n",
    "| `MinTrace(method='wls_struct')` | `MinTraceSparse(method='wls_struct')` | Uses iterative `bicgstab` solver |\n",
    "| `MinTrace(method='wls_var')` | `MinTraceSparse(method='wls_var')` | Uses iterative `bicgstab` solver |\n",
    "\n",
    "`MinTraceSparse` constructs a `scipy.sparse.linalg.LinearOperator` for the projection matrix **P** and solves the system iteratively with `bicgstab`, avoiding materialization of the full dense P matrix.\n",
    "\n",
    "**When to use sparse methods**: When your hierarchy has thousands of bottom-level series or more. For small hierarchies (< 500 series), the overhead of sparse operations may negate the savings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare dense vs sparse methods\n",
    "dense_reconcilers = [\n",
    "    BottomUp(),\n",
    "    MinTrace(method='ols'),\n",
    "    MinTrace(method='wls_struct'),\n",
    "]\n",
    "sparse_reconcilers = [\n",
    "    BottomUpSparse(),\n",
    "    MinTraceSparse(method='ols'),\n",
    "    MinTraceSparse(method='wls_struct'),\n",
    "]\n",
    "\n",
    "hrec_dense = HierarchicalReconciliation(reconcilers=dense_reconcilers)\n",
    "hrec_sparse = HierarchicalReconciliation(reconcilers=sparse_reconcilers)\n",
    "\n",
    "with CodeTimer('Dense methods', verbose=True):\n",
    "    Y_rec_dense = hrec_dense.reconcile(\n",
    "        Y_hat_df=Y_hat_df, S_df=S_df, tags=tags, is_balanced=True,\n",
    "    )\n",
    "\n",
    "with CodeTimer('Sparse methods', verbose=True):\n",
    "    Y_rec_sparse = hrec_sparse.reconcile(\n",
    "        Y_hat_df=Y_hat_df, S_df=S_df, tags=tags, is_balanced=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tip 4: Choose the Right Reconciliation Method\n",
    "\n",
    "Reconciliation methods vary significantly in computational cost. Here is a rough ranking from fastest to slowest:\n",
    "\n",
    "| Speed | Method | Requires insample data? | Notes |\n",
    "|---|---|---|---|\n",
    "| Fastest | `BottomUp` / `BottomUpSparse` | No | Simple aggregation, no optimization |\n",
    "| Fast | `TopDown` / `TopDownSparse` | Depends on variant | Strictly hierarchical only |\n",
    "| Fast | `MinTrace(method='ols')` | No | Closed-form, `np.linalg.solve` |\n",
    "| Fast | `MinTrace(method='wls_struct')` | No | Closed-form, weighted by hierarchy structure |\n",
    "| Medium | `MinTraceSparse(method='ols'/'wls_struct')` | No | Iterative solver; better for very large S |\n",
    "| Medium | `MinTrace(method='wls_var')` | Yes | Diagonal covariance from residuals |\n",
    "| Slow | `MinTrace(method='mint_shrink')` | Yes | Full covariance with shrinkage (Numba-accelerated) |\n",
    "| Slow | `MinTrace(method='mint_cov')` | Yes | Full empirical covariance (Numba-accelerated) |\n",
    "| Slow | `ERM(method='closed')` | Yes | Pseudoinverse on large matrices |\n",
    "| Slowest | `ERM(method='reg')` | Yes | Lasso coordinate descent (iterative) |\n",
    "| +Cost | Any method with `nonnegative=True` | — | Adds a QP solve per horizon step |\n",
    "\n",
    "**Key takeaways**:\n",
    "- `BottomUp` and `MinTrace(method='ols')` are the cheapest and don't require insample data (`Y_df`).\n",
    "- Methods requiring insample residuals (`wls_var`, `mint_shrink`, `mint_cov`) need the `Y_df` argument passed to `reconcile()`. Omitting `Y_df` when not needed saves computation.\n",
    "- For very large hierarchies, prefer `MinTraceSparse` over `MinTrace` to avoid dense matrix operations.\n",
    "- `mint_shrink` and `mint_cov` compute a full n×n covariance matrix — this scales quadratically with the number of series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tip 5: Parallelize Non-Negative Reconciliation\n",
    "\n",
    "When using `nonnegative=True`, a quadratic programming (QP) problem is solved for each forecast horizon step. Use `num_threads` to parallelize these independent QP calls:\n",
    "\n",
    "```python\n",
    "MinTrace(method='ols', nonnegative=True, num_threads=4)\n",
    "MinTraceSparse(method='ols', nonnegative=True, num_threads=4)\n",
    "```\n",
    "\n",
    "The `num_threads` parameter controls the `ThreadPoolExecutor` pool size. Set it to the number of available CPU cores. Note that `num_threads` only takes effect when `nonnegative=True`.\n",
    "\n",
    "`MinTraceSparse` also offers a `qp` parameter (default `True`). Setting `qp=False` replaces the full QP solve with simple clipping of negative values — much faster but lower quality:\n",
    "\n",
    "```python\n",
    "# Fast non-negative approximation (clipping)\n",
    "MinTraceSparse(method='ols', nonnegative=True, qp=False)\n",
    "\n",
    "# Full QP solve with parallelism (more accurate)\n",
    "MinTraceSparse(method='ols', nonnegative=True, qp=True, num_threads=4)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tip 6: Avoid Unnecessary Computation\n",
    "\n",
    "**Skip probabilistic forecasts when not needed.** If you only need point forecasts, don't pass the `level` parameter:\n",
    "\n",
    "```python\n",
    "# Point forecasts only (fast)\n",
    "Y_rec_df = hrec.reconcile(Y_hat_df=Y_hat_df, S_df=S_df, tags=tags)\n",
    "\n",
    "# With prediction intervals (slower)\n",
    "Y_rec_df = hrec.reconcile(\n",
    "    Y_hat_df=Y_hat_df, S_df=S_df, tags=tags,\n",
    "    Y_df=Y_fitted_df,\n",
    "    level=[80, 95],\n",
    "    intervals_method='normality',  # cheapest interval method\n",
    ")\n",
    "```\n",
    "\n",
    "If you do need intervals, `intervals_method='normality'` is the cheapest option. The `'bootstrap'` and `'permbu'` methods require many re-evaluations and are significantly slower.\n",
    "\n",
    "**Skip diagnostics in production.** The `diagnostics=True` flag computes coherence checks across all forecasts — useful for debugging but adds overhead:\n",
    "\n",
    "```python\n",
    "# During development\n",
    "Y_rec_df = hrec.reconcile(..., diagnostics=True)\n",
    "\n",
    "# In production\n",
    "Y_rec_df = hrec.reconcile(..., diagnostics=False)  # default\n",
    "```\n",
    "\n",
    "**Don't pass insample data when not needed.** Methods like `BottomUp`, `TopDown(method='forecast_proportions')`, `MinTrace(method='ols')`, and `MinTrace(method='wls_struct')` don't use insample data. Omitting `Y_df` skips the data preparation for insample values:\n",
    "\n",
    "```python\n",
    "# These methods don't need Y_df\n",
    "reconcilers = [BottomUp(), MinTrace(method='ols')]\n",
    "hrec = HierarchicalReconciliation(reconcilers=reconcilers)\n",
    "Y_rec_df = hrec.reconcile(Y_hat_df=Y_hat_df, S_df=S_df, tags=tags)\n",
    "#                         ^^^^^^ no Y_df argument needed\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tip 7: Profile Your Pipeline\n",
    "\n",
    "`HierarchicalForecast` provides built-in profiling tools to identify bottlenecks.\n",
    "\n",
    "**`execution_times` attribute**: After calling `reconcile()`, the `HierarchicalReconciliation` instance exposes a dictionary of per-method execution times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconcilers = [\n",
    "    BottomUp(),\n",
    "    MinTrace(method='ols'),\n",
    "    MinTrace(method='wls_var'),\n",
    "    MinTrace(method='mint_shrink'),\n",
    "]\n",
    "hrec = HierarchicalReconciliation(reconcilers=reconcilers)\n",
    "Y_rec_df = hrec.reconcile(\n",
    "    Y_hat_df=Y_hat_df, Y_df=Y_fitted_df,\n",
    "    S_df=S_df, tags=tags, is_balanced=True,\n",
    ")\n",
    "\n",
    "# Inspect per-method timing\n",
    "for method_name, elapsed in hrec.execution_times.items():\n",
    "    print(f'{method_name}: {elapsed:.4f}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`CodeTimer` context manager**: Use this to time any block of code in your pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with CodeTimer('Full reconciliation pipeline', verbose=True):\n",
    "    reconcilers = [MinTraceSparse(method='ols')]\n",
    "    hrec = HierarchicalReconciliation(reconcilers=reconcilers)\n",
    "    Y_rec_df = hrec.reconcile(\n",
    "        Y_hat_df=Y_hat_df, S_df=S_df, tags=tags, is_balanced=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Summary Checklist\n",
    "\n",
    "Here is a quick checklist to optimize your hierarchical forecasting pipeline:\n",
    "\n",
    "| Tip | Action | Impact |\n",
    "|-----|--------|--------|\n",
    "| Use Polars | Pass Polars DataFrames instead of Pandas | Faster DataFrame ops, lower memory |\n",
    "| `is_balanced=True` | Set when all series have equal length | Skips expensive pivot |\n",
    "| Sparse methods | Use `BottomUpSparse`, `MinTraceSparse`, etc. | Less memory, iterative solvers |\n",
    "| Right method | Start with `BottomUp` or `MinTrace(method='ols')` | Avoid unnecessary covariance computation |\n",
    "| `num_threads` | Set >1 when using `nonnegative=True` | Parallel QP solves |\n",
    "| Skip intervals | Omit `level` parameter for point forecasts | Avoids sampling/interval computation |\n",
    "| Skip `Y_df` | Omit when reconciler doesn't need insample data | Skips data preparation |\n",
    "| Profile | Check `hrec.execution_times` and use `CodeTimer` | Find bottlenecks |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "- [Wickramasuriya, S. L., Athanasopoulos, G., & Hyndman, R. J. (2019). \"Optimal forecast reconciliation for hierarchical and grouped time series through trace minimization\". Journal of the American Statistical Association, 114, 804-819.](https://robjhyndman.com/publications/mint/)\n",
    "- [Wickramasuriya, S.L., Turlach, B.A. & Hyndman, R.J. (2020). \"Optimal non-negative forecast reconciliation\". Stat Comput 30, 1167-1182.](https://robjhyndman.com/publications/nnmint/)\n",
    "- [Hyndman, R.J., & Athanasopoulos, G. (2021). \"Forecasting: principles and practice, 3rd edition: Chapter 11: Forecasting hierarchical and grouped series.\". OTexts: Melbourne, Australia.](https://otexts.com/fpp3/hierarchical.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 2,
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
