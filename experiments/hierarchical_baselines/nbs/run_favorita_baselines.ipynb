{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Favorita Hierarchical Baselines\n",
    "\n",
    "This notebooks runs and saves the forecasts of hierarchical statistical baseline methods.\n",
    "\n",
    "- It reads a preprocessed Favorita dataset as defined in [datasetsforecast.favorita](https://github.com/Nixtla/datasetsforecast/blob/feat/favorita_dataset/nbs/favorita.ipynb).\n",
    "- It filters the dataset by `item_id`.\n",
    "- It fits base forecasts using StatsForecast's `AutoARIMA`.\n",
    "- It reconciles the geographic aggregation levels using HierarchicalForecast's baselines, (10 bootstrap seeds).\n",
    "- It saves the reconciled forecasts, test, and train subsets for each `item_id` into its respective folder `./data/Favorita200/item_id/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# !pip install statsforecast\n",
    "# !pip install hierarchicalforecast\n",
    "# !pip install git+https://github.com/Nixtla/datasetsforecast.git@feat/favorita_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kin/anaconda3/envs/hierarchical_baselines/lib/python3.10/site-packages/statsforecast/core.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from statsforecast.core import StatsForecast\n",
    "from statsforecast.models import AutoARIMA, Naive\n",
    "\n",
    "from hierarchicalforecast.core import HierarchicalReconciliation\n",
    "from hierarchicalforecast.evaluation import HierarchicalEvaluation\n",
    "from hierarchicalforecast.methods import BottomUp, TopDown, MinTrace, ERM\n",
    "\n",
    "from hierarchicalforecast.utils import is_strictly_hierarchical\n",
    "from hierarchicalforecast.utils import HierarchicalPlot, CodeTimer\n",
    "from hierarchicalforecast.evaluation import scaled_crps, msse, energy_score\n",
    "\n",
    "from datasetsforecast.favorita import FavoritaData, FavoritaInfo\n",
    "\n",
    "import warnings\n",
    "# Avoid pandas fragmentation warning and positive definite warning\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FavoritaHierarchicalDataset(object):\n",
    "    # Class with loading, processing and\n",
    "    # prediction evaluation methods for hierarchical data\n",
    "\n",
    "    available_datasets = ['Favorita200', 'Favorita500', 'FavoritaComplete']\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_hierarchical_scrps(hier_idxs, Y, Yq_hat, q_to_pred):\n",
    "        # We use the indexes obtained from the aggregation tags\n",
    "        # to compute scaled CRPS across the hierarchy levels\n",
    "        # # [n_items, n_stores, n_time, n_quants] \n",
    "        scrps_list = []\n",
    "        for idxs in hier_idxs:\n",
    "            y      = Y[:, idxs, :]\n",
    "            yq_hat = Yq_hat[:, idxs, :, :]\n",
    "            scrps  = scaled_crps(y, yq_hat, q_to_pred)\n",
    "            scrps_list.append(scrps)\n",
    "        return scrps_list\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_hierarchical_msse(hier_idxs, Y, Y_hat, Y_train):\n",
    "        # We use the indexes obtained from the aggregation tags\n",
    "        # to compute scaled CRPS across the hierarchy levels         \n",
    "        msse_list = []\n",
    "        for idxs in hier_idxs:\n",
    "            y       = Y[:, idxs, :]\n",
    "            y_hat   = Y_hat[:, idxs, :]\n",
    "            y_train = Y_train[:, idxs, :]\n",
    "            crps    = msse(y, y_hat, y_train)\n",
    "            msse_list.append(crps)\n",
    "        return msse_list    \n",
    "\n",
    "    @staticmethod\n",
    "    def _sort_hier_df(Y_df, S_df):\n",
    "        # NeuralForecast core, sorts unique_id lexicographically\n",
    "        # deviating from S_df, this class matches S_df and Y_hat_df order.\n",
    "        Y_df.unique_id = Y_df.unique_id.astype('category')\n",
    "        Y_df.unique_id = Y_df.unique_id.cat.set_categories(S_df.index)\n",
    "        Y_df = Y_df.sort_values(by=['unique_id', 'ds'])\n",
    "        return Y_df\n",
    "\n",
    "    @staticmethod\n",
    "    def _nonzero_indexes_by_row(M):\n",
    "        return [np.nonzero(M[row,:])[0] for row in range(len(M))]\n",
    "\n",
    "    @staticmethod\n",
    "    def load_item_data(item_id, dataset='Favorita200', directory='./data'):\n",
    "        # Load data\n",
    "        data_info = FavoritaInfo[dataset]\n",
    "        Y_df, S_df, tags = FavoritaData.load(directory=directory,\n",
    "                                             group=dataset)\n",
    "\n",
    "        # Parse and augment data\n",
    "        # + hack geographic hier_id to treat it as unique_id\n",
    "        Y_df['ds'] = pd.to_datetime(Y_df['ds'])\n",
    "        Y_df = Y_df[Y_df.item_id==item_id]\n",
    "        Y_df = Y_df.rename(columns={'hier_id': 'unique_id'})\n",
    "        Y_df = FavoritaHierarchicalDataset._sort_hier_df(Y_df=Y_df, S_df=S_df)\n",
    "\n",
    "        # Obtain indexes for plots and evaluation\n",
    "        hier_levels = ['Overall'] + list(tags.keys())\n",
    "        hier_idxs = [np.arange(len(S_df))] +\\\n",
    "            [S_df.index.get_indexer(tags[level]) for level in list(tags.keys())]\n",
    "        hier_linked_idxs = FavoritaHierarchicalDataset._nonzero_indexes_by_row(S_df.values.T)\n",
    "\n",
    "        # MinT along other methods require a positive definite covariance matrix\n",
    "        # for the residuals, when dealing with 0s as residuals the methods break\n",
    "        # data is augmented with minimal normal noise to avoid this error.\n",
    "        Y_df['y'] = Y_df['y'] + np.random.normal(loc=0.0, scale=0.01, size=len(Y_df))\n",
    "\n",
    "        # Final output\n",
    "        data = dict(Y_df=Y_df, S_df=S_df, tags=tags,\n",
    "                    # Hierarchical idxs\n",
    "                    hier_idxs=hier_idxs,\n",
    "                    hier_levels=hier_levels,\n",
    "                    hier_linked_idxs=hier_linked_idxs,\n",
    "                    # Dataset Properties\n",
    "                    horizon=data_info.horizon,\n",
    "                    freq=data_info.freq,\n",
    "                    seasonality=data_info.seasonality)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 930M/930M [00:21<00:00, 43.4MiB/s] \n",
      "INFO:datasetsforecast.utils:Successfully downloaded favorita-grocery-sales-forecasting2.zip, 930159981, bytes.\n",
      "INFO:datasetsforecast.utils:Decompressing zip file...\n",
      "INFO:datasetsforecast.utils:Successfully decompressed data/favorita/favorita-grocery-sales-forecasting2.zip\n",
      "INFO:datasetsforecast.utils:Decompressing zip file...\n",
      "INFO:datasetsforecast.utils:Successfully decompressed data/favorita/holidays_events.csv.zip\n",
      "INFO:datasetsforecast.utils:Decompressing zip file...\n",
      "INFO:datasetsforecast.utils:Successfully decompressed data/favorita/items.csv.zip\n",
      "INFO:datasetsforecast.utils:Decompressing zip file...\n",
      "INFO:datasetsforecast.utils:Successfully decompressed data/favorita/oil.csv.zip\n",
      "INFO:datasetsforecast.utils:Decompressing zip file...\n",
      "INFO:datasetsforecast.utils:Successfully decompressed data/favorita/sample_submission.csv.zip\n",
      "INFO:datasetsforecast.utils:Decompressing zip file...\n",
      "INFO:datasetsforecast.utils:Successfully decompressed data/favorita/stores.csv.zip\n",
      "INFO:datasetsforecast.utils:Decompressing zip file...\n",
      "INFO:datasetsforecast.utils:Successfully decompressed data/favorita/test.csv.zip\n",
      "INFO:datasetsforecast.utils:Decompressing zip file...\n",
      "INFO:datasetsforecast.utils:Successfully decompressed data/favorita/train.csv.zip\n",
      "INFO:datasetsforecast.utils:Decompressing zip file...\n",
      "INFO:datasetsforecast.utils:Successfully decompressed data/favorita/transactions.csv.zip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved train.csv to train.feather for fast access\n"
     ]
    }
   ],
   "source": [
    "data = FavoritaHierarchicalDataset.load_item_data(item_id=1916577, directory = './data/favorita', dataset='Favorita200')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_baselines(intervals_method, item_id=1916577, dataset='Favorita200', verbose=False, seed=0):\n",
    "    with CodeTimer('Read and Parse data   ', verbose):\n",
    "        data = FavoritaHierarchicalDataset.load_item_data(item_id=item_id,\n",
    "                                                     dataset=dataset, directory = './data/favorita')\n",
    "        Y_df = data['Y_df'][[\"unique_id\", 'ds', 'y']]\n",
    "        S_df, tags = data['S_df'], data['tags']\n",
    "        horizon = data['horizon']\n",
    "        seasonality = data['seasonality']\n",
    "        freq = data['freq']\n",
    "\n",
    "        # Train/Test Splits\n",
    "        Y_test_df  = Y_df.groupby('unique_id').tail(horizon)\n",
    "        Y_train_df = Y_df.drop(Y_test_df.index)\n",
    "        Y_test_df  = Y_test_df.set_index('unique_id')\n",
    "        Y_train_df = Y_train_df.set_index('unique_id')\n",
    "\n",
    "        dataset_str = f'{dataset} item_id={item_id}, h={horizon} '\n",
    "        dataset_str += f'n_series={len(S_df)}, n_bottom={len(S_df.columns)} \\n'\n",
    "        dataset_str += f'test ds=[{min(Y_test_df.ds), max(Y_test_df.ds)}] '\n",
    "        print(dataset_str)\n",
    "\n",
    "    with CodeTimer('Fit/Predict Model\t  ', verbose):\n",
    "        # Read to avoid unnecesary AutoARIMA computation\n",
    "        item_path = f'./data/{dataset}/{item_id}'\n",
    "        os.makedirs(item_path, exist_ok=True)\n",
    "        yhat_file = f'{item_path}/Y_hat.csv'\n",
    "        yfitted_file = f'{item_path}/Y_fitted.csv'\n",
    "        yrec_file = f'{item_path}/{intervals_method}_rec.csv'\n",
    "\n",
    "        if os.path.exists(yhat_file):\n",
    "            Y_hat_df = pd.read_csv(yhat_file)\n",
    "            Y_fitted_df = pd.read_csv(yfitted_file)\n",
    "\n",
    "        else:\n",
    "            if not os.path.exists(f'./data/{dataset}'):\n",
    "                os.makedirs(f'./data/{dataset}')\t\t\t\n",
    "            fcst = StatsForecast(\n",
    "                df=Y_train_df, \n",
    "                models=[AutoARIMA(season_length=seasonality)],\n",
    "                fallback_model=[Naive()],\n",
    "                freq=freq, \n",
    "                n_jobs=-1\n",
    "            )\n",
    "            Y_hat_df = fcst.forecast(h=horizon, fitted=True, level=LEVEL)\n",
    "            Y_fitted_df = fcst.forecast_fitted_values()\n",
    "\n",
    "            Y_hat_df = Y_hat_df.reset_index()\n",
    "            Y_fitted_df = Y_fitted_df.reset_index()\n",
    "            Y_hat_df.to_csv(yhat_file, index=False)\n",
    "            Y_fitted_df.to_csv(yfitted_file, index=False)\n",
    "\n",
    "        Y_hat_df = Y_hat_df.set_index('unique_id')\n",
    "        Y_fitted_df = Y_fitted_df.set_index('unique_id')\n",
    "\n",
    "    with CodeTimer('Reconcile Predictions ', verbose):\n",
    "        if is_strictly_hierarchical(S=S_df.values.astype(np.float32), \n",
    "            tags={key: S_df.index.get_indexer(val) for key, val in tags.items()}):\n",
    "            reconcilers = [\n",
    "                BottomUp(),\n",
    "                TopDown(method='average_proportions'),\n",
    "                TopDown(method='proportion_averages'),\n",
    "                MinTrace(method='ols'),\n",
    "                MinTrace(method='mint_shrink', mint_shr_ridge=1e-6),\n",
    "                #ERM(method='reg_bu', lambda_reg=100) # Extremely inneficient\n",
    "                ERM(method='closed')\n",
    "            ]\n",
    "        else:\n",
    "            reconcilers = [\n",
    "                BottomUp(),\n",
    "                MinTrace(method='ols'),\n",
    "                MinTrace(method='mint_shrink', mint_shr_ridge=1e-6),\n",
    "                #ERM(method='reg_bu', lambda_reg=100) # Extremely inneficient\n",
    "                ERM(method='closed')\n",
    "            ]\n",
    "        \n",
    "        hrec = HierarchicalReconciliation(reconcilers=reconcilers)\n",
    "        Y_rec_df = hrec.bootstrap_reconcile(Y_hat_df=Y_hat_df,\n",
    "                                            Y_df=Y_fitted_df,\n",
    "                                            S_df=S_df, tags=tags,\n",
    "                                            level=LEVEL,\n",
    "                                            intervals_method=intervals_method,\n",
    "                                            num_samples=10, num_seeds=10)\n",
    "\n",
    "        # Matching Y_test/Y_rec/S index ordering\n",
    "        Y_test_df = Y_test_df.reset_index()\n",
    "        Y_test_df.unique_id = Y_test_df.unique_id.astype('category')\n",
    "        Y_test_df.unique_id = Y_test_df.unique_id.cat.set_categories(S_df.index)\n",
    "        Y_test_df = Y_test_df.sort_values(by=['unique_id', 'ds'])\n",
    "\n",
    "        Y_rec_df = Y_rec_df.reset_index()\n",
    "        Y_rec_df.unique_id = Y_rec_df.unique_id.astype('category')\n",
    "        Y_rec_df.unique_id = Y_rec_df.unique_id.cat.set_categories(S_df.index)\n",
    "        Y_rec_df = Y_rec_df.sort_values(by=['seed', 'unique_id', 'ds'])\n",
    "\n",
    "        Y_rec_df.to_csv(yrec_file, index=False)\n",
    "\n",
    "        return Y_rec_df, Y_test_df, Y_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "BOOTSTRAP Favorita200 statistical baselines evaluation \n",
      "\n",
      "Favorita200 item_id=112830, h=34 n_series=93, n_bottom=54 \n",
      "test ds=[(Timestamp('2017-07-13 00:00:00'), Timestamp('2017-08-15 00:00:00'))] \n",
      "Code block 'Read and Parse data   ' took:\t0.60365 seconds\n",
      "Code block 'Fit/Predict Model\t  ' took:\t30.04792 seconds\n",
      "Code block 'Reconcile Predictions ' took:\t38.54011 seconds\n",
      "\n",
      "\n",
      "Favorita200 item_id=1916577, h=34 n_series=93, n_bottom=54 \n",
      "test ds=[(Timestamp('2017-07-13 00:00:00'), Timestamp('2017-08-15 00:00:00'))] \n",
      "Code block 'Read and Parse data   ' took:\t0.61150 seconds\n",
      "Code block 'Fit/Predict Model\t  ' took:\t29.67945 seconds\n",
      "Code block 'Reconcile Predictions ' took:\t38.78811 seconds\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parse execution parameters\n",
    "verbose = True\n",
    "intervals_method = 'bootstrap'\n",
    "dataset = 'Favorita200'\n",
    "\n",
    "assert intervals_method in ['bootstrap', 'normality', 'permbu'], \\\n",
    "    \"Select `--intervals_method` from ['bootstrap', 'normality', 'permbu']\"\n",
    "\n",
    "available_datasets = ['Favorita200', 'Favorita500', 'FavoritaComplete']\n",
    "assert dataset in available_datasets, \\\n",
    "    \"Select `--dataset` from ['Favorita200', 'Favorita500', 'FavoritaComplete']\"\n",
    "\n",
    "LEVEL = np.arange(0, 100, 2)\n",
    "qs = [[50-lv/2, 50+lv/2] for lv in LEVEL]\n",
    "QUANTILES = np.sort(np.concatenate(qs)/100)\n",
    "\n",
    "# Run experiments\n",
    "Y_all_df, S_df, tags = FavoritaData.load(directory='./data/favorita/', group='Favorita200')\n",
    "items = Y_all_df.item_id.unique()\n",
    "\n",
    "print('\\n')\n",
    "print(f'{intervals_method.upper()} {dataset} statistical baselines evaluation \\n')\n",
    "#for item_id in items:\n",
    "for item_id in [112830, 1916577]:\n",
    "    Y_rec_df, Y_test_df, Y_train_df = run_baselines(item_id=item_id, dataset=dataset,\n",
    "                                                    intervals_method=intervals_method,\n",
    "                                                    verbose=verbose)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hierarchicalforecast",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
