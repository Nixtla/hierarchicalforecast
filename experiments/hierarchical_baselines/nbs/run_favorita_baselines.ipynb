{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Favorita Hierarchical Baselines\n",
    "\n",
    "This notebooks runs and saves the forecasts of hierarchical statistical baseline methods.\n",
    "\n",
    "- It reads a preprocessed Favorita dataset as defined in [datasetsforecast.favorita](https://github.com/Nixtla/datasetsforecast/blob/feat/favorita_dataset/nbs/favorita.ipynb).\n",
    "- It filters the dataset by `item_id`.\n",
    "- It fits base forecasts using StatsForecast's `AutoARIMA`.\n",
    "- It reconciles the geographic aggregation levels using HierarchicalForecast's baselines, (10 bootstrap seeds).\n",
    "- It saves the reconciled forecasts, test, and train subsets for each `item_id` into its respective folder `./data/Favorita200/item_id/`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{.callout-warning collapse=\"false\"}\n",
    "#### Large data memory requirements\n",
    "\n",
    "Note that in its current implementation, this notebook requires approximately 300G of memory.\n",
    "You can diminish the memomry usage operating on `Favorita200`, a 200 sample of the 4036 items of the `FavoritaComplete`\n",
    "dataset.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# !pip install statsforecast\n",
    "# !pip install hierarchicalforecast\n",
    "# !pip install git+https://github.com/Nixtla/datasetsforecast.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/hierarchicalforecast/lib/python3.10/site-packages/statsforecast/core.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from statsforecast.core import StatsForecast\n",
    "from statsforecast.models import AutoARIMA, Naive\n",
    "\n",
    "from hierarchicalforecast.core import HierarchicalReconciliation\n",
    "from hierarchicalforecast.evaluation import HierarchicalEvaluation\n",
    "from hierarchicalforecast.methods import BottomUp, TopDown, MinTrace, ERM\n",
    "\n",
    "from hierarchicalforecast.utils import is_strictly_hierarchical\n",
    "from hierarchicalforecast.utils import HierarchicalPlot, CodeTimer\n",
    "from hierarchicalforecast.evaluation import scaled_crps, msse, energy_score\n",
    "\n",
    "from datasetsforecast.favorita import FavoritaData, FavoritaInfo\n",
    "\n",
    "import warnings\n",
    "# Avoid pandas fragmentation warning and positive definite warning\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Union\n",
    "def _metric_protections(y: np.ndarray, y_hat: np.ndarray,\n",
    "                        weights: Optional[np.ndarray]) -> None:\n",
    "    if not ((weights is None) or (np.sum(weights) > 0)):\n",
    "        raise Exception('Sum of `weights` cannot be 0')\n",
    "    if not ((weights is None) or (weights.shape == y.shape)):\n",
    "        raise Exception(\n",
    "        f'Wrong weight dimension weights.shape {weights.shape}, y.shape {y.shape}')\n",
    "\n",
    "def mse(y: np.ndarray, y_hat: np.ndarray,\n",
    "        weights: Optional[np.ndarray] = None,\n",
    "        axis: Optional[int] = None) -> Union[float, np.ndarray]:\n",
    "    _metric_protections(y, y_hat, weights)\n",
    "\n",
    "    delta_y = np.square(y - y_hat)\n",
    "    if weights is not None:\n",
    "        mse = np.average(delta_y[~np.isnan(delta_y)],\n",
    "                         weights=weights[~np.isnan(delta_y)],\n",
    "                         axis=axis)\n",
    "    else:\n",
    "        mse = np.nanmean(delta_y, axis=axis)\n",
    "    return mse\n",
    "\n",
    "def rel_mse(y, y_hat, y_train, mask=None):\n",
    "    if mask is None:\n",
    "       mask = np.ones_like(y)\n",
    "    n_series, n_hier, horizon = y.shape\n",
    "\n",
    "    eps = np.finfo(float).eps\n",
    "    y_naive = np.repeat(y_train[:,:,[-1]], horizon, axis=2)\n",
    "    norm = mse(y=y, y_hat=y_naive)\n",
    "    loss = mse(y=y, y_hat=y_hat, weights=mask)\n",
    "    loss = loss / (norm + eps)\n",
    "    return loss\n",
    "\n",
    "class FavoritaHierarchicalDataset(object):\n",
    "    # Class with loading, processing and\n",
    "    # prediction evaluation methods for hierarchical data\n",
    "\n",
    "    available_datasets = ['Favorita200', 'Favorita500', 'FavoritaComplete']\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_hierarchical_scrps(hier_idxs, Y, Yq_hat, q_to_pred):\n",
    "        # We use the indexes obtained from the aggregation tags\n",
    "        # to compute scaled CRPS across the hierarchy levels\n",
    "        # # [n_items, n_stores, n_time, n_quants] \n",
    "        scrps_list = []\n",
    "        for idxs in hier_idxs:\n",
    "            y      = Y[:, idxs, :]\n",
    "            yq_hat = Yq_hat[:, idxs, :, :]\n",
    "            scrps  = scaled_crps(y, yq_hat, q_to_pred)\n",
    "            scrps_list.append(scrps)\n",
    "        return scrps_list\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_hierarchical_msse(hier_idxs, Y, Y_hat, Y_train):\n",
    "        # We use the indexes obtained from the aggregation tags\n",
    "        # to compute scaled CRPS across the hierarchy levels         \n",
    "        msse_list = []\n",
    "        for idxs in hier_idxs:\n",
    "            y       = Y[:, idxs, :]\n",
    "            y_hat   = Y_hat[:, idxs, :]\n",
    "            y_train = Y_train[:, idxs, :]\n",
    "            crps    = msse(y, y_hat, y_train)\n",
    "            msse_list.append(crps)\n",
    "        return msse_list    \n",
    "    \n",
    "    @staticmethod\n",
    "    def _get_hierarchical_rel_mse(hier_idxs, Y, Y_hat, Y_train):\n",
    "        # We use the indexes obtained from the aggregation tags\n",
    "        # to compute relative MSE across the hierarchy levels\n",
    "        rel_mse_list = []\n",
    "        for idxs in hier_idxs:\n",
    "            y       = Y[:,idxs, :]\n",
    "            y_hat   = Y_hat[:,idxs, :]\n",
    "            y_train = Y_train[:,idxs, :]\n",
    "            level_rel_mse = rel_mse(y, y_hat, y_train)\n",
    "            rel_mse_list.append(level_rel_mse)\n",
    "        return rel_mse_list\n",
    "\n",
    "    @staticmethod\n",
    "    def _sort_hier_df(Y_df, S_df):\n",
    "        # NeuralForecast core, sorts unique_id lexicographically\n",
    "        # deviating from S_df, this class matches S_df and Y_hat_df order.\n",
    "        Y_df.unique_id = Y_df.unique_id.astype('category')\n",
    "        Y_df.unique_id = Y_df.unique_id.cat.set_categories(S_df.index)\n",
    "        Y_df = Y_df.sort_values(by=['unique_id', 'ds'])\n",
    "        return Y_df\n",
    "\n",
    "    @staticmethod\n",
    "    def _nonzero_indexes_by_row(M):\n",
    "        return [np.nonzero(M[row,:])[0] for row in range(len(M))]\n",
    "\n",
    "    @staticmethod\n",
    "    def load_item_data(item_id, dataset='Favorita200', directory='./data'):\n",
    "        # Load data\n",
    "        data_info = FavoritaInfo[dataset]\n",
    "        Y_df, S_df, tags = FavoritaData.load(directory=directory,\n",
    "                                             group=dataset)\n",
    "\n",
    "        # Parse and augment data\n",
    "        # + hack geographic hier_id to treat it as unique_id\n",
    "        Y_df['ds'] = pd.to_datetime(Y_df['ds'])\n",
    "        Y_df = Y_df[Y_df.item_id==item_id]\n",
    "        Y_df = Y_df.rename(columns={'hier_id': 'unique_id'})\n",
    "        Y_df = FavoritaHierarchicalDataset._sort_hier_df(Y_df=Y_df, S_df=S_df)\n",
    "\n",
    "        # Obtain indexes for plots and evaluation\n",
    "        hier_levels = ['Overall'] + list(tags.keys())\n",
    "        hier_idxs = [np.arange(len(S_df))] +\\\n",
    "            [S_df.index.get_indexer(tags[level]) for level in list(tags.keys())]\n",
    "        hier_linked_idxs = FavoritaHierarchicalDataset._nonzero_indexes_by_row(S_df.values.T)\n",
    "\n",
    "        # MinT along other methods require a positive definite covariance matrix\n",
    "        # for the residuals, when dealing with 0s as residuals the methods break\n",
    "        # data is augmented with minimal normal noise to avoid this error.\n",
    "        Y_df['y'] = Y_df['y'] + np.random.normal(loc=0.0, scale=0.01, size=len(Y_df))\n",
    "\n",
    "        # Final output\n",
    "        data = dict(Y_df=Y_df, S_df=S_df, tags=tags,\n",
    "                    # Hierarchical idxs\n",
    "                    hier_idxs=hier_idxs,\n",
    "                    hier_levels=hier_levels,\n",
    "                    hier_linked_idxs=hier_linked_idxs,\n",
    "                    # Dataset Properties\n",
    "                    horizon=data_info.horizon,\n",
    "                    freq=data_info.freq,\n",
    "                    seasonality=data_info.seasonality)\n",
    "        return data\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_process_data(dataset='Favorita200', directory='./data'):\n",
    "        # Load data\n",
    "        data_info = FavoritaInfo[dataset]\n",
    "        Y_df, S_df, tags = FavoritaData.load(directory=directory,\n",
    "                                             group=dataset)\n",
    "\n",
    "        # Parse and augment data\n",
    "        # + hack geographic hier_id to treat it as unique_id\n",
    "        Y_df['ds'] = pd.to_datetime(Y_df['ds'])\n",
    "        #Y_df = Y_df[Y_df.item_id==item_id]\n",
    "        Y_df = Y_df.rename(columns={'hier_id': 'unique_id'})\n",
    "        Y_df = FavoritaHierarchicalDataset._sort_hier_df(Y_df=Y_df, S_df=S_df)\n",
    "\n",
    "        # Obtain indexes for plots and evaluation\n",
    "        hier_levels = ['Overall'] + list(tags.keys())\n",
    "        hier_idxs = [np.arange(len(S_df))] +\\\n",
    "            [S_df.index.get_indexer(tags[level]) for level in list(tags.keys())]\n",
    "        hier_linked_idxs = FavoritaHierarchicalDataset._nonzero_indexes_by_row(S_df.values.T)\n",
    "\n",
    "        # MinT along other methods require a positive definite covariance matrix\n",
    "        # for the residuals, when dealing with 0s as residuals the methods break\n",
    "        # data is augmented with minimal normal noise to avoid this error.\n",
    "        Y_df['y'] = Y_df['y'] + np.random.normal(loc=0.0, scale=0.01, size=len(Y_df))\n",
    "\n",
    "        # Final output\n",
    "        data = dict(Y_df=Y_df, S_df=S_df, tags=tags,\n",
    "                    # Hierarchical idxs\n",
    "                    hier_idxs=hier_idxs,\n",
    "                    hier_levels=hier_levels,\n",
    "                    hier_linked_idxs=hier_linked_idxs,\n",
    "                    # Dataset Properties\n",
    "                    horizon=data_info.horizon,\n",
    "                    freq=data_info.freq,\n",
    "                    seasonality=data_info.seasonality)\n",
    "        return data\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = FavoritaHierarchicalDataset.load_item_data(item_id=112830, directory = './data/favorita', dataset='Favorita200')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_baselines(item_id, data, intervals_method='bootstrap', dataset='Favorita200', verbose=False, seed=0):\n",
    "    with CodeTimer('Read and Parse data   ', verbose):\n",
    "        Y_df_item = data['Y_df'][data['Y_df'].item_id ==item_id]\n",
    "        Y_df = Y_df_item[[\"unique_id\", 'ds', 'y']]\n",
    "        S_df, tags = data['S_df'], data['tags']\n",
    "        horizon = data['horizon']\n",
    "        seasonality = data['seasonality']\n",
    "        freq = data['freq']\n",
    "\n",
    "        # Train/Test Splits\n",
    "        Y_test_df  = Y_df.groupby('unique_id').tail(horizon)\n",
    "        Y_train_df = Y_df.drop(Y_test_df.index)\n",
    "        Y_test_df  = Y_test_df.set_index('unique_id')\n",
    "        Y_train_df = Y_train_df.set_index('unique_id')\n",
    "\n",
    "        dataset_str = f'{dataset} item_id={item_id}, h={horizon} '\n",
    "        dataset_str += f'n_series={len(S_df)}, n_bottom={len(S_df.columns)} \\n'\n",
    "        dataset_str += f'test ds=[{min(Y_test_df.ds), max(Y_test_df.ds)}] '\n",
    "\n",
    "    with CodeTimer('Fit/Predict Model\t  ', verbose):\n",
    "        # Read to avoid unnecesary AutoARIMA computation\n",
    "        item_path = f'./data/{dataset}/{item_id}'\n",
    "        os.makedirs(item_path, exist_ok=True)\n",
    "        yhat_file = f'{item_path}/Y_hat.csv'\n",
    "        yfitted_file = f'{item_path}/Y_fitted.csv'\n",
    "        yrec_file = f'{item_path}/{intervals_method}_rec.csv'\n",
    "\n",
    "        if os.path.exists(yhat_file):\n",
    "            Y_hat_df = pd.read_csv(yhat_file)\n",
    "            Y_fitted_df = pd.read_csv(yfitted_file)\n",
    "\n",
    "        else:\n",
    "            if not os.path.exists(f'./data/{dataset}'):\n",
    "                os.makedirs(f'./data/{dataset}')\t\t\t\n",
    "            fcst = StatsForecast(\n",
    "                df=Y_train_df, \n",
    "                models=[AutoARIMA(season_length=seasonality)],\n",
    "                fallback_model=[Naive()],\n",
    "                freq=freq, \n",
    "                n_jobs=-1\n",
    "            )\n",
    "            Y_hat_df = fcst.forecast(h=horizon, fitted=True, level=LEVEL)\n",
    "            Y_fitted_df = fcst.forecast_fitted_values()\n",
    "\n",
    "            Y_hat_df = Y_hat_df.reset_index()\n",
    "            Y_fitted_df = Y_fitted_df.reset_index()\n",
    "            Y_hat_df.to_csv(yhat_file, index=False)\n",
    "            Y_fitted_df.to_csv(yfitted_file, index=False)\n",
    "\n",
    "        Y_hat_df = Y_hat_df.set_index('unique_id')\n",
    "        Y_fitted_df = Y_fitted_df.set_index('unique_id')\n",
    "\n",
    "    with CodeTimer('Reconcile Predictions ', verbose):\n",
    "        if is_strictly_hierarchical(S=S_df.values.astype(np.float32), \n",
    "            tags={key: S_df.index.get_indexer(val) for key, val in tags.items()}):\n",
    "            reconcilers = [\n",
    "#                 BottomUp(),\n",
    "#                 TopDown(method='average_proportions'),\n",
    "#                 TopDown(method='proportion_averages'),\n",
    "                 MinTrace(method='ols'),\n",
    "#                 MinTrace(method='mint_shrink', mint_shr_ridge=1e-6),\n",
    "                #ERM(method='reg_bu', lambda_reg=100) # Extremely inneficient\n",
    "                #ERM(method='closed')\n",
    "            ]\n",
    "        else:\n",
    "            reconcilers = [\n",
    "#                 BottomUp(),\n",
    "                 MinTrace(method='ols'),\n",
    "#                 MinTrace(method='mint_shrink', mint_shr_ridge=1e-6),\n",
    "                #ERM(method='reg_bu', lambda_reg=100) # Extremely inneficient\n",
    "                #ERM(method='closed')\n",
    "            ]\n",
    "        \n",
    "        hrec = HierarchicalReconciliation(reconcilers=reconcilers)\n",
    "        Y_rec_df = hrec.bootstrap_reconcile(Y_hat_df=Y_hat_df,\n",
    "                                            Y_df=Y_fitted_df,\n",
    "                                            S_df=S_df, tags=tags,\n",
    "                                            level=LEVEL,\n",
    "                                            intervals_method=intervals_method,\n",
    "                                            num_samples=10, num_seeds=10)\n",
    "\n",
    "        # Matching Y_test/Y_rec/S index ordering\n",
    "        Y_test_df = Y_test_df.reset_index()\n",
    "        Y_test_df.unique_id = Y_test_df.unique_id.astype('category')\n",
    "        Y_test_df.unique_id = Y_test_df.unique_id.cat.set_categories(S_df.index)\n",
    "        Y_test_df = Y_test_df.sort_values(by=['unique_id', 'ds'])\n",
    "\n",
    "        Y_rec_df = Y_rec_df.reset_index()\n",
    "        Y_rec_df.unique_id = Y_rec_df.unique_id.astype('category')\n",
    "        Y_rec_df.unique_id = Y_rec_df.unique_id.cat.set_categories(S_df.index)\n",
    "        Y_rec_df = Y_rec_df.sort_values(by=['seed', 'unique_id', 'ds'])\n",
    "\n",
    "        #Y_rec_df.to_csv(yrec_file, index=False)\n",
    "\n",
    "        \n",
    "        # Parsing model level columns\n",
    "        flat_cols = list(hrec.level_names.keys())\n",
    "        for model in hrec.level_names:\n",
    "            flat_cols += hrec.level_names[model]\n",
    "        for model in hrec.sample_names:\n",
    "            flat_cols += hrec.sample_names[model]\n",
    "        y_rec  = Y_rec_df[flat_cols]\n",
    "        model_columns = y_rec.columns\n",
    "        model = list(hrec.level_names.keys())[0]\n",
    "        col_idx = model_columns.get_indexer(hrec.level_names[model])\n",
    "        col_idx_mean = model_columns.get_loc(model)\n",
    "        n_series = len(S_df)\n",
    "        n_seeds = len(Y_rec_df.seed.unique())\n",
    "        y_rec  = y_rec.values.reshape(n_seeds, n_series, horizon, len(model_columns))\n",
    "        y_rec_model = y_rec[:,:,:,col_idx]\n",
    "        y_rec_mean = y_rec[:,:,:,col_idx_mean]\n",
    "        y_test = Y_test_df['y'].values.reshape(n_series, horizon)\n",
    "        y_train = Y_train_df['y'].values.reshape(n_series, -1)\n",
    "        \n",
    "        \n",
    "#         # [n_items, n_samples, horizon, n_hier] 0,1,2,3\n",
    "#         # -> [n_items, n_hier, horizon, n_samples] 0,3,2,1\n",
    "#         samples = np.concatenate(samples_list, axis=0)\n",
    "#         samples = np.transpose(samples, (0,3,2,1))\n",
    "#         #col_idx = model_columns.get_loc(model)\n",
    "#         Y_hat  = np.mean(samples, axis=3)\n",
    "#         Yq_hat = np.quantile(samples, q=QUANTILES, axis=3)\n",
    "#         Yq_hat = np.transpose(Yq_hat, (1,2,3,0))\n",
    "\n",
    "#         Y_test = data['Y_hier'][:,:,-data['horizon']:]\n",
    "#         Y_train = data['Y_hier'][:,:,:-data['horizon']]\n",
    "        \n",
    "        \n",
    "        return y_rec_model,y_rec_mean, y_test, y_train\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "BOOTSTRAP FavoritaComplete statistical baselines evaluation \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parse execution parameters\n",
    "verbose = False\n",
    "intervals_method = 'bootstrap'\n",
    "dataset = 'FavoritaComplete'\n",
    "\n",
    "assert intervals_method in ['bootstrap', 'normality', 'permbu'], \\\n",
    "    \"Select `--intervals_method` from ['bootstrap', 'normality', 'permbu']\"\n",
    "\n",
    "available_datasets = ['Favorita200', 'Favorita500', 'FavoritaComplete']\n",
    "assert dataset in available_datasets, \\\n",
    "    \"Select `--dataset` from ['Favorita200', 'Favorita500', 'FavoritaComplete']\"\n",
    "\n",
    "LEVEL = np.arange(0, 100, 2)\n",
    "qs = [[50-lv/2, 50+lv/2] for lv in LEVEL]\n",
    "QUANTILES = np.sort(np.concatenate(qs)/100)\n",
    "\n",
    "# Run experiments\n",
    "Y_all_df, S_df, tags = FavoritaData.load(directory='./data/favorita/', group=dataset)\n",
    "items = Y_all_df.item_id.unique()\n",
    "\n",
    "print('\\n')\n",
    "print(f'{intervals_method.upper()} {dataset} statistical baselines evaluation \\n')\n",
    "Y_rec_list = []\n",
    "Y_test_list = []\n",
    "Y_train_list = []\n",
    "Y_rec_mean_list = []\n",
    "directory = './data/favorita'\n",
    "data = FavoritaHierarchicalDataset.load_process_data(dataset=dataset, directory = directory)\n",
    "import sys\n",
    "#with open('./log3.txt', 'w') as sys.stdout:\n",
    "for count,item_id in enumerate(items):\n",
    "    Y_rec_curr,Y_rec_mean_curr, Y_test_curr, Y_train_curr = run_baselines(item_id = item_id, data =data, dataset=dataset,intervals_method=intervals_method,verbose=verbose)\n",
    "    Y_rec_list.append(Y_rec_curr)\n",
    "    Y_rec_mean_list.append(Y_rec_mean_curr)\n",
    "    Y_test_list.append(Y_test_curr)\n",
    "    Y_train_list.append(Y_train_curr)\n",
    "        #print(f'{count} ')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_rec = np.array(Y_rec_list)\n",
    "Y_rec_mean = np.array(Y_rec_mean_list)\n",
    "\n",
    "Y_test = np.array(Y_test_list)\n",
    "Y_train = np.array(Y_train_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4036, 10, 93, 34)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_rec_mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(dict(level=['Overall']+list(data['tags'].keys())))\n",
    "\n",
    "\n",
    "hier_idxs = data['hier_idxs']\n",
    "LEVEL = np.arange(0, 100, 2)\n",
    "qs = [[50-lv/2, 50+lv/2] for lv in LEVEL]\n",
    "QUANTILES = np.sort(np.concatenate(qs)/100)\n",
    "\n",
    "seed_model_crps = []\n",
    "seed_model_relmse = []\n",
    "for seed_i in range(10):\n",
    "    Y_rec_i = np.maximum(Y_rec[:,seed_i,:,:,:],0)\n",
    "    Y_hat = np.maximum(Y_rec_mean[:,seed_i,:,:],0)\n",
    "    #Y_hat = Y_rec_i.mean(axis=-1)\n",
    "    curr_crps = FavoritaHierarchicalDataset._get_hierarchical_scrps(hier_idxs, Y_test, Y_rec_i, q_to_pred=QUANTILES)\n",
    "    curr_relmse = FavoritaHierarchicalDataset._get_hierarchical_rel_mse(hier_idxs, Y_test, Y_hat, Y_train = Y_train)\n",
    "    seed_model_crps.append(curr_crps)\n",
    "    seed_model_relmse.append(curr_relmse)\n",
    "\n",
    "seed_model_crps = np.array(seed_model_crps)\n",
    "seed_model_relmse = np.array(seed_model_relmse)\n",
    "\n",
    "levels_crps = []\n",
    "levels_relmse = []\n",
    "for level_j in range(results_df.shape[0]):\n",
    "    curr_level_crps = f'{np.mean(seed_model_crps[:,level_j],axis=0):.4f}±{(1.96 * np.std(seed_model_crps[:,level_j],axis=0)):.4f}'\n",
    "    curr_level_relmse = f'{np.mean(seed_model_relmse[:,level_j],axis=0):.4f}±{(1.96 * np.std(seed_model_relmse[:,level_j],axis=0)):.4f}'\n",
    "\n",
    "    levels_crps.append(curr_level_crps)\n",
    "    levels_relmse.append(curr_level_relmse)\n",
    "\n",
    "results_df['scrps'] = levels_crps\n",
    "results_df['rel_mse'] = levels_relmse\n",
    "\n",
    "results_df.to_csv(f'./data/{intervals_method}_metrics_minT_ols.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level</th>\n",
       "      <th>scrps</th>\n",
       "      <th>rel_mse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Overall</td>\n",
       "      <td>0.3968±0.0007</td>\n",
       "      <td>1.1270±0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Country</td>\n",
       "      <td>0.2652±0.0005</td>\n",
       "      <td>1.0833±0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Country/State</td>\n",
       "      <td>0.3782±0.0006</td>\n",
       "      <td>1.1164±0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Country/State/City</td>\n",
       "      <td>0.4028±0.0008</td>\n",
       "      <td>1.1341±0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Country/State/City/Store</td>\n",
       "      <td>0.5410±0.0010</td>\n",
       "      <td>1.3157±0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      level          scrps        rel_mse\n",
       "0                   Overall  0.3968±0.0007  1.1270±0.0000\n",
       "1                   Country  0.2652±0.0005  1.0833±0.0000\n",
       "2             Country/State  0.3782±0.0006  1.1164±0.0000\n",
       "3        Country/State/City  0.4028±0.0008  1.1341±0.0000\n",
       "4  Country/State/City/Store  0.5410±0.0010  1.3157±0.0000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hierarchical_baselines",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
