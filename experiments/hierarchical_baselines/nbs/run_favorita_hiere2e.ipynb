{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwEmMBAeZsYA"
      },
      "source": [
        "# HierE2E Favorita Baseline\n",
        "\n",
        "This notebook runs and evaluates HierE2E's baseline method predictions for the Favorita dataset.\n",
        "\n",
        "- It reads a preprocessed Favorita dataset.\n",
        "- It fits a HierE2E's model.\n",
        "- It evaluates HierE2E forecasts' sCRPS and MSSE.\n",
        "\n",
        "## References\n",
        "- [GluonTS, DeepVARHierarchicalEstimator](https://ts.gluon.ai/stable/api/gluonts/gluonts.mx.model.deepvar_hierarchical.html?highlight=deepvarhierarchicalestimator#gluonts.mx.model.deepvar_hierarchical.DeepVARHierarchicalEstimator)\n",
        "- [Syama Sundar Rangapuram, Lucien D Werner, Konstantinos Benidis, Pedro Mercado, Jan Gasthaus, Tim Januschowski. (2021). End-to-End Learning of Coherent Probabilistic Forecasts for Hierarchical Time Series. Proceedings of the 38th International Conference on Machine Learning (ICML).](https://proceedings.mlr.press/v139/rangapuram21a.html)\n",
        "\n",
        "\n",
        "<br>\n",
        "You can run these experiments using GPU with Google Colab.\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/Nixtla/hierarchicalforecast/blob/main/experiments/hierarchical_baselines/nbs/run_favorita_hiere2e.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install mxnet-cu112"
      ],
      "metadata": {
        "id": "RA9WuY6Sa3nr"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import mxnet as mx\n",
        "\n",
        "assert mx.context.num_gpus()>0"
      ],
      "metadata": {
        "id": "iQ3oRjlQa5zE"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install gluonts\n",
        "!pip install \"gluonts[mxnet,pro]\"\n",
        "!pip install git+https://github.com/Nixtla/hierarchicalforecast.git\n",
        "!pip install git+https://github.com/Nixtla/datasetsforecast.git@feat/favorita_dataset"
      ],
      "metadata": {
        "id": "wQrFyxlqa66R"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from gluonts.mx.trainer import Trainer\n",
        "from gluonts.dataset.hierarchical import HierarchicalTimeSeries\n",
        "from gluonts.dataset.common import Dataset, ListDataset\n",
        "from gluonts.mx.model.deepvar_hierarchical import DeepVARHierarchicalEstimator\n",
        "\n",
        "from hierarchicalforecast.evaluation import scaled_crps, rel_mse, msse\n",
        "from datasetsforecast.favorita import FavoritaData, FavoritaInfo\n",
        "\n",
        "import warnings\n",
        "# Avoid pandas fragmentation warning and positive definite warning\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "wziHeOxZgXWf"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "7DBVhznZZsYE"
      },
      "outputs": [],
      "source": [
        "class FavoritaHierarchicalDataset(object):\n",
        "    # Class with loading, processing and\n",
        "    # prediction evaluation methods for hierarchical data\n",
        "\n",
        "    available_datasets = ['Favorita200', 'Favorita500', 'FavoritaComplete']\n",
        "\n",
        "    @staticmethod\n",
        "    def _get_hierarchical_scrps(hier_idxs, Y, Yq_hat, q_to_pred):\n",
        "        # We use the indexes obtained from the aggregation tags\n",
        "        # to compute scaled CRPS across the hierarchy levels\n",
        "        # # [n_items, n_stores, n_time, n_quants]\n",
        "        scrps_list = []\n",
        "        for idxs in hier_idxs:\n",
        "            y      = Y[:, idxs, :]\n",
        "            yq_hat = Yq_hat[:, idxs, :, :]\n",
        "            scrps  = scaled_crps(y, yq_hat, q_to_pred)\n",
        "            scrps_list.append(scrps)\n",
        "        return scrps_list\n",
        "\n",
        "    @staticmethod\n",
        "    def _get_hierarchical_msse(hier_idxs, Y, Y_hat, Y_train):\n",
        "        # We use the indexes obtained from the aggregation tags\n",
        "        # to compute scaled CRPS across the hierarchy levels\n",
        "        msse_list = []\n",
        "        for idxs in hier_idxs:\n",
        "            y       = Y[:, idxs, :]\n",
        "            y_hat   = Y_hat[:, idxs, :]\n",
        "            y_train = Y_train[:, idxs, :]\n",
        "            crps    = msse(y, y_hat, y_train)\n",
        "            msse_list.append(crps)\n",
        "        return msse_list\n",
        "\n",
        "    @staticmethod\n",
        "    def _sort_hier_df(Y_df, S_df):\n",
        "        # NeuralForecast core, sorts unique_id lexicographically\n",
        "        # deviating from S_df, this class matches S_df and Y_hat_df order.\n",
        "        Y_df.unique_id = Y_df.unique_id.astype('category')\n",
        "        Y_df.unique_id = Y_df.unique_id.cat.set_categories(S_df.index)\n",
        "        Y_df = Y_df.sort_values(by=['unique_id', 'ds'])\n",
        "        return Y_df\n",
        "\n",
        "    @staticmethod\n",
        "    def _nonzero_indexes_by_row(M):\n",
        "        return [np.nonzero(M[row,:])[0] for row in range(len(M))]\n",
        "\n",
        "    @staticmethod\n",
        "    def load_item_data(item_id, dataset='Favorita200', directory='./data'):\n",
        "        # Load data\n",
        "        data_info = FavoritaInfo[dataset]\n",
        "        Y_df, S_df, tags = FavoritaData.load(directory=directory,\n",
        "                                             group=dataset)\n",
        "\n",
        "        # Parse and augment data\n",
        "        # + hack geographic hier_id to treat it as unique_id\n",
        "        Y_df['ds'] = pd.to_datetime(Y_df['ds'])\n",
        "        Y_df = Y_df[Y_df.item_id==item_id]\n",
        "        Y_df = Y_df.rename(columns={'hier_id': 'unique_id'})\n",
        "        Y_df = FavoritaHierarchicalDataset._sort_hier_df(Y_df=Y_df, S_df=S_df)\n",
        "\n",
        "        # Obtain indexes for plots and evaluation\n",
        "        hier_levels = ['Overall'] + list(tags.keys())\n",
        "        hier_idxs = [np.arange(len(S_df))] +\\\n",
        "            [S_df.index.get_indexer(tags[level]) for level in list(tags.keys())]\n",
        "        hier_linked_idxs = FavoritaHierarchicalDataset._nonzero_indexes_by_row(S_df.values.T)\n",
        "\n",
        "        # MinT along other methods require a positive definite covariance matrix\n",
        "        # for the residuals, when dealing with 0s as residuals the methods break\n",
        "        # data is augmented with minimal normal noise to avoid this error.\n",
        "        Y_df['y'] = Y_df['y'] + np.random.normal(loc=0.0, scale=0.01, size=len(Y_df))\n",
        "\n",
        "        # Final output\n",
        "        data = dict(Y_df=Y_df, S_df=S_df, tags=tags,\n",
        "                    # Hierarchical idxs\n",
        "                    hier_idxs=hier_idxs,\n",
        "                    hier_levels=hier_levels,\n",
        "                    hier_linked_idxs=hier_linked_idxs,\n",
        "                    # Dataset Properties\n",
        "                    horizon=data_info.horizon,\n",
        "                    freq=data_info.freq,\n",
        "                    seasonality=data_info.seasonality)\n",
        "        return data\n",
        "\n",
        "    @staticmethod\n",
        "    def load_all_data(dataset='Favorita200', directory='./data'):\n",
        "        print('Hello')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YR4_W_l8ZsYF",
        "outputId": "ed1cf57d-4777-458f-a81e-f40c89064a82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 930M/930M [00:31<00:00, 29.5MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "saved train.csv to train.feather for fast access\n"
          ]
        }
      ],
      "source": [
        "data = FavoritaHierarchicalDataset.load_item_data(item_id=1916577,\n",
        "                          directory = './data/favorita', dataset='Favorita200')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assert 1<0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "id": "AIt8ajjebQi5",
        "outputId": "01dca1a9-24c9-4555-8020-95e510d66220"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-471962f75282>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = FavoritaHierarchicalDataset.load_all_data(directory='./data/favorita',\n",
        "                                                 dataset='Favorita200')\n",
        "dataset = 'Favorita200'\n",
        "data_info = FavoritaInfo[dataset]\n",
        "Y_df, S_df, tags = FavoritaData.load(directory='./data/favorita', group=dataset)\n",
        "Y_df['unique_id'] = Y_df['hier_id'] + '_' + Y_df['item_id'].astype(str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oYzIDOGcdu76",
        "outputId": "ab6f59e4-f63b-4157-d6eb-b9eb4c66c062"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "is_validation = False\n",
        "freq = data_info.freq\n",
        "horizon = data_info.horizon\n",
        "dates = Y_df.ds.unique() # already sorted\n",
        "\n",
        "n_items = len(Y_df.item_id.unique())\n",
        "n_hier = len(Y_df.hier_id.unique())\n",
        "n_dates = len(Y_df.ds.unique())\n",
        "\n",
        "Y = np.reshape(Y_df.y.values, (n_items, n_hier, n_dates))\n",
        "S = S_df.values\n",
        "\n",
        "# Get the right split for validation or test: `evaluation_length` time steps will be evaluated.\n",
        "if is_validation:\n",
        "    target_train = Y[:, :, :len(dates) - 2 * horizon]\n",
        "    target_test = Y[:, :, :len(dates) - horizon]\n",
        "    valid_plus_test_length = 2 * horizon\n",
        "    valid_length = horizon\n",
        "else:\n",
        "    target_train = Y[:, :, :len(dates) - horizon]\n",
        "    target_test = Y\n",
        "    valid_plus_test_length = horizon\n",
        "    valid_length = 0\n",
        "\n",
        "train_dataset = ListDataset(\n",
        "    [{\"start\":dates[0], \"item_id\": \"all_items\", \\\n",
        "      \"target\": target_train[idx,:,:]} for idx in range(n_items)],\n",
        "    freq=freq,\n",
        "    one_dim_target=False\n",
        ")\n",
        "test_dataset = ListDataset(\n",
        "    [{\"start\": dates[0], \"item_id\": \"all_items\", \\\n",
        "      \"target\": target_test[idx,:,:]} for idx in range(n_items)],\n",
        "    freq=freq,\n",
        "    one_dim_target=False\n",
        ")"
      ],
      "metadata": {
        "id": "ewrPKlfOjIu8"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimal parameters reported from ICML 2021 code\n",
        "config = {\"epochs\": 1, \"num_batches_per_epoch\": 50, \"scaling\": True,\n",
        "       \"pick_incomplete\": False, \"batch_size\": 4, \"num_parallel_samples\": 200,\n",
        "       \"hybridize\": False, \"learning_rate\": 0.001, \"context_length\": 36,\n",
        "       \"rank\": 0, \"assert_reconciliation\": False, \"num_deep_models\": 1,\n",
        "       \"num_layers\": 2, \"num_cells\": 40, \"coherent_train_samples\": True,\n",
        "       \"coherent_pred_samples\": True, \"likelihood_weight\": 1.0,\n",
        "       \"CRPS_weight\": 0.0, \"num_samples_for_loss\": 50, \"sample_LH\": True,\n",
        "       \"seq_axis\": [1], \"warmstart_epoch_frac\": 0.0}"
      ],
      "metadata": {
        "id": "o7MVHLI9veW6"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "estimator = DeepVARHierarchicalEstimator(\n",
        "    freq=freq, # Set TourismSmall freq='M', 'Q' Freq fails\n",
        "    prediction_length=horizon,\n",
        "    target_dim=n_hier,\n",
        "    S=S,\n",
        "    trainer=Trainer(ctx = mx.context.gpu(),\n",
        "                    epochs=config['epochs'],\n",
        "                    num_batches_per_epoch=config['num_batches_per_epoch'],\n",
        "                    hybridize=config['hybridize'],\n",
        "                    learning_rate=config['learning_rate']),\n",
        "    scaling=config['scaling'],\n",
        "    pick_incomplete=config['pick_incomplete'],\n",
        "    batch_size=config['batch_size'],\n",
        "    num_parallel_samples=config['num_parallel_samples'],\n",
        "    context_length=config['context_length'],\n",
        "    num_layers=config['num_layers'],\n",
        "    num_cells=config['num_cells'],\n",
        "    coherent_train_samples=config['coherent_train_samples'],\n",
        "    coherent_pred_samples=config['coherent_pred_samples'],\n",
        "    likelihood_weight=config['likelihood_weight'],\n",
        "    CRPS_weight=config['CRPS_weight'],\n",
        "    num_samples_for_loss=config['num_samples_for_loss'],\n",
        "    sample_LH=config['sample_LH'],\n",
        "    seq_axis=config['seq_axis'],\n",
        "    warmstart_epoch_frac = config['warmstart_epoch_frac'],\n",
        ")\n",
        "\n",
        "predictor = estimator.train(train_dataset)\n",
        "forecast_it = predictor.predict(train_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZilLdARtobmS",
        "outputId": "fd06ad79-a64f-4a37-d56a-4b7f98526136"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50/50 [00:16<00:00,  2.98it/s, epoch=1/1, avg_epoch_loss=301]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assert 1<0"
      ],
      "metadata": {
        "id": "pgJnLKglwL_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "data_info = FavoritaInfo[dataset]\n",
        "Y_df, S_df, tags = FavoritaData.load(directory=directory,\n",
        "                                      group=dataset)\n",
        "\n",
        "# Parse and augment data\n",
        "# + hack geographic hier_id to treat it as unique_id\n",
        "Y_df['ds'] = pd.to_datetime(Y_df['ds'])\n",
        "Y_df = Y_df[Y_df.item_id==item_id]\n",
        "Y_df = Y_df.rename(columns={'hier_id': 'unique_id'})\n",
        "Y_df = FavoritaHierarchicalDataset._sort_hier_df(Y_df=Y_df, S_df=S_df)"
      ],
      "metadata": {
        "id": "rWmR4TcCc7OS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1byCoHjyZsYG"
      },
      "outputs": [],
      "source": [
        "def run_baselines(intervals_method, item_id=1916577, dataset='Favorita200', verbose=False, seed=0):\n",
        "    with CodeTimer('Read and Parse data   ', verbose):\n",
        "        data = FavoritaHierarchicalDataset.load_item_data(item_id=item_id,\n",
        "                                                     dataset=dataset, directory = './data/favorita')\n",
        "        Y_df = data['Y_df'][[\"unique_id\", 'ds', 'y']]\n",
        "        S_df, tags = data['S_df'], data['tags']\n",
        "        horizon = data['horizon']\n",
        "        seasonality = data['seasonality']\n",
        "        freq = data['freq']\n",
        "\n",
        "        # Train/Test Splits\n",
        "        Y_test_df  = Y_df.groupby('unique_id').tail(horizon)\n",
        "        Y_train_df = Y_df.drop(Y_test_df.index)\n",
        "        Y_test_df  = Y_test_df.set_index('unique_id')\n",
        "        Y_train_df = Y_train_df.set_index('unique_id')\n",
        "\n",
        "        dataset_str = f'{dataset} item_id={item_id}, h={horizon} '\n",
        "        dataset_str += f'n_series={len(S_df)}, n_bottom={len(S_df.columns)} \\n'\n",
        "        dataset_str += f'test ds=[{min(Y_test_df.ds), max(Y_test_df.ds)}] '\n",
        "        print(dataset_str)\n",
        "\n",
        "    with CodeTimer('Fit/Predict Model\t  ', verbose):\n",
        "        # Read to avoid unnecesary AutoARIMA computation\n",
        "        item_path = f'./data/{dataset}/{item_id}'\n",
        "        os.makedirs(item_path, exist_ok=True)\n",
        "        yhat_file = f'{item_path}/Y_hat.csv'\n",
        "        yfitted_file = f'{item_path}/Y_fitted.csv'\n",
        "        yrec_file = f'{item_path}/{intervals_method}_rec.csv'\n",
        "\n",
        "        if os.path.exists(yhat_file):\n",
        "            Y_hat_df = pd.read_csv(yhat_file)\n",
        "            Y_fitted_df = pd.read_csv(yfitted_file)\n",
        "\n",
        "        else:\n",
        "            if not os.path.exists(f'./data/{dataset}'):\n",
        "                os.makedirs(f'./data/{dataset}')\n",
        "            fcst = StatsForecast(\n",
        "                df=Y_train_df,\n",
        "                models=[AutoARIMA(season_length=seasonality)],\n",
        "                fallback_model=[Naive()],\n",
        "                freq=freq,\n",
        "                n_jobs=-1\n",
        "            )\n",
        "            Y_hat_df = fcst.forecast(h=horizon, fitted=True, level=LEVEL)\n",
        "            Y_fitted_df = fcst.forecast_fitted_values()\n",
        "\n",
        "            Y_hat_df = Y_hat_df.reset_index()\n",
        "            Y_fitted_df = Y_fitted_df.reset_index()\n",
        "            Y_hat_df.to_csv(yhat_file, index=False)\n",
        "            Y_fitted_df.to_csv(yfitted_file, index=False)\n",
        "\n",
        "        Y_hat_df = Y_hat_df.set_index('unique_id')\n",
        "        Y_fitted_df = Y_fitted_df.set_index('unique_id')\n",
        "\n",
        "    with CodeTimer('Reconcile Predictions ', verbose):\n",
        "        if is_strictly_hierarchical(S=S_df.values.astype(np.float32),\n",
        "            tags={key: S_df.index.get_indexer(val) for key, val in tags.items()}):\n",
        "            reconcilers = [\n",
        "                BottomUp(),\n",
        "                TopDown(method='average_proportions'),\n",
        "                TopDown(method='proportion_averages'),\n",
        "                MinTrace(method='ols'),\n",
        "                MinTrace(method='mint_shrink', mint_shr_ridge=1e-6),\n",
        "                #ERM(method='reg_bu', lambda_reg=100) # Extremely inneficient\n",
        "                ERM(method='closed')\n",
        "            ]\n",
        "        else:\n",
        "            reconcilers = [\n",
        "                BottomUp(),\n",
        "                MinTrace(method='ols'),\n",
        "                MinTrace(method='mint_shrink', mint_shr_ridge=1e-6),\n",
        "                #ERM(method='reg_bu', lambda_reg=100) # Extremely inneficient\n",
        "                ERM(method='closed')\n",
        "            ]\n",
        "\n",
        "        hrec = HierarchicalReconciliation(reconcilers=reconcilers)\n",
        "        Y_rec_df = hrec.bootstrap_reconcile(Y_hat_df=Y_hat_df,\n",
        "                                            Y_df=Y_fitted_df,\n",
        "                                            S_df=S_df, tags=tags,\n",
        "                                            level=LEVEL,\n",
        "                                            intervals_method=intervals_method,\n",
        "                                            num_samples=10, num_seeds=10)\n",
        "\n",
        "        # Matching Y_test/Y_rec/S index ordering\n",
        "        Y_test_df = Y_test_df.reset_index()\n",
        "        Y_test_df.unique_id = Y_test_df.unique_id.astype('category')\n",
        "        Y_test_df.unique_id = Y_test_df.unique_id.cat.set_categories(S_df.index)\n",
        "        Y_test_df = Y_test_df.sort_values(by=['unique_id', 'ds'])\n",
        "\n",
        "        Y_rec_df = Y_rec_df.reset_index()\n",
        "        Y_rec_df.unique_id = Y_rec_df.unique_id.astype('category')\n",
        "        Y_rec_df.unique_id = Y_rec_df.unique_id.cat.set_categories(S_df.index)\n",
        "        Y_rec_df = Y_rec_df.sort_values(by=['seed', 'unique_id', 'ds'])\n",
        "\n",
        "        Y_rec_df.to_csv(yrec_file, index=False)\n",
        "\n",
        "        return Y_rec_df, Y_test_df, Y_train_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yAXweIR8ZsYH"
      },
      "outputs": [],
      "source": [
        "# Parse execution parameters\n",
        "verbose = True\n",
        "intervals_method = 'bootstrap'\n",
        "dataset = 'Favorita200'\n",
        "\n",
        "assert intervals_method in ['bootstrap', 'normality', 'permbu'], \\\n",
        "    \"Select `--intervals_method` from ['bootstrap', 'normality', 'permbu']\"\n",
        "\n",
        "available_datasets = ['Favorita200', 'Favorita500', 'FavoritaComplete']\n",
        "assert dataset in available_datasets, \\\n",
        "    \"Select `--dataset` from ['Favorita200', 'Favorita500', 'FavoritaComplete']\"\n",
        "\n",
        "LEVEL = np.arange(0, 100, 2)\n",
        "qs = [[50-lv/2, 50+lv/2] for lv in LEVEL]\n",
        "QUANTILES = np.sort(np.concatenate(qs)/100)\n",
        "\n",
        "# Run experiments\n",
        "Y_all_df, S_df, tags = FavoritaData.load(directory='./data/favorita/', group='Favorita200')\n",
        "items = Y_all_df.item_id.unique()\n",
        "\n",
        "print('\\n')\n",
        "print(f'{intervals_method.upper()} {dataset} statistical baselines evaluation \\n')\n",
        "#for item_id in items:\n",
        "for item_id in [112830, 1916577]:\n",
        "    Y_rec_df, Y_test_df, Y_train_df = run_baselines(item_id=item_id, dataset=dataset,\n",
        "                                                    intervals_method=intervals_method,\n",
        "                                                    verbose=verbose)\n",
        "    print('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sDsBdyGKZsYH"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}