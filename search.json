[
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "Core",
    "section": "",
    "text": "HierarchicalForecast contains pure Python implementations of hierarchical reconciliation methods as well as a core.HierarchicalReconciliation wrapper class that enables easy interaction with these methods through pandas DataFrames containing the hierarchical time series and the base predictions.\nThe core.HierarchicalReconciliation reconciliation class operates with the hierarchical time series pd.DataFrame Y_df, the base predictions pd.DataFrame Y_hat_df, the aggregation constraints matrix S. For more information on the creation of aggregation constraints matrix see the utils aggregation method.\n\ncore.HierarchicalReconciliation\n\nsource\n\ninit\n\n init (reconcilers:List[Callable])\n\nHierarchical Reconciliation Class.\nThe core.HierarchicalReconciliation class allows you to efficiently fit multiple HierarchicaForecast methods for a collection of time series and base predictions stored in pandas DataFrames. The Y_df dataframe identifies series and datestamps with the unique_id and ds columns while the y column denotes the target time series variable. The Y_h dataframe stores the base predictions, example (AutoARIMA, ETS, etc.).\nParameters: reconcilers: A list of instantiated classes of the reconciliation methods module .\nReferences: Rob J. Hyndman and George Athanasopoulos (2018). “Forecasting principles and practice, Hierarchical and Grouped Series”.\n\nsource\n\n\nHierarchicalReconciliation\n\n HierarchicalReconciliation (reconcilers:List[Callable])\n\nHierarchical Reconciliation Class.\nThe core.HierarchicalReconciliation class allows you to efficiently fit multiple HierarchicaForecast methods for a collection of time series and base predictions stored in pandas DataFrames. The Y_df dataframe identifies series and datestamps with the unique_id and ds columns while the y column denotes the target time series variable. The Y_h dataframe stores the base predictions, example (AutoARIMA, ETS, etc.).\nParameters: reconcilers: A list of instantiated classes of the reconciliation methods module .\nReferences: Rob J. Hyndman and George Athanasopoulos (2018). “Forecasting principles and practice, Hierarchical and Grouped Series”.\n\nsource\n\n\nreconcile\n\n reconcile (Y_hat_df:pandas.core.frame.DataFrame,\n            S:pandas.core.frame.DataFrame, tags:Dict[str,numpy.ndarray],\n            Y_df:Optional[pandas.core.frame.DataFrame]=None,\n            level:Optional[List[int]]=None,\n            intervals_method:str='normality', num_samples:int=-1,\n            seed:int=0, sort_df:bool=True, is_balanced:bool=False)\n\nHierarchical Reconciliation Method.\nThe reconcile method is analogous to SKLearn fit_predict method, it applies different reconciliation techniques instantiated in the reconcilers list.\nMost reconciliation methods can be described by the following convenient linear algebra notation:\n\\[\\tilde{\\mathbf{y}}_{[a,b],\\tau} = \\mathbf{S}_{[a,b][b]} \\mathbf{P}_{[b][a,b]} \\hat{\\mathbf{y}}_{[a,b],\\tau}\\]\nwhere \\(a, b\\) represent the aggregate and bottom levels, \\(\\mathbf{S}_{[a,b][b]}\\) contains the hierarchical aggregation constraints, and \\(\\mathbf{P}_{[b][a,b]}\\) varies across reconciliation methods. The reconciled predictions are \\(\\tilde{\\mathbf{y}}_{[a,b],\\tau}\\), and the base predictions \\(\\hat{\\mathbf{y}}_{[a,b],\\tau}\\).\nParameters: Y_hat_df: pd.DataFrame, base forecasts with columns ds and models to reconcile indexed by unique_id. Y_df: pd.DataFrame, training set of base time series with columns ['ds', 'y'] indexed by unique_id. If a class of self.reconciles receives y_hat_insample, Y_df must include them as columns. S: pd.DataFrame with summing matrix of size (base, bottom), see aggregate method. tags: Each key is a level and its value contains tags associated to that level. level: positive float list [0,100), confidence levels for prediction intervals. intervals_method: str, method used to calculate prediction intervals, one of normality, bootstrap, permbu. num_samples: int=-1, if positive return that many probabilistic coherent samples. seed: int=0, random seed for numpy generator’s replicability. sort_df : bool (default=True), if True, sort df by [unique_id,ds]. is_balanced: bool=False, wether Y_df is balanced, set it to True to speed things up if Y_df is balanced.\nReturns: Y_tilde_df: pd.DataFrame, with reconciled predictions.\n\nsource\n\n\nbootstrap_reconcile\n\n bootstrap_reconcile (Y_hat_df:pandas.core.frame.DataFrame,\n                      S_df:pandas.core.frame.DataFrame,\n                      tags:Dict[str,numpy.ndarray],\n                      Y_df:Optional[pandas.core.frame.DataFrame]=None,\n                      level:Optional[List[int]]=None,\n                      intervals_method:str='normality',\n                      num_samples:int=-1, num_seeds:int=1,\n                      sort_df:bool=True)\n\nBootstraped Hierarchical Reconciliation Method.\nApplies N times, based on different random seeds, the reconcile method for the different reconciliation techniques instantiated in the reconcilers list.\nParameters: Y_hat_df: pd.DataFrame, base forecasts with columns ds and models to reconcile indexed by unique_id. Y_df: pd.DataFrame, training set of base time series with columns ['ds', 'y'] indexed by unique_id. If a class of self.reconciles receives y_hat_insample, Y_df must include them as columns. S: pd.DataFrame with summing matrix of size (base, bottom), see aggregate method. tags: Each key is a level and its value contains tags associated to that level. level: positive float list [0,100), confidence levels for prediction intervals. intervals_method: str, method used to calculate prediction intervals, one of normality, bootstrap, permbu. num_samples: int=-1, if positive return that many probabilistic coherent samples. num_seeds: int=1, random seed for numpy generator’s replicability. sort_df : bool (default=True), if True, sort df by [unique_id,ds].\nReturns: Y_bootstrap_df: pd.DataFrame, with bootstraped reconciled predictions.\n\n\n\nExample\n\nimport numpy as np\nimport pandas as pd\n\nfrom statsforecast.core import StatsForecast\nfrom statsforecast.models import ETS, Naive\n\nfrom hierarchicalforecast.utils import aggregate\nfrom hierarchicalforecast.core import HierarchicalReconciliation\nfrom hierarchicalforecast.methods import BottomUp, MinTrace\n\n# Load TourismSmall dataset\ndf = pd.read_csv('https://raw.githubusercontent.com/Nixtla/transfer-learning-time-series/main/datasets/tourism.csv')\ndf = df.rename({'Trips': 'y', 'Quarter': 'ds'}, axis=1)\ndf.insert(0, 'Country', 'Australia')\n\n# Create hierarchical seires based on geographic levels and purpose\n# And Convert quarterly ds string to pd.datetime format\nhierarchy_levels = [['Country'],\n                    ['Country', 'State'], \n                    ['Country', 'Purpose'], \n                    ['Country', 'State', 'Region'], \n                    ['Country', 'State', 'Purpose'], \n                    ['Country', 'State', 'Region', 'Purpose']]\n\nY_df, S_df, tags = aggregate(df=df, spec=hierarchy_levels)\nqs = Y_df['ds'].str.replace(r'(\\d+) (Q\\d)', r'\\1-\\2', regex=True)\nY_df['ds'] = pd.PeriodIndex(qs, freq='Q').to_timestamp()\nY_df = Y_df.reset_index()\n\n# Split train/test sets\nY_test_df  = Y_df.groupby('unique_id').tail(4)\nY_train_df = Y_df.drop(Y_test_df.index)\n\n# Compute base auto-ETS predictions\n# Careful identifying correct data freq, this data quarterly 'Q'\nfcst = StatsForecast(df=Y_train_df,\n                     #models=[ETS(season_length=12), Naive()],\n                     models=[Naive()],\n                     freq='Q', n_jobs=-1)\nY_hat_df = fcst.forecast(h=4, fitted=True)\nY_fitted_df = fcst.forecast_fitted_values()\n\n# Reconcile the base predictions\nY_train_df = Y_train_df.reset_index().set_index('unique_id')\nY_hat_df = Y_hat_df.reset_index().set_index('unique_id')\nreconcilers = [BottomUp(),\n               MinTrace(method='mint_shrink')]\nhrec = HierarchicalReconciliation(reconcilers=reconcilers)\nY_rec_df = hrec.reconcile(Y_hat_df=Y_hat_df, \n                          Y_df=Y_fitted_df,\n                          S=S_df, tags=tags)\nY_rec_df.groupby('unique_id').head(2)\n\n\n\n\n\nIf you find the code useful, please ⭐ us on Github"
  },
  {
    "objectID": "evaluation.html",
    "href": "evaluation.html",
    "title": "Hierarchical Evaluation",
    "section": "",
    "text": "To assist the evaluation of hierarchical forecasting systems, we make available accuracy metrics along with the HierarchicalEvaluation module that facilitates the measurement of prediction’s accuracy through the hierarchy levels.\nThe available metrics include point and probabilistic multivariate scoring rules that were used in previous hierarchical forecasting studies.\nIf you find the code useful, please ⭐ us on Github"
  },
  {
    "objectID": "evaluation.html#relative-mean-squared-error",
    "href": "evaluation.html#relative-mean-squared-error",
    "title": "Hierarchical Evaluation",
    "section": "Relative Mean Squared Error",
    "text": "Relative Mean Squared Error\n\nsource\n\nrel_mse\n\n rel_mse (y, y_hat, y_train, mask=None)\n\nRelative Mean Squared Error\nComputes Relative mean squared error (RelMSE), as proposed by Hyndman & Koehler (2006) as an alternative to percentage errors, to avoid measure unstability.\n\\[ \\mathrm{RelMSE}(\\mathbf{y}, \\mathbf{\\hat{y}}, \\mathbf{\\hat{y}}^{naive1}) =\n\\frac{\\mathrm{MSE}(\\mathbf{y}, \\mathbf{\\hat{y}})}{\\mathrm{MSE}(\\mathbf{y}, \\mathbf{\\hat{y}}^{naive1})} \\]\nParameters: y: numpy array, Actual values of size (n_series, horizon). y_hat: numpy array, Predicted values (n_series, horizon). mask: numpy array, Specifies date stamps per serie to consider in loss.\nReturns: loss: float.\nReferences: - Hyndman, R. J and Koehler, A. B. (2006). “Another look at measures of forecast accuracy”, International Journal of Forecasting, Volume 22, Issue 4. - Kin G. Olivares, O. Nganba Meetei, Ruijun Ma, Rohan Reddy, Mengfei Cao, Lee Dicker. “Probabilistic Hierarchical Forecasting with Deep Poisson Mixtures. Submitted to the International Journal Forecasting, Working paper available at arxiv."
  },
  {
    "objectID": "evaluation.html#mean-squared-scaled-error",
    "href": "evaluation.html#mean-squared-scaled-error",
    "title": "Hierarchical Evaluation",
    "section": "Mean Squared Scaled Error",
    "text": "Mean Squared Scaled Error\n\nsource\n\nmsse\n\n msse (y, y_hat, y_train, mask=None)\n\nMean Squared Scaled Error\nComputes Mean squared scaled error (MSSE), as proposed by Hyndman & Koehler (2006) as an alternative to percentage errors, to avoid measure unstability.\n\\[ \\mathrm{MSSE}(\\mathbf{y}, \\mathbf{\\hat{y}}, \\mathbf{y}^{in-sample}) =\n\\frac{\\frac{1}{h} \\sum^{t+h}_{\\tau=t+1} (y_{\\tau} - \\hat{y}_{\\tau})^2}{\\frac{1}{t-1} \\sum^{t}_{\\tau=2} (y_{\\tau} - y_{\\tau-1})^2},\\]\nwhere \\(n\\) (\\(n=\\)n) is the size of the training data, and \\(h\\) is the forecasting horizon (\\(h=\\)horizon).\nParameters: y: numpy array, Actual values of size (n_series, horizon). y_hat: numpy array, Predicted values (n_series, horizon). y_train: numpy array, Predicted values (n_series, n). mask: numpy array, Specifies date stamps per serie to consider in loss.\nReturns: loss: float.\nReferences: - Hyndman, R. J and Koehler, A. B. (2006). “Another look at measures of forecast accuracy”, International Journal of Forecasting, Volume 22, Issue 4."
  },
  {
    "objectID": "evaluation.html#scaled-crps",
    "href": "evaluation.html#scaled-crps",
    "title": "Hierarchical Evaluation",
    "section": "Scaled CRPS",
    "text": "Scaled CRPS\n\nsource\n\nscaled_crps\n\n scaled_crps (y, y_hat, quantiles)\n\nScaled Continues Ranked Probability Score\nCalculates a scaled variation of the CRPS, as proposed by Rangapuram (2021), to measure the accuracy of predicted quantiles y_hat compared to the observation y.\nThis metric averages percentual weighted absolute deviations as defined by the quantile losses.\n\\[ \\mathrm{sCRPS}(\\hat{F}_{\\tau}, \\mathbf{y}_{\\tau}) = \\frac{2}{N} \\sum_{i}\n\\int^{1}_{0}\n\\frac{\\mathrm{QL}(\\hat{F}_{i,\\tau}, y_{i,\\tau})_{q}}{\\sum_{i} | y_{i,\\tau} |} dq \\]\nwhere \\(\\hat{F}_{\\tau}\\) is the an estimated multivariate distribution, and \\(y_{i,\\tau}\\) are its realizations.\nParameters: y: numpy array, Actual values of size (n_series, horizon). y_hat: numpy array, Predicted quantiles of size (n_series, horizon, n_quantiles). quantiles: numpy array,(n_quantiles). Quantiles to estimate from the distribution of y.\nReturns: loss: float.\nReferences: - Gneiting, Tilmann. (2011). “Quantiles as optimal point forecasts”. International Journal of Forecasting. - Spyros Makridakis, Evangelos Spiliotis, Vassilios Assimakopoulos, Zhi Chen, Anil Gaba, Ilia Tsetlin, Robert L. Winkler. (2022). “The M5 uncertainty competition: Results, findings and conclusions”. International Journal of Forecasting. - Syama Sundar Rangapuram, Lucien D Werner, Konstantinos Benidis, Pedro Mercado, Jan Gasthaus, Tim Januschowski. (2021). “End-to-End Learning of Coherent Probabilistic Forecasts for Hierarchical Time Series”. Proceedings of the 38th International Conference on Machine Learning (ICML)."
  },
  {
    "objectID": "evaluation.html#energy-score",
    "href": "evaluation.html#energy-score",
    "title": "Hierarchical Evaluation",
    "section": "Energy Score",
    "text": "Energy Score\n\nsource\n\nenergy_score\n\n energy_score (y, y_sample1, y_sample2, beta=2)\n\nEnergy Score\nCalculates Gneiting’s Energy Score sample approximation for y and independent multivariate samples y_sample1 and y_sample2. The Energy Score generalizes the CRPS (beta=1) in the multivariate setting.\n\\[ \\mathrm{ES}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}_{\\tau}, \\mathbf{\\hat{y}}_{\\tau}')\n= \\frac{1}{2} \\mathbb{E}_{\\hat{P}} \\left[ ||\\mathbf{\\hat{y}}_{\\tau} - \\mathbf{\\hat{y}}_{\\tau}'||^{\\beta} \\right]\n-  \\mathbb{E}_{\\hat{P}} \\left[ ||\\mathbf{y}_{\\tau} - \\mathbf{\\hat{y}}_{\\tau}||^{\\beta} \\right]\n\\quad \\beta \\in (0,2]\\]\nwhere \\(\\mathbf{\\hat{y}}_{\\tau}, \\mathbf{\\hat{y}}_{\\tau}'\\) are independent samples drawn from \\(\\hat{P}\\).\nParameters: y: numpy array, Actual values of size (n_series, horizon). y_sample1: numpy array, predictive distribution sample of size (n_series, horizon, n_samples). y_sample2: numpy array, predictive distribution sample of size (n_series, horizon, n_samples). beta: float in (0,2], defines the energy score’s power for the euclidean metric.\nReturns: score: float.\nReferences: - Gneiting, Tilmann, and Adrian E. Raftery. (2007). “Strictly proper scoring rules, prediction and estimation”. Journal of the American Statistical Association. - Anastasios Panagiotelis, Puwasala Gamakumara, George Athanasopoulos, Rob J. Hyndman. (2022). “Probabilistic forecast reconciliation: Properties, evaluation and score optimisation”. European Journal of Operational Research.\n\nsource\n\n\nlog_score\n\n log_score (y, y_hat, cov, allow_singular=True)\n\nLog Score.\nOne of the simplest multivariate probability scoring rules, it evaluates the negative density at the value of the realisation.\n\\[ \\mathrm{LS}(\\mathbf{y}_{\\tau}, \\mathbf{P}(\\theta_{\\tau}))\n= - \\log(f(\\mathbf{y}_{\\tau}, \\theta_{\\tau}))\\]\nwhere \\(f\\) is the density, \\(\\mathbf{P}(\\theta_{\\tau})\\) is a parametric distribution and \\(f(\\mathbf{y}_{\\tau}, \\theta_{\\tau})\\) represents its density. For the moment we only support multivariate normal log score.\n\\[f(\\mathbf{y}_{\\tau}, \\theta_{\\tau}) =\n(2\\pi )^{-k/2}\\det({\\boldsymbol{\\Sigma }})^{-1/2}\n\\,\\exp \\left(\n-{\\frac {1}{2}}(\\mathbf{y}_{\\tau} -\\hat{\\mathbf{y}}_{\\tau})^{\\!{\\mathsf{T}}}\n{\\boldsymbol{\\Sigma }}^{-1}\n(\\mathbf{y}_{\\tau} -\\hat{\\mathbf{y}}_{\\tau})\n\\right)\\]\nParameters: y: numpy array, Actual values of size (n_series, horizon). y_hat: numpy array, Predicted values (n_series, horizon). cov: numpy matrix, Predicted values covariance (n_series, n_series, horizon). allow_singular: bool=True, if true allows singular covariance.\nReturns: score: float.\n\nx = np.linspace(0, 5, 10, endpoint=False)\ny = multivariate_normal.pdf(x, mean=2.5, cov=0.5)\ny"
  },
  {
    "objectID": "examples/hierarchicalforecast-gluonts.html",
    "href": "examples/hierarchicalforecast-gluonts.html",
    "title": "GluonTS",
    "section": "",
    "text": "This example notebook demonstrates the compatibility of HierarchicalForecast’s reconciliation methods with popular machine-learning libraries, specifically GluonTS.\nThe notebook utilizes the GluonTS DeepAREstimator to create base forecasts for the TourismLarge Hierarchical Dataset. We make the base forecasts compatible with HierarchicalForecast’s reconciliation functions via the samples_to_quantiles_df utility function that transforms GluonTS’ output forecasts into a compatible data frame format. After that, we use HierarchicalForecast to reconcile the base predictions.\nReferences - David Salinas, Valentin Flunkert, Jan Gasthaus, Tim Januschowski (2020). “DeepAR: Probabilistic forecasting with autoregressive recurrent networks”. International Journal of Forecasting. - Alexander Alexandrov et. al (2020). “GluonTS: Probabilistic and Neural Time Series Modeling in Python”. Journal of Machine Learning Research.\nYou can run these experiments using CPU or GPU with Google Colab.\nIf you find the code useful, please ⭐ us on Github"
  },
  {
    "objectID": "examples/hierarchicalforecast-gluonts.html#installing-packages",
    "href": "examples/hierarchicalforecast-gluonts.html#installing-packages",
    "title": "GluonTS",
    "section": "1. Installing packages",
    "text": "1. Installing packages\n\n!pip install mxnet-cu112\n\n\nimport mxnet as mx\n\nassert mx.context.num_gpus()&gt;0\n\n\n!pip install gluonts\n!pip install datasetsforecast\n!pip install git+https://github.com/Nixtla/hierarchicalforecast.git\n\n\nimport numpy as np\nimport pandas as pd\n\nfrom datasetsforecast.hierarchical import HierarchicalData\n\nfrom gluonts.mx.trainer import Trainer\nfrom gluonts.dataset.pandas import PandasDataset\nfrom gluonts.mx.model.deepar import DeepAREstimator\n\nfrom hierarchicalforecast.methods import BottomUp, MinTrace\nfrom hierarchicalforecast.core import HierarchicalReconciliation\nfrom hierarchicalforecast.evaluation import scaled_crps\nfrom hierarchicalforecast.utils import samples_to_quantiles_df\n\n/usr/local/lib/python3.10/dist-packages/gluonts/json.py:101: UserWarning: Using `json`-module for json-handling. Consider installing one of `orjson`, `ujson` to speed up serialization and deserialization.\n  warnings.warn("
  },
  {
    "objectID": "examples/hierarchicalforecast-gluonts.html#load-hierarchical-dataset",
    "href": "examples/hierarchicalforecast-gluonts.html#load-hierarchical-dataset",
    "title": "GluonTS",
    "section": "2. Load hierarchical dataset",
    "text": "2. Load hierarchical dataset\nThis detailed Australian Tourism Dataset comes from the National Visitor Survey, managed by the Tourism Research Australia, it is composed of 555 monthly series from 1998 to 2016, it is organized geographically, and purpose of travel. The natural geographical hierarchy comprises seven states, divided further in 27 zones and 76 regions. The purpose of travel categories are holiday, visiting friends and relatives (VFR), business and other. The MinT (Wickramasuriya et al., 2019), among other hierarchical forecasting studies has used the dataset it in the past. The dataset can be accessed in the MinT reconciliation webpage, although other sources are available.\n\n\n\n\n\n\n\n\n\nGeographical Division\nNumber of series per division\nNumber of series per purpose\nTotal\n\n\n\n\nAustralia\n1\n4\n5\n\n\nStates\n7\n28\n35\n\n\nZones\n27\n108\n135\n\n\nRegions\n76\n304\n380\n\n\nTotal\n111\n444\n555\n\n\n\n\ndataset = 'TourismLarge'\nY_df, S_df, tags = HierarchicalData.load(directory = \"./data\", group=dataset)\nY_df['ds'] = pd.to_datetime(Y_df['ds'])\n\n\ndef sort_hier_df(Y_df, S_df):\n    # sorts unique_id lexicographically\n    Y_df.unique_id = Y_df.unique_id.astype('category')\n    Y_df.unique_id = Y_df.unique_id.cat.set_categories(S_df.index)\n    Y_df = Y_df.sort_values(by=['unique_id', 'ds'])\n    return Y_df\n\nY_df = sort_hier_df(Y_df, S_df)\n\n\nhorizon = 12\n\nY_test_df = Y_df.groupby('unique_id').tail(horizon)\nY_train_df = Y_df.drop(Y_test_df.index)\nY_train_df\n\n\n  \n    \n      \n\n\n\n\n\n\nunique_id\nds\ny\n\n\n\n\n0\nTotalAll\n1998-01-01\n45151.071280\n\n\n1\nTotalAll\n1998-02-01\n17294.699551\n\n\n2\nTotalAll\n1998-03-01\n20725.114184\n\n\n3\nTotalAll\n1998-04-01\n25388.612353\n\n\n4\nTotalAll\n1998-05-01\n20330.035211\n\n\n...\n...\n...\n...\n\n\n126523\nGBDOth\n2015-08-01\n17.683774\n\n\n126524\nGBDOth\n2015-09-01\n0.000000\n\n\n126525\nGBDOth\n2015-10-01\n0.000000\n\n\n126526\nGBDOth\n2015-11-01\n0.000000\n\n\n126527\nGBDOth\n2015-12-01\n0.000000\n\n\n\n\n\n119880 rows × 3 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nds = PandasDataset.from_long_dataframe(Y_train_df, target=\"y\", item_id=\"unique_id\")"
  },
  {
    "objectID": "examples/hierarchicalforecast-gluonts.html#fit-and-predict-model",
    "href": "examples/hierarchicalforecast-gluonts.html#fit-and-predict-model",
    "title": "GluonTS",
    "section": "3. Fit and Predict Model",
    "text": "3. Fit and Predict Model\n\nestimator = DeepAREstimator(\n    freq=\"M\",\n    prediction_length=horizon,\n    trainer=Trainer(ctx = mx.context.gpu(),\n                    epochs=20),\n)\npredictor = estimator.train(ds)\n\nforecast_it = predictor.predict(ds, num_samples=1000)\n\nforecasts = list(forecast_it)\nforecasts = np.array([arr.samples for arr in forecasts])\nforecasts.shape\n\n100%|██████████| 50/50 [00:11&lt;00:00,  4.39it/s, epoch=1/20, avg_epoch_loss=5.35]\n100%|██████████| 50/50 [00:05&lt;00:00,  8.75it/s, epoch=2/20, avg_epoch_loss=5.22]\n100%|██████████| 50/50 [00:03&lt;00:00, 14.41it/s, epoch=3/20, avg_epoch_loss=5.17]\n100%|██████████| 50/50 [00:02&lt;00:00, 20.76it/s, epoch=4/20, avg_epoch_loss=5.02]\n100%|██████████| 50/50 [00:02&lt;00:00, 19.27it/s, epoch=5/20, avg_epoch_loss=5.05]\n100%|██████████| 50/50 [00:04&lt;00:00, 11.52it/s, epoch=6/20, avg_epoch_loss=5.12]\n100%|██████████| 50/50 [00:03&lt;00:00, 16.59it/s, epoch=7/20, avg_epoch_loss=4.97]\n100%|██████████| 50/50 [00:03&lt;00:00, 16.27it/s, epoch=8/20, avg_epoch_loss=4.97]\n100%|██████████| 50/50 [00:02&lt;00:00, 19.96it/s, epoch=9/20, avg_epoch_loss=5.11]\n100%|██████████| 50/50 [00:04&lt;00:00, 11.36it/s, epoch=10/20, avg_epoch_loss=4.97]\n100%|██████████| 50/50 [00:03&lt;00:00, 16.62it/s, epoch=11/20, avg_epoch_loss=5.05]\n100%|██████████| 50/50 [00:02&lt;00:00, 17.76it/s, epoch=12/20, avg_epoch_loss=5.04]\n100%|██████████| 50/50 [00:02&lt;00:00, 21.56it/s, epoch=13/20, avg_epoch_loss=4.99]\n100%|██████████| 50/50 [00:02&lt;00:00, 20.64it/s, epoch=14/20, avg_epoch_loss=5.03]\n100%|██████████| 50/50 [00:03&lt;00:00, 13.22it/s, epoch=15/20, avg_epoch_loss=4.97]\n100%|██████████| 50/50 [00:02&lt;00:00, 17.79it/s, epoch=16/20, avg_epoch_loss=4.95]\n100%|██████████| 50/50 [00:02&lt;00:00, 18.29it/s, epoch=17/20, avg_epoch_loss=5.02]\n100%|██████████| 50/50 [00:02&lt;00:00, 17.73it/s, epoch=18/20, avg_epoch_loss=5.02]\n100%|██████████| 50/50 [00:02&lt;00:00, 19.10it/s, epoch=19/20, avg_epoch_loss=5.02]\n100%|██████████| 50/50 [00:03&lt;00:00, 13.29it/s, epoch=20/20, avg_epoch_loss=5]\n\n\n(555, 1000, 12)"
  },
  {
    "objectID": "examples/hierarchicalforecast-gluonts.html#reconciliation",
    "href": "examples/hierarchicalforecast-gluonts.html#reconciliation",
    "title": "GluonTS",
    "section": "4. Reconciliation",
    "text": "4. Reconciliation\n\nlevel = np.arange(1, 100, 2)\n\n#transform the output of DeepAREstimator to a form that is compatible with HierarchicalForecast\nquantiles, forecast_df = samples_to_quantiles_df(samples=forecasts, \n                               unique_ids=S_df.index, \n                               dates=Y_test_df['ds'].unique(), \n                               level=level,\n                               model_name='DeepAREstimator')\n\n#reconcile forecasts\nreconcilers = [\n    BottomUp(),\n    MinTrace('ols')\n]\nhrec = HierarchicalReconciliation(reconcilers=reconcilers)\n\nforecast_rec = hrec.reconcile(Y_hat_df=forecast_df, S=S_df, tags=tags, level=level)\n\n\nforecast_rec\n\n\n  \n    \n      \n\n\n\n\n\n\nds\nDeepAREstimator\nDeepAREstimator-median\nDeepAREstimator-lo-99\nDeepAREstimator-lo-97\nDeepAREstimator-lo-95\nDeepAREstimator-lo-93\nDeepAREstimator-lo-91\nDeepAREstimator-lo-89\nDeepAREstimator-lo-87\n...\nDeepAREstimator/MinTrace_method-ols-hi-81\nDeepAREstimator/MinTrace_method-ols-hi-83\nDeepAREstimator/MinTrace_method-ols-hi-85\nDeepAREstimator/MinTrace_method-ols-hi-87\nDeepAREstimator/MinTrace_method-ols-hi-89\nDeepAREstimator/MinTrace_method-ols-hi-91\nDeepAREstimator/MinTrace_method-ols-hi-93\nDeepAREstimator/MinTrace_method-ols-hi-95\nDeepAREstimator/MinTrace_method-ols-hi-97\nDeepAREstimator/MinTrace_method-ols-hi-99\n\n\nunique_id\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTotalAll\n2016-01-01\n43165.929688\n43002.058594\n27712.297979\n30371.243516\n32741.458740\n33305.429492\n34446.465957\n35164.380410\n35732.592422\n...\n48703.132046\n48956.752480\n49233.843836\n49540.743219\n49886.826218\n50286.877928\n50766.394577\n51375.717577\n52240.506366\n53910.351214\n\n\nTotalAll\n2016-02-01\n20326.796875\n20469.210938\n13156.550879\n15086.488257\n15738.457031\n16134.386343\n16696.160010\n16828.676436\n17139.442129\n...\n22902.635244\n23019.412684\n23146.997118\n23288.306411\n23447.657478\n23631.857993\n23852.647485\n24133.205242\n24531.390118\n25300.256426\n\n\nTotalAll\n2016-03-01\n24362.203125\n24237.250977\n17340.837197\n18470.071582\n19132.180615\n19658.168945\n19974.223359\n20339.483584\n20519.382959\n...\n26759.166634\n26873.896338\n26999.243530\n27138.074912\n27294.631699\n27475.602189\n27692.520055\n27968.158127\n28359.360682\n29114.744632\n\n\nTotalAll\n2016-04-01\n29131.662109\n29236.008789\n19923.623740\n21814.112246\n22685.987500\n23350.113418\n23721.056963\n24168.286201\n24513.198066\n...\n32277.209370\n32427.584386\n32591.875632\n32773.840464\n32979.037796\n33216.233913\n33500.545877\n33861.821798\n34374.566871\n35364.640665\n\n\nTotalAll\n2016-05-01\n22587.779297\n22638.541016\n14453.285947\n16236.985869\n17163.251807\n17894.046758\n18559.204453\n18789.053066\n19055.381455\n...\n25400.976716\n25532.984575\n25677.208902\n25836.948122\n26017.082170\n26225.306596\n26474.892029\n26792.040860\n27242.158045\n28111.301901\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\nGBDOth\n2016-08-01\n-0.300811\n-0.316894\n-2.994549\n-2.208182\n-2.005075\n-1.725068\n-1.620723\n-1.501304\n-1.355108\n...\n27.151595\n28.293141\n29.540330\n30.921685\n32.479405\n34.280039\n36.438344\n39.180908\n43.073324\n50.589300\n\n\nGBDOth\n2016-09-01\n-0.089410\n-0.079164\n-2.981229\n-2.356738\n-1.812428\n-1.499515\n-1.365453\n-1.199702\n-1.120727\n...\n24.912080\n26.035044\n27.261932\n28.620801\n30.153165\n31.924489\n34.047662\n36.745584\n40.574640\n47.968273\n\n\nGBDOth\n2016-10-01\n-0.196041\n-0.207104\n-2.829650\n-2.270969\n-1.674091\n-1.289834\n-1.153728\n-1.078916\n-1.029915\n...\n25.423958\n26.550973\n27.782287\n29.146059\n30.683952\n32.461666\n34.592499\n37.300154\n41.143025\n48.563331\n\n\nGBDOth\n2016-11-01\n-0.315826\n-0.274183\n-2.461571\n-1.829249\n-1.535889\n-1.329642\n-1.260961\n-1.134465\n-1.007276\n...\n25.125960\n26.257991\n27.494784\n28.864625\n30.409361\n32.194986\n34.335301\n37.055005\n40.914977\n48.368305\n\n\nGBDOth\n2016-12-01\n-0.291579\n-0.268462\n-3.987842\n-2.078746\n-1.619226\n-1.385310\n-1.253607\n-1.156472\n-1.092625\n...\n26.216098\n27.310535\n28.506255\n29.830604\n31.324041\n33.050366\n35.119603\n37.748987\n41.480772\n48.686579\n\n\n\n\n\n6660 rows × 305 columns"
  },
  {
    "objectID": "examples/hierarchicalforecast-gluonts.html#evaluation",
    "href": "examples/hierarchicalforecast-gluonts.html#evaluation",
    "title": "GluonTS",
    "section": "5. Evaluation",
    "text": "5. Evaluation\nTo evaluate we use a scaled variation of the CRPS, as proposed by Rangapuram (2021), to measure the accuracy of predicted quantiles y_hat compared to the observation y.\n\\[ \\mathrm{sCRPS}(\\hat{F}_{\\tau}, \\mathbf{y}_{\\tau}) = \\frac{2}{N} \\sum_{i}\n\\int^{1}_{0}\n\\frac{\\mathrm{QL}(\\hat{F}_{i,\\tau}, y_{i,\\tau})_{q}}{\\sum_{i} | y_{i,\\tau} |} dq \\]\nAs you can see, HierarchicalForecast results improve on the results of specialized algorithms like HierE2E.\n\nrec_model_names = ['DeepAREstimator/MinTrace_method-ols', 'DeepAREstimator/BottomUp']\n\nquantiles = np.array(quantiles[1:]) #remove first quantile (median)\nn_quantiles = len(quantiles)\nn_series = len(S_df)\n\nfor name in rec_model_names:\n    quantile_columns = [col for col in forecast_rec.columns if (name+'-') in col]\n    y_rec  = forecast_rec[quantile_columns].values \n    y_test = Y_test_df['y'].values\n\n    y_rec  = y_rec.reshape(n_series, horizon, n_quantiles)\n    y_test = y_test.reshape(n_series, horizon)\n    scrps  = scaled_crps(y=y_test, y_hat=y_rec, quantiles=quantiles)\n    print(\"{:&lt;40} {:.5f}\".format(name+\":\", scrps))\n\nDeepAREstimator/MinTrace_method-ols:     0.12632\nDeepAREstimator/BottomUp:                0.13933"
  },
  {
    "objectID": "examples/australiandomestictourism-permbu-intervals.html",
    "href": "examples/australiandomestictourism-permbu-intervals.html",
    "title": "PERMBU",
    "section": "",
    "text": "In many cases, only the time series at the lowest level of the hierarchies (bottom time series) are available. HierarchicalForecast has tools to create time series for all hierarchies and also allows you to calculate prediction intervals for all hierarchies. In this notebook we will see how to do it.\n!pip install hierarchicalforecast statsforecast\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# compute base forecast no coherent\nfrom statsforecast.models import AutoARIMA\nfrom statsforecast.core import StatsForecast\n\n#obtain hierarchical reconciliation methods and evaluation\nfrom hierarchicalforecast.methods import BottomUp, MinTrace\nfrom hierarchicalforecast.utils import aggregate, HierarchicalPlot\nfrom hierarchicalforecast.core import HierarchicalReconciliation\nfrom hierarchicalforecast.evaluation import HierarchicalEvaluation\n\n/Users/fedex/miniconda3/envs/hierarchicalforecast/lib/python3.10/site-packages/statsforecast/core.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from tqdm.autonotebook import tqdm\nIf you find the code useful, please ⭐ us on Github"
  },
  {
    "objectID": "examples/australiandomestictourism-permbu-intervals.html#aggregate-bottom-time-series",
    "href": "examples/australiandomestictourism-permbu-intervals.html#aggregate-bottom-time-series",
    "title": "PERMBU",
    "section": "Aggregate bottom time series",
    "text": "Aggregate bottom time series\nIn this example we will use the Tourism dataset from the Forecasting: Principles and Practice book. The dataset only contains the time series at the lowest level, so we need to create the time series for all hierarchies.\n\nY_df = pd.read_csv('https://raw.githubusercontent.com/Nixtla/transfer-learning-time-series/main/datasets/tourism.csv')\nY_df = Y_df.rename({'Trips': 'y', 'Quarter': 'ds'}, axis=1)\nY_df.insert(0, 'Country', 'Australia')\nY_df = Y_df[['Country', 'Region', 'State', 'Purpose', 'ds', 'y']]\nY_df['ds'] = Y_df['ds'].str.replace(r'(\\d+) (Q\\d)', r'\\1-\\2', regex=True)\nY_df['ds'] = pd.to_datetime(Y_df['ds'])\nY_df.head()\n\n\n\n\n\n\n\n\nCountry\nRegion\nState\nPurpose\nds\ny\n\n\n\n\n0\nAustralia\nAdelaide\nSouth Australia\nBusiness\n1998-01-01\n135.077690\n\n\n1\nAustralia\nAdelaide\nSouth Australia\nBusiness\n1998-04-01\n109.987316\n\n\n2\nAustralia\nAdelaide\nSouth Australia\nBusiness\n1998-07-01\n166.034687\n\n\n3\nAustralia\nAdelaide\nSouth Australia\nBusiness\n1998-10-01\n127.160464\n\n\n4\nAustralia\nAdelaide\nSouth Australia\nBusiness\n1999-01-01\n137.448533\n\n\n\n\n\n\n\nThe dataset can be grouped in the following strictly hierarchical structure.\n\nspec = [\n    ['Country'],\n    ['Country', 'State'], \n    ['Country', 'State', 'Region']\n]\n\nUsing the aggregate function from HierarchicalForecast we can get the full set of time series.\n\nY_df, S_df, tags = aggregate(df=Y_df, spec=spec)\nY_df = Y_df.reset_index()\n\n/Users/fedex/miniconda3/envs/hierarchicalforecast/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:828: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n  warnings.warn(\n\n\n\nY_df.head()\n\n\n\n\n\n\n\n\nunique_id\nds\ny\n\n\n\n\n0\nAustralia\n1998-01-01\n23182.197269\n\n\n1\nAustralia\n1998-04-01\n20323.380067\n\n\n2\nAustralia\n1998-07-01\n19826.640511\n\n\n3\nAustralia\n1998-10-01\n20830.129891\n\n\n4\nAustralia\n1999-01-01\n22087.353380\n\n\n\n\n\n\n\n\nS_df.iloc[:5, :5]\n\n\n\n\n\n\n\n\nAustralia/ACT/Canberra\nAustralia/New South Wales/Blue Mountains\nAustralia/New South Wales/Capital Country\nAustralia/New South Wales/Central Coast\nAustralia/New South Wales/Central NSW\n\n\n\n\nAustralia\n1.0\n1.0\n1.0\n1.0\n1.0\n\n\nAustralia/ACT\n1.0\n0.0\n0.0\n0.0\n0.0\n\n\nAustralia/New South Wales\n0.0\n1.0\n1.0\n1.0\n1.0\n\n\nAustralia/Northern Territory\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\nAustralia/Queensland\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n\n\n\n\ntags['Country/State']\n\narray(['Australia/ACT', 'Australia/New South Wales',\n       'Australia/Northern Territory', 'Australia/Queensland',\n       'Australia/South Australia', 'Australia/Tasmania',\n       'Australia/Victoria', 'Australia/Western Australia'], dtype=object)\n\n\nWe can visualize the S matrix and the data using the HierarchicalPlot class as follows.\n\nhplot = HierarchicalPlot(S=S_df, tags=tags)\n\n\nhplot.plot_summing_matrix()\n\n\n\n\n\nhplot.plot_hierarchically_linked_series(\n    bottom_series='Australia/ACT/Canberra',\n    Y_df=Y_df.set_index('unique_id')\n)\n\n\n\n\n\nSplit Train/Test sets\nWe use the final two years (8 quarters) as test set.\n\nY_test_df = Y_df.groupby('unique_id').tail(8)\nY_train_df = Y_df.drop(Y_test_df.index)\n\n\nY_test_df = Y_test_df.set_index('unique_id')\nY_train_df = Y_train_df.set_index('unique_id')\n\n\nY_train_df.groupby('unique_id').size()\n\nunique_id\nAustralia                                                 72\nAustralia/ACT                                             72\nAustralia/ACT/Canberra                                    72\nAustralia/New South Wales                                 72\nAustralia/New South Wales/Blue Mountains                  72\n                                                          ..\nAustralia/Western Australia/Australia's Coral Coast       72\nAustralia/Western Australia/Australia's Golden Outback    72\nAustralia/Western Australia/Australia's North West        72\nAustralia/Western Australia/Australia's South West        72\nAustralia/Western Australia/Experience Perth              72\nLength: 85, dtype: int64"
  },
  {
    "objectID": "examples/australiandomestictourism-permbu-intervals.html#computing-base-forecasts",
    "href": "examples/australiandomestictourism-permbu-intervals.html#computing-base-forecasts",
    "title": "PERMBU",
    "section": "Computing base forecasts",
    "text": "Computing base forecasts\nThe following cell computes the base forecasts for each time series in Y_df using the AutoARIMA and model. Observe that Y_hat_df contains the forecasts but they are not coherent. To reconcile the prediction intervals we need to calculate the uncoherent intervals using the level argument of StatsForecast.\n\nfcst = StatsForecast(df=Y_train_df,\n                     models=[AutoARIMA(season_length=4)], \n                     freq='QS', n_jobs=-1)\nY_hat_df = fcst.forecast(h=8, fitted=True, level=[80, 90])\nY_fitted_df = fcst.forecast_fitted_values()"
  },
  {
    "objectID": "examples/australiandomestictourism-permbu-intervals.html#reconcile-forecasts-and-compute-prediction-intervals-using-permbu",
    "href": "examples/australiandomestictourism-permbu-intervals.html#reconcile-forecasts-and-compute-prediction-intervals-using-permbu",
    "title": "PERMBU",
    "section": "Reconcile forecasts and compute prediction intervals using PERMBU",
    "text": "Reconcile forecasts and compute prediction intervals using PERMBU\nThe following cell makes the previous forecasts coherent using the HierarchicalReconciliation class. In this example we use BottomUp and MinTrace. If you want to calculate prediction intervals, you have to use the level argument as follows and also intervals_method='permbu'.\n\nreconcilers = [\n    BottomUp(),\n    MinTrace(method='mint_shrink'),\n    MinTrace(method='ols')\n]\nhrec = HierarchicalReconciliation(reconcilers=reconcilers)\nY_rec_df = hrec.reconcile(Y_hat_df=Y_hat_df, Y_df=Y_fitted_df,\n                          S=S_df, tags=tags,\n                          level=[80, 90], intervals_method='permbu')\n\n/Users/fedex/miniconda3/envs/hierarchicalforecast/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:828: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n  warnings.warn(\n/Users/fedex/miniconda3/envs/hierarchicalforecast/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:828: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n  warnings.warn(\n/Users/fedex/miniconda3/envs/hierarchicalforecast/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:828: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n  warnings.warn(\n/Users/fedex/miniconda3/envs/hierarchicalforecast/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:828: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n  warnings.warn(\n/Users/fedex/miniconda3/envs/hierarchicalforecast/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:828: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n  warnings.warn(\n/Users/fedex/miniconda3/envs/hierarchicalforecast/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:828: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n  warnings.warn(\n\n\nThe dataframe Y_rec_df contains the reconciled forecasts.\n\nY_rec_df.head()\n\n\n\n\n\n\n\n\nds\nAutoARIMA\nAutoARIMA-lo-90\nAutoARIMA-lo-80\nAutoARIMA-hi-80\nAutoARIMA-hi-90\nAutoARIMA/BottomUp\nAutoARIMA/BottomUp-lo-90\nAutoARIMA/BottomUp-lo-80\nAutoARIMA/BottomUp-hi-80\n...\nAutoARIMA/MinTrace_method-mint_shrink\nAutoARIMA/MinTrace_method-mint_shrink-lo-90\nAutoARIMA/MinTrace_method-mint_shrink-lo-80\nAutoARIMA/MinTrace_method-mint_shrink-hi-80\nAutoARIMA/MinTrace_method-mint_shrink-hi-90\nAutoARIMA/MinTrace_method-ols\nAutoARIMA/MinTrace_method-ols-lo-90\nAutoARIMA/MinTrace_method-ols-lo-80\nAutoARIMA/MinTrace_method-ols-hi-80\nAutoARIMA/MinTrace_method-ols-hi-90\n\n\nunique_id\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAustralia\n2016-01-01\n26212.554688\n24694.224609\n25029.580078\n27395.527344\n27730.884766\n24865.636719\n24106.802510\n24373.962043\n25423.566450\n...\n25395.411928\n24733.046633\n24824.274681\n25939.345007\n25998.692460\n26133.758953\n25516.484518\n25600.644926\n26662.923204\n26855.585562\n\n\nAustralia\n2016-04-01\n25033.667969\n23324.066406\n23701.669922\n26365.666016\n26743.269531\n23247.097656\n22696.597930\n22821.256357\n23830.632567\n...\n23986.272540\n23289.811432\n23525.630785\n24563.998645\n24739.826238\n24934.260399\n24402.570904\n24481.968560\n25567.085565\n25696.312229\n\n\nAustralia\n2016-07-01\n24507.027344\n22625.500000\n23041.076172\n25972.978516\n26388.554688\n22658.207031\n21816.988906\n22011.905035\n23158.311619\n...\n23345.821184\n22688.605574\n22780.602574\n23934.244609\n24033.906220\n24374.026569\n23539.673724\n23797.836651\n24893.463090\n25098.321828\n\n\nAustralia\n2016-10-01\n25598.929688\n23559.919922\n24010.281250\n27187.578125\n27637.937500\n23330.804688\n22567.948299\n22694.449708\n23850.068162\n...\n24275.423420\n23392.040447\n23626.165662\n24828.207813\n24958.926002\n25477.951913\n24793.436993\n24911.271547\n26006.244161\n26152.362329\n\n\nAustralia\n2017-01-01\n26982.578125\n24651.535156\n25166.396484\n28798.757812\n29313.619141\n24497.001953\n23578.418503\n23731.657437\n25114.564017\n...\n25485.433879\n24549.969625\n24802.819691\n26073.792872\n26284.826386\n26842.564741\n26037.725561\n26248.171831\n27436.222761\n27668.067666\n\n\n\n\n5 rows × 21 columns"
  },
  {
    "objectID": "examples/australiandomestictourism-permbu-intervals.html#plot-forecasts",
    "href": "examples/australiandomestictourism-permbu-intervals.html#plot-forecasts",
    "title": "PERMBU",
    "section": "Plot forecasts",
    "text": "Plot forecasts\nThen we can plot the probabilist forecasts using the following function.\n\nplot_df = pd.concat([Y_df.set_index(['unique_id', 'ds']), \n                     Y_rec_df.set_index('ds', append=True)], axis=1)\nplot_df = plot_df.reset_index('ds')\n\n\nPlot single time series\n\nhplot.plot_series(\n    series='Australia',\n    Y_df=plot_df, \n    models=['y', 'AutoARIMA', \n            'AutoARIMA/MinTrace_method-ols',\n            'AutoARIMA/BottomUp'\n           ],\n    level=[80]\n)\n\n\n\n\n\n\nPlot hierarchichally linked time series\n\nhplot.plot_hierarchically_linked_series(\n    bottom_series='Australia/Western Australia/Experience Perth',\n    Y_df=plot_df, \n    models=['y', 'AutoARIMA', 'AutoARIMA/MinTrace_method-ols', 'AutoARIMA/BottomUp'],\n    level=[80]\n)\n\n\n\n\n\n# ACT only has Canberra\nhplot.plot_hierarchically_linked_series(\n    bottom_series='Australia/ACT/Canberra',\n    Y_df=plot_df, \n    models=['y', 'AutoARIMA/MinTrace_method-mint_shrink'],\n    level=[80, 90]\n)\n\n\n\n\n\n\nReferences\n\nHyndman, R.J., & Athanasopoulos, G. (2021). “Forecasting: principles and practice, 3rd edition: Chapter 11: Forecasting hierarchical and grouped series.”. OTexts: Melbourne, Australia. OTexts.com/fpp3 Accessed on July 2022.\nShanika L. Wickramasuriya, George Athanasopoulos, and Rob J. Hyndman. Optimal forecast reconciliation for hierarchical and grouped time series through trace minimization.Journal of the American Statistical Association, 114(526):804–819, 2019. doi: 10.1080/01621459.2018.1448825. URL https://robjhyndman.com/publications/mint/."
  },
  {
    "objectID": "examples/nonnegativereconciliation.html",
    "href": "examples/nonnegativereconciliation.html",
    "title": "Non-Negative MinTrace",
    "section": "",
    "text": "Large collections of time series organized into structures at different aggregation levels often require their forecasts to follow their aggregation constraints and to be nonnegative, which poses the challenge of creating novel algorithms capable of coherent forecasts.\nThe HierarchicalForecast package provides a wide collection of Python implementations of hierarchical forecasting algorithms that follow nonnegative hierarchical reconciliation.\nIn this notebook, we will show how to use the HierarchicalForecast package to perform nonnegative reconciliation of forecasts on Wiki2 dataset.\nYou can run these experiments using CPU or GPU with Google Colab.\n!pip install hierarchicalforecast statsforecast datasetsforecast\nIf you find the code useful, please ⭐ us on Github"
  },
  {
    "objectID": "examples/nonnegativereconciliation.html#load-data",
    "href": "examples/nonnegativereconciliation.html#load-data",
    "title": "Non-Negative MinTrace",
    "section": "1. Load Data",
    "text": "1. Load Data\nIn this example we will use the Wiki2 dataset. The following cell gets the time series for the different levels in the hierarchy, the summing dataframe S_df which recovers the full dataset from the bottom level hierarchy and the indices of each hierarchy denoted by tags.\n\nimport numpy as np\nimport pandas as pd\n\nfrom datasetsforecast.hierarchical import HierarchicalData\n\n\nY_df, S_df, tags = HierarchicalData.load('./data', 'Wiki2')\nY_df['ds'] = pd.to_datetime(Y_df['ds'])\n\n\nY_df.head()\n\n\n\n\n\n\n\n\nunique_id\nds\ny\n\n\n\n\n0\nTotal\n2016-01-01\n156508\n\n\n1\nTotal\n2016-01-02\n129902\n\n\n2\nTotal\n2016-01-03\n138203\n\n\n3\nTotal\n2016-01-04\n115017\n\n\n4\nTotal\n2016-01-05\n126042\n\n\n\n\n\n\n\n\nS_df.iloc[:5, :5]\n\n\n\n\n\n\n\n\nde_AAC_AAG_001\nde_AAC_AAG_010\nde_AAC_AAG_014\nde_AAC_AAG_045\nde_AAC_AAG_063\n\n\n\n\nTotal\n1\n1\n1\n1\n1\n\n\nde\n1\n1\n1\n1\n1\n\n\nen\n0\n0\n0\n0\n0\n\n\nfr\n0\n0\n0\n0\n0\n\n\nja\n0\n0\n0\n0\n0\n\n\n\n\n\n\n\n\ntags\n\n{'Views': array(['Total'], dtype=object),\n 'Views/Country': array(['de', 'en', 'fr', 'ja', 'ru', 'zh'], dtype=object),\n 'Views/Country/Access': array(['de_AAC', 'de_DES', 'de_MOB', 'en_AAC', 'en_DES', 'en_MOB',\n        'fr_AAC', 'fr_DES', 'fr_MOB', 'ja_AAC', 'ja_DES', 'ja_MOB',\n        'ru_AAC', 'ru_DES', 'ru_MOB', 'zh_AAC', 'zh_DES', 'zh_MOB'],\n       dtype=object),\n 'Views/Country/Access/Agent': array(['de_AAC_AAG', 'de_AAC_SPD', 'de_DES_AAG', 'de_MOB_AAG',\n        'en_AAC_AAG', 'en_AAC_SPD', 'en_DES_AAG', 'en_MOB_AAG',\n        'fr_AAC_AAG', 'fr_AAC_SPD', 'fr_DES_AAG', 'fr_MOB_AAG',\n        'ja_AAC_AAG', 'ja_AAC_SPD', 'ja_DES_AAG', 'ja_MOB_AAG',\n        'ru_AAC_AAG', 'ru_AAC_SPD', 'ru_DES_AAG', 'ru_MOB_AAG',\n        'zh_AAC_AAG', 'zh_AAC_SPD', 'zh_DES_AAG', 'zh_MOB_AAG'],\n       dtype=object),\n 'Views/Country/Access/Agent/Topic': array(['de_AAC_AAG_001', 'de_AAC_AAG_010', 'de_AAC_AAG_014',\n        'de_AAC_AAG_045', 'de_AAC_AAG_063', 'de_AAC_AAG_100',\n        'de_AAC_AAG_110', 'de_AAC_AAG_123', 'de_AAC_AAG_143',\n        'de_AAC_SPD_012', 'de_AAC_SPD_074', 'de_AAC_SPD_080',\n        'de_AAC_SPD_105', 'de_AAC_SPD_115', 'de_AAC_SPD_133',\n        'de_DES_AAG_064', 'de_DES_AAG_116', 'de_DES_AAG_131',\n        'de_MOB_AAG_015', 'de_MOB_AAG_020', 'de_MOB_AAG_032',\n        'de_MOB_AAG_059', 'de_MOB_AAG_062', 'de_MOB_AAG_088',\n        'de_MOB_AAG_095', 'de_MOB_AAG_109', 'de_MOB_AAG_122',\n        'de_MOB_AAG_149', 'en_AAC_AAG_044', 'en_AAC_AAG_049',\n        'en_AAC_AAG_075', 'en_AAC_AAG_114', 'en_AAC_AAG_119',\n        'en_AAC_AAG_141', 'en_AAC_SPD_004', 'en_AAC_SPD_011',\n        'en_AAC_SPD_026', 'en_AAC_SPD_048', 'en_AAC_SPD_067',\n        'en_AAC_SPD_126', 'en_AAC_SPD_140', 'en_DES_AAG_016',\n        'en_DES_AAG_024', 'en_DES_AAG_042', 'en_DES_AAG_069',\n        'en_DES_AAG_082', 'en_DES_AAG_102', 'en_MOB_AAG_018',\n        'en_MOB_AAG_022', 'en_MOB_AAG_101', 'en_MOB_AAG_124',\n        'fr_AAC_AAG_029', 'fr_AAC_AAG_046', 'fr_AAC_AAG_070',\n        'fr_AAC_AAG_087', 'fr_AAC_AAG_098', 'fr_AAC_AAG_104',\n        'fr_AAC_AAG_111', 'fr_AAC_AAG_112', 'fr_AAC_AAG_142',\n        'fr_AAC_SPD_025', 'fr_AAC_SPD_027', 'fr_AAC_SPD_035',\n        'fr_AAC_SPD_077', 'fr_AAC_SPD_084', 'fr_AAC_SPD_097',\n        'fr_AAC_SPD_130', 'fr_DES_AAG_023', 'fr_DES_AAG_043',\n        'fr_DES_AAG_051', 'fr_DES_AAG_058', 'fr_DES_AAG_061',\n        'fr_DES_AAG_091', 'fr_DES_AAG_093', 'fr_DES_AAG_094',\n        'fr_DES_AAG_136', 'fr_MOB_AAG_006', 'fr_MOB_AAG_030',\n        'fr_MOB_AAG_066', 'fr_MOB_AAG_117', 'fr_MOB_AAG_120',\n        'fr_MOB_AAG_121', 'fr_MOB_AAG_135', 'fr_MOB_AAG_147',\n        'ja_AAC_AAG_038', 'ja_AAC_AAG_047', 'ja_AAC_AAG_055',\n        'ja_AAC_AAG_076', 'ja_AAC_AAG_099', 'ja_AAC_AAG_128',\n        'ja_AAC_AAG_132', 'ja_AAC_AAG_134', 'ja_AAC_AAG_137',\n        'ja_AAC_SPD_013', 'ja_AAC_SPD_034', 'ja_AAC_SPD_050',\n        'ja_AAC_SPD_060', 'ja_AAC_SPD_078', 'ja_AAC_SPD_106',\n        'ja_DES_AAG_079', 'ja_DES_AAG_081', 'ja_DES_AAG_113',\n        'ja_MOB_AAG_065', 'ja_MOB_AAG_073', 'ja_MOB_AAG_092',\n        'ja_MOB_AAG_127', 'ja_MOB_AAG_129', 'ja_MOB_AAG_144',\n        'ru_AAC_AAG_008', 'ru_AAC_AAG_145', 'ru_AAC_AAG_146',\n        'ru_AAC_SPD_000', 'ru_AAC_SPD_090', 'ru_AAC_SPD_148',\n        'ru_DES_AAG_003', 'ru_DES_AAG_007', 'ru_DES_AAG_017',\n        'ru_DES_AAG_041', 'ru_DES_AAG_071', 'ru_DES_AAG_072',\n        'ru_MOB_AAG_002', 'ru_MOB_AAG_040', 'ru_MOB_AAG_083',\n        'ru_MOB_AAG_086', 'ru_MOB_AAG_103', 'ru_MOB_AAG_107',\n        'ru_MOB_AAG_118', 'ru_MOB_AAG_125', 'zh_AAC_AAG_021',\n        'zh_AAC_AAG_033', 'zh_AAC_AAG_037', 'zh_AAC_AAG_052',\n        'zh_AAC_AAG_057', 'zh_AAC_AAG_085', 'zh_AAC_AAG_108',\n        'zh_AAC_SPD_039', 'zh_AAC_SPD_096', 'zh_DES_AAG_009',\n        'zh_DES_AAG_019', 'zh_DES_AAG_053', 'zh_DES_AAG_054',\n        'zh_DES_AAG_056', 'zh_DES_AAG_068', 'zh_DES_AAG_089',\n        'zh_DES_AAG_139', 'zh_MOB_AAG_005', 'zh_MOB_AAG_028',\n        'zh_MOB_AAG_031', 'zh_MOB_AAG_036', 'zh_MOB_AAG_138'], dtype=object)}\n\n\nWe split the dataframe in train/test splits.\n\nY_test_df = Y_df.groupby('unique_id').tail(7)\nY_train_df = Y_df.drop(Y_test_df.index)\n\n\nY_test_df = Y_test_df.set_index('unique_id')\nY_train_df = Y_train_df.set_index('unique_id')"
  },
  {
    "objectID": "examples/nonnegativereconciliation.html#base-forecasts",
    "href": "examples/nonnegativereconciliation.html#base-forecasts",
    "title": "Non-Negative MinTrace",
    "section": "2. Base Forecasts",
    "text": "2. Base Forecasts\nThe following cell computes the base forecast for each time series using the ETS and naive models. Observe that Y_hat_df contains the forecasts but they are not coherent.\n\nfrom statsforecast.models import ETS, Naive\nfrom statsforecast.core import StatsForecast\n\n\nfcst = StatsForecast(\n    df=Y_train_df, \n    models=[ETS(season_length=7, model='ZAA'), Naive()], \n    freq='D', \n    n_jobs=-1\n)\nY_hat_df = fcst.forecast(h=7)\n\nObserve that the ETS model computes negative forecasts for some series.\n\nY_hat_df.query('ETS &lt; 0')\n\n\n\n\n\n\n\n\nds\nETS\nNaive\n\n\nunique_id\n\n\n\n\n\n\n\nde_AAC_AAG_001\n2016-12-25\n-487.601532\n340.0\n\n\nde_AAC_AAG_001\n2016-12-26\n-215.634201\n340.0\n\n\nde_AAC_AAG_001\n2016-12-27\n-173.175613\n340.0\n\n\nde_AAC_AAG_001\n2016-12-30\n-290.836060\n340.0\n\n\nde_AAC_AAG_001\n2016-12-31\n-784.441040\n340.0\n\n\n...\n...\n...\n...\n\n\nzh_AAC_AAG_033\n2016-12-31\n-86.526421\n37.0\n\n\nzh_MOB\n2016-12-26\n-199.534882\n1036.0\n\n\nzh_MOB\n2016-12-27\n-69.527260\n1036.0\n\n\nzh_MOB_AAG\n2016-12-26\n-199.534882\n1036.0\n\n\nzh_MOB_AAG\n2016-12-27\n-69.527260\n1036.0\n\n\n\n\n99 rows × 3 columns"
  },
  {
    "objectID": "examples/nonnegativereconciliation.html#non-negative-reconciliation",
    "href": "examples/nonnegativereconciliation.html#non-negative-reconciliation",
    "title": "Non-Negative MinTrace",
    "section": "3. Non-Negative Reconciliation",
    "text": "3. Non-Negative Reconciliation\nThe following cell makes the previous forecasts coherent and nonnegative using the HierarchicalReconciliation class.\n\nfrom hierarchicalforecast.methods import MinTrace\nfrom hierarchicalforecast.core import HierarchicalReconciliation\n\n\nreconcilers = [\n    MinTrace(method='ols'),\n    MinTrace(method='ols', nonnegative=True)\n]\nhrec = HierarchicalReconciliation(reconcilers=reconcilers)\nY_rec_df = hrec.reconcile(Y_hat_df=Y_hat_df, Y_df=Y_train_df,\n                          S=S_df, tags=tags)\n\nObserve that the nonnegative reconciliation method obtains nonnegative forecasts.\n\nY_rec_df.query('`ETS/MinTrace_method-ols_nonnegative-True` &lt; 0')\n\n\n\n\n\n\n\n\nds\nETS\nNaive\nETS/MinTrace_method-ols\nNaive/MinTrace_method-ols\nETS/MinTrace_method-ols_nonnegative-True\nNaive/MinTrace_method-ols_nonnegative-True\n\n\nunique_id\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe free reconciliation method gets negative forecasts.\n\nY_rec_df.query('`ETS/MinTrace_method-ols` &lt; 0')\n\n\n\n\n\n\n\n\nds\nETS\nNaive\nETS/MinTrace_method-ols\nNaive/MinTrace_method-ols\nETS/MinTrace_method-ols_nonnegative-True\nNaive/MinTrace_method-ols_nonnegative-True\n\n\nunique_id\n\n\n\n\n\n\n\n\n\n\n\nde_DES\n2016-12-25\n-2553.932861\n495.0\n-3468.745214\n495.0\n2.262540e-15\n495.0\n\n\nde_DES\n2016-12-26\n-2155.228271\n495.0\n-2985.587125\n495.0\n1.356705e-30\n495.0\n\n\nde_DES\n2016-12-27\n-2720.993896\n495.0\n-3698.680055\n495.0\n6.857413e-30\n495.0\n\n\nde_DES\n2016-12-29\n-3429.432617\n495.0\n-2965.207609\n495.0\n2.456449e+02\n495.0\n\n\nde_DES\n2016-12-30\n-3963.202637\n495.0\n-3217.360371\n495.0\n3.646790e+02\n495.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\nzh_MOB_AAG_036\n2016-12-26\n75.298317\n115.0\n-165.799776\n115.0\n3.207772e-14\n115.0\n\n\nzh_MOB_AAG_036\n2016-12-27\n72.895554\n115.0\n-134.340626\n115.0\n2.308198e-14\n115.0\n\n\nzh_MOB_AAG_138\n2016-12-25\n94.796623\n65.0\n-47.009813\n65.0\n3.116938e-14\n65.0\n\n\nzh_MOB_AAG_138\n2016-12-26\n71.293983\n65.0\n-169.804110\n65.0\n0.000000e+00\n65.0\n\n\nzh_MOB_AAG_138\n2016-12-27\n62.049744\n65.0\n-145.186436\n65.0\n0.000000e+00\n65.0\n\n\n\n\n240 rows × 7 columns"
  },
  {
    "objectID": "examples/nonnegativereconciliation.html#evaluation",
    "href": "examples/nonnegativereconciliation.html#evaluation",
    "title": "Non-Negative MinTrace",
    "section": "4. Evaluation",
    "text": "4. Evaluation\nThe HierarchicalForecast package includes the HierarchicalEvaluation class to evaluate the different hierarchies and also is capable of compute scaled metrics compared to a benchmark model.\n\nfrom hierarchicalforecast.evaluation import HierarchicalEvaluation\n\n\ndef mse(y, y_hat):\n    return np.mean((y-y_hat)**2)\n\nevaluator = HierarchicalEvaluation(evaluators=[mse])\nevaluation = evaluator.evaluate(\n        Y_hat_df=Y_rec_df, Y_test_df=Y_test_df, \n        tags=tags, benchmark='Naive'\n)\nevaluation.filter(like='ETS', axis=1).T\n\n\n\n\n\n\n\nlevel\nOverall\nViews\nViews/Country\nViews/Country/Access\nViews/Country/Access/Agent\nViews/Country/Access/Agent/Topic\n\n\nmetric\nmse-scaled\nmse-scaled\nmse-scaled\nmse-scaled\nmse-scaled\nmse-scaled\n\n\n\n\nETS\n1.011585\n0.7358\n1.190354\n1.103657\n1.089515\n1.397139\n\n\nETS/MinTrace_method-ols\n0.979163\n0.698355\n1.062521\n1.143277\n1.113349\n1.354041\n\n\nETS/MinTrace_method-ols_nonnegative-True\n0.945075\n0.677892\n1.004639\n1.184719\n1.141442\n1.158672\n\n\n\n\n\n\n\nObserve that the nonnegative reconciliation method performs better that its unconstrained counterpart.\n\nReferences\n\nHyndman, R.J., & Athanasopoulos, G. (2021). “Forecasting: principles and practice, 3rd edition: Chapter 11: Forecasting hierarchical and grouped series.”. OTexts: Melbourne, Australia. OTexts.com/fpp3 Accessed on July 2022.\nWickramasuriya, S. L., Athanasopoulos, G., & Hyndman, R. J. (2019). \"Optimal forecast reconciliation for hierarchical and grouped time series through trace minimization\". Journal of the American Statistical Association, 114 , 804–819. doi:10.1080/01621459.2018.1448825..\nWickramasuriya, S.L., Turlach, B.A. & Hyndman, R.J. (2020). \"Optimal non-negative forecast reconciliation”. Stat Comput 30, 1167–1182, https://doi.org/10.1007/s11222-020-09930-0."
  },
  {
    "objectID": "examples/australianprisonpopulation.html",
    "href": "examples/australianprisonpopulation.html",
    "title": "Geographical Aggregation (Prison Population)",
    "section": "",
    "text": "In many applications, a set of time series is hierarchically organized. Examples include the presence of geographic levels, products, or categories that define different types of aggregations. In such scenarios, forecasters are often required to provide predictions for all disaggregate and aggregate series. A natural desire is for those predictions to be “coherent”, that is, for the bottom series to add up precisely to the forecasts of the aggregated series.\nIn this notebook we present an example on how to use HierarchicalForecast to produce coherent forecasts between geographical levels. We will use the Australian Prison Population dataset.\nWe will first load the dataset and produce base forecasts using an ETS model from StatsForecast, and then reconciliate the forecasts with several reconciliation algorithms from HierarchicalForecast. Finally, we show the performance is comparable with the results reported by the Forecasting: Principles and Practice which uses the R package fable.\nYou can run these experiments using CPU or GPU with Google Colab.\n!pip install hierarchicalforecast\n!pip install -U statsforecast numba\nIf you find the code useful, please ⭐ us on Github"
  },
  {
    "objectID": "examples/australianprisonpopulation.html#load-and-process-data",
    "href": "examples/australianprisonpopulation.html#load-and-process-data",
    "title": "Geographical Aggregation (Prison Population)",
    "section": "1. Load and Process Data",
    "text": "1. Load and Process Data\nThe dataset only contains the time series at the lowest level, so we need to create the time series for all hierarchies.\n\nimport numpy as np\nimport pandas as pd\n\n\nY_df = pd.read_csv('https://OTexts.com/fpp3/extrafiles/prison_population.csv')\nY_df = Y_df.rename({'Count': 'y', 'Date': 'ds'}, axis=1)\nY_df.insert(0, 'Country', 'Australia')\nY_df = Y_df[['Country', 'State', 'Gender', 'Legal', 'Indigenous', 'ds', 'y']]\nY_df['ds'] = pd.to_datetime(Y_df['ds'])\nY_df.head()\n\n\n\n\n\n\n\n\nCountry\nState\nGender\nLegal\nIndigenous\nds\ny\n\n\n\n\n0\nAustralia\nACT\nFemale\nRemanded\nATSI\n2005-03-01\n0\n\n\n1\nAustralia\nACT\nFemale\nRemanded\nNon-ATSI\n2005-03-01\n2\n\n\n2\nAustralia\nACT\nFemale\nSentenced\nATSI\n2005-03-01\n0\n\n\n3\nAustralia\nACT\nFemale\nSentenced\nNon-ATSI\n2005-03-01\n5\n\n\n4\nAustralia\nACT\nMale\nRemanded\nATSI\n2005-03-01\n7\n\n\n\n\n\n\n\nThe dataset can be grouped in the following grouped structure.\n\nhiers = [\n    ['Country'],\n    ['Country', 'State'], \n    ['Country', 'Gender'], \n    ['Country', 'Legal'], \n    ['Country', 'State', 'Gender', 'Legal']\n]\n\nUsing the aggregate function from HierarchicalForecast we can get the full set of time series.\n\nfrom hierarchicalforecast.utils import aggregate\n\n\nY_df, S_df, tags = aggregate(Y_df, hiers)\nY_df['y'] = Y_df['y']/1e3\nY_df = Y_df.reset_index()\n\n\nY_df.head()\n\n\n\n\n\n\n\n\nunique_id\nds\ny\n\n\n\n\n0\nAustralia\n2005-03-01\n24.296\n\n\n1\nAustralia\n2005-06-01\n24.643\n\n\n2\nAustralia\n2005-09-01\n24.511\n\n\n3\nAustralia\n2005-12-01\n24.393\n\n\n4\nAustralia\n2006-03-01\n24.524\n\n\n\n\n\n\n\n\nS_df.iloc[:5, :5]\n\n\n\n\n\n\n\n\nAustralia/ACT/Female/Remanded\nAustralia/ACT/Female/Sentenced\nAustralia/ACT/Male/Remanded\nAustralia/ACT/Male/Sentenced\nAustralia/NSW/Female/Remanded\n\n\n\n\nAustralia\n1.0\n1.0\n1.0\n1.0\n1.0\n\n\nAustralia/ACT\n1.0\n1.0\n1.0\n1.0\n0.0\n\n\nAustralia/NSW\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\nAustralia/NT\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\nAustralia/QLD\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n\n\n\n\ntags\n\n{'Country': array(['Australia'], dtype=object),\n 'Country/State': array(['Australia/ACT', 'Australia/NSW', 'Australia/NT', 'Australia/QLD',\n        'Australia/SA', 'Australia/TAS', 'Australia/VIC', 'Australia/WA'],\n       dtype=object),\n 'Country/Gender': array(['Australia/Female', 'Australia/Male'], dtype=object),\n 'Country/Legal': array(['Australia/Remanded', 'Australia/Sentenced'], dtype=object),\n 'Country/State/Gender/Legal': ['Australia/ACT/Female/Remanded',\n  'Australia/ACT/Female/Sentenced',\n  'Australia/ACT/Male/Remanded',\n  'Australia/ACT/Male/Sentenced',\n  'Australia/NSW/Female/Remanded',\n  'Australia/NSW/Female/Sentenced',\n  'Australia/NSW/Male/Remanded',\n  'Australia/NSW/Male/Sentenced',\n  'Australia/NT/Female/Remanded',\n  'Australia/NT/Female/Sentenced',\n  'Australia/NT/Male/Remanded',\n  'Australia/NT/Male/Sentenced',\n  'Australia/QLD/Female/Remanded',\n  'Australia/QLD/Female/Sentenced',\n  'Australia/QLD/Male/Remanded',\n  'Australia/QLD/Male/Sentenced',\n  'Australia/SA/Female/Remanded',\n  'Australia/SA/Female/Sentenced',\n  'Australia/SA/Male/Remanded',\n  'Australia/SA/Male/Sentenced',\n  'Australia/TAS/Female/Remanded',\n  'Australia/TAS/Female/Sentenced',\n  'Australia/TAS/Male/Remanded',\n  'Australia/TAS/Male/Sentenced',\n  'Australia/VIC/Female/Remanded',\n  'Australia/VIC/Female/Sentenced',\n  'Australia/VIC/Male/Remanded',\n  'Australia/VIC/Male/Sentenced',\n  'Australia/WA/Female/Remanded',\n  'Australia/WA/Female/Sentenced',\n  'Australia/WA/Male/Remanded',\n  'Australia/WA/Male/Sentenced']}\n\n\n\nSplit Train/Test sets\nWe use the final two years (8 quarters) as test set.\n\nY_test_df = Y_df.groupby('unique_id').tail(8)\nY_train_df = Y_df.drop(Y_test_df.index)\n\n\nY_test_df = Y_test_df.set_index('unique_id')\nY_train_df = Y_train_df.set_index('unique_id')"
  },
  {
    "objectID": "examples/australianprisonpopulation.html#computing-base-forecasts",
    "href": "examples/australianprisonpopulation.html#computing-base-forecasts",
    "title": "Geographical Aggregation (Prison Population)",
    "section": "2. Computing base forecasts",
    "text": "2. Computing base forecasts\nThe following cell computes the base forecasts for each time series in Y_df using the ETS model. Observe that Y_hat_df contains the forecasts but they are not coherent.\n\nfrom statsforecast.models import ETS\nfrom statsforecast.core import StatsForecast\n\n\nfcst = StatsForecast(df=Y_train_df,\n                     models=[ETS(season_length=4, model='ZMZ')], \n                     freq='QS', n_jobs=-1)\nY_hat_df = fcst.forecast(h=8, fitted=True)\nY_fitted_df = fcst.forecast_fitted_values()"
  },
  {
    "objectID": "examples/australianprisonpopulation.html#reconcile-forecasts",
    "href": "examples/australianprisonpopulation.html#reconcile-forecasts",
    "title": "Geographical Aggregation (Prison Population)",
    "section": "3. Reconcile forecasts",
    "text": "3. Reconcile forecasts\nThe following cell makes the previous forecasts coherent using the HierarchicalReconciliation class. Since the hierarchy structure is not strict, we can’t use methods such as TopDown or MiddleOut. In this example we use BottomUp and MinTrace.\n\nfrom hierarchicalforecast.methods import BottomUp, MinTrace\nfrom hierarchicalforecast.core import HierarchicalReconciliation\n\n\nreconcilers = [\n    BottomUp(),\n    MinTrace(method='mint_shrink')\n]\nhrec = HierarchicalReconciliation(reconcilers=reconcilers)\nY_rec_df = hrec.reconcile(Y_hat_df=Y_hat_df, Y_df=Y_fitted_df, S=S_df, tags=tags)\n\nThe dataframe Y_rec_df contains the reconciled forecasts.\n\nY_rec_df.head()\n\n\n\n\n\n\n\n\nds\nETS\nETS/BottomUp\nETS/MinTrace_method-mint_shrink\n\n\nunique_id\n\n\n\n\n\n\n\n\nAustralia\n2015-01-01\n34.799496\n34.933891\n34.927244\n\n\nAustralia\n2015-04-01\n35.192638\n35.473560\n35.440861\n\n\nAustralia\n2015-07-01\n35.188217\n35.687363\n35.476427\n\n\nAustralia\n2015-10-01\n35.888626\n36.010685\n35.946153\n\n\nAustralia\n2016-01-01\n36.045437\n36.400101\n36.244707"
  },
  {
    "objectID": "examples/australianprisonpopulation.html#evaluation",
    "href": "examples/australianprisonpopulation.html#evaluation",
    "title": "Geographical Aggregation (Prison Population)",
    "section": "4. Evaluation",
    "text": "4. Evaluation\nThe HierarchicalForecast package includes the HierarchicalEvaluation class to evaluate the different hierarchies and also is capable of compute scaled metrics compared to a benchmark model.\n\nfrom hierarchicalforecast.evaluation import HierarchicalEvaluation\n\n\ndef mase(y, y_hat, y_insample, seasonality=4):\n    errors = np.mean(np.abs(y - y_hat), axis=1)\n    scale = np.mean(np.abs(y_insample[:, seasonality:] - y_insample[:, :-seasonality]), axis=1)\n    return np.mean(errors / scale)\n\neval_tags = {}\neval_tags['Total'] = tags['Country']\neval_tags['State'] = tags['Country/State']\neval_tags['Legal status'] = tags['Country/Legal']\neval_tags['Gender'] = tags['Country/Gender']\neval_tags['Bottom'] = tags['Country/State/Gender/Legal']\neval_tags['All series'] = np.concatenate(list(tags.values()))\n\nevaluator = HierarchicalEvaluation(evaluators=[mase])\nevaluation = evaluator.evaluate(\n    Y_hat_df=Y_rec_df, Y_test_df=Y_test_df,\n    tags=eval_tags,\n    Y_df=Y_train_df\n)\nevaluation = evaluation.reset_index().drop(columns='metric').drop(0).set_index('level')\nevaluation.columns = ['Base', 'BottomUp', 'MinTrace(mint_shrink)']\nevaluation.applymap('{:.2f}'.format)\n\n\n\n\n\n\n\n\nBase\nBottomUp\nMinTrace(mint_shrink)\n\n\nlevel\n\n\n\n\n\n\n\nTotal\n1.36\n1.02\n1.16\n\n\nState\n1.54\n1.57\n1.61\n\n\nLegal status\n2.40\n2.50\n2.40\n\n\nGender\n1.08\n0.81\n0.95\n\n\nBottom\n2.17\n2.17\n2.16\n\n\nAll series\n2.00\n2.00\n2.00\n\n\n\n\n\n\n\n\nFable Comparison\nObserve that we can recover the results reported by the Forecasting: Principles and Practice book. The original results were calculated using the R package fable.\n\n\n\nFable’s reconciliation results\n\n\n\n\nReferences\n\nHyndman, R.J., & Athanasopoulos, G. (2021). “Forecasting: principles and practice, 3rd edition: Chapter 11: Forecasting hierarchical and grouped series.”. OTexts: Melbourne, Australia. OTexts.com/fpp3 Accessed on July 2022.\nRob Hyndman, Alan Lee, Earo Wang, Shanika Wickramasuriya, and Maintainer Earo Wang (2021). “hts: Hierarchical and Grouped Time Series”. URL https://CRAN.R-project.org/package=hts. R package version 0.3.1.\nMitchell O’Hara-Wild, Rob Hyndman, Earo Wang, Gabriel Caceres, Tim-Gunnar Hensel, and Timothy Hyndman (2021). “fable: Forecasting Models for Tidy Time Series”. URL https://CRAN.R-project.org/package=fable. R package version 6.0.2."
  },
  {
    "objectID": "examples/installation.html",
    "href": "examples/installation.html",
    "title": "Install",
    "section": "",
    "text": "You can install the released version of HierachicalForecast from the Python package index with:\npip install hierarchicalforecast\nor\nconda install -c conda-forge hierarchicalforecast\n\n\n\n\n\n\nTip\n\n\n\nWe recommend installing your libraries inside a python virtual or conda environment.\n\n\n\nUser our env (optional)\nIf you don’t have a Conda environment and need tools like Numba, Pandas, NumPy, Jupyter, StatsModels, and Nbdev you can use ours by following these steps:\n\nClone the HierachicalForecast repo:\n\n$ git clone https://github.com/Nixtla/hierachicalforecast.git && cd hierachicalforecast\n\nCreate the environment using the environment.yml file:\n\n$ conda env create -f environment.yml\n\nActivate the environment:\n\n$ conda activate statsforecast\n\n\n\n\nIf you find the code useful, please ⭐ us on Github"
  },
  {
    "objectID": "examples/australiandomestictourism.html",
    "href": "examples/australiandomestictourism.html",
    "title": "Geographical Aggregation (Tourism)",
    "section": "",
    "text": "In many applications, a set of time series is hierarchically organized. Examples include the presence of geographic levels, products, or categories that define different types of aggregations. In such scenarios, forecasters are often required to provide predictions for all disaggregate and aggregate series. A natural desire is for those predictions to be “coherent”, that is, for the bottom series to add up precisely to the forecasts of the aggregated series.\nIn this notebook we present an example on how to use HierarchicalForecast to produce coherent forecasts between geographical levels. We will use the classic Australian Domestic Tourism (Tourism) dataset, which contains monthly time series of the number of visitors to each state of Australia.\nWe will first load the Tourism data and produce base forecasts using an ETS model from StatsForecast, and then reconciliate the forecasts with several reconciliation algorithms from HierarchicalForecast. Finally, we show the performance is comparable with the results reported by the Forecasting: Principles and Practice which uses the R package fable.\nYou can run these experiments using CPU or GPU with Google Colab.\n!pip install hierarchicalforecast\n!pip install -U statsforecast numba\nIf you find the code useful, please ⭐ us on Github"
  },
  {
    "objectID": "examples/australiandomestictourism.html#load-and-process-data",
    "href": "examples/australiandomestictourism.html#load-and-process-data",
    "title": "Geographical Aggregation (Tourism)",
    "section": "1. Load and Process Data",
    "text": "1. Load and Process Data\nIn this example we will use the Tourism dataset from the Forecasting: Principles and Practice book.\nThe dataset only contains the time series at the lowest level, so we need to create the time series for all hierarchies.\n\nimport numpy as np\nimport pandas as pd\n\n\nY_df = pd.read_csv('https://raw.githubusercontent.com/Nixtla/transfer-learning-time-series/main/datasets/tourism.csv')\nY_df = Y_df.rename({'Trips': 'y', 'Quarter': 'ds'}, axis=1)\nY_df.insert(0, 'Country', 'Australia')\nY_df = Y_df[['Country', 'Region', 'State', 'Purpose', 'ds', 'y']]\nY_df['ds'] = Y_df['ds'].str.replace(r'(\\d+) (Q\\d)', r'\\1-\\2', regex=True)\nY_df['ds'] = pd.to_datetime(Y_df['ds'])\nY_df.head()\n\n\n\n\n\n\n\n\nCountry\nRegion\nState\nPurpose\nds\ny\n\n\n\n\n0\nAustralia\nAdelaide\nSouth Australia\nBusiness\n1998-01-01\n135.077690\n\n\n1\nAustralia\nAdelaide\nSouth Australia\nBusiness\n1998-04-01\n109.987316\n\n\n2\nAustralia\nAdelaide\nSouth Australia\nBusiness\n1998-07-01\n166.034687\n\n\n3\nAustralia\nAdelaide\nSouth Australia\nBusiness\n1998-10-01\n127.160464\n\n\n4\nAustralia\nAdelaide\nSouth Australia\nBusiness\n1999-01-01\n137.448533\n\n\n\n\n\n\n\nThe dataset can be grouped in the following non-strictly hierarchical structure.\n\nspec = [\n    ['Country'],\n    ['Country', 'State'], \n    ['Country', 'Purpose'], \n    ['Country', 'State', 'Region'], \n    ['Country', 'State', 'Purpose'], \n    ['Country', 'State', 'Region', 'Purpose']\n]\n\nUsing the aggregate function from HierarchicalForecast we can get the full set of time series.\n\nfrom hierarchicalforecast.utils import aggregate\n\n\nY_df, S_df, tags = aggregate(Y_df, spec)\nY_df = Y_df.reset_index()\n\n\nY_df.head()\n\n\n\n\n\n\n\n\nunique_id\nds\ny\n\n\n\n\n0\nAustralia\n1998-01-01\n23182.197269\n\n\n1\nAustralia\n1998-04-01\n20323.380067\n\n\n2\nAustralia\n1998-07-01\n19826.640511\n\n\n3\nAustralia\n1998-10-01\n20830.129891\n\n\n4\nAustralia\n1999-01-01\n22087.353380\n\n\n\n\n\n\n\n\nS_df.iloc[:5, :5]\n\n\n\n\n\n\n\n\nAustralia/ACT/Canberra/Business\nAustralia/ACT/Canberra/Holiday\nAustralia/ACT/Canberra/Other\nAustralia/ACT/Canberra/Visiting\nAustralia/New South Wales/Blue Mountains/Business\n\n\n\n\nAustralia\n1.0\n1.0\n1.0\n1.0\n1.0\n\n\nAustralia/ACT\n1.0\n1.0\n1.0\n1.0\n0.0\n\n\nAustralia/New South Wales\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\nAustralia/Northern Territory\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\nAustralia/Queensland\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n\n\n\n\ntags['Country/Purpose']\n\narray(['Australia/Business', 'Australia/Holiday', 'Australia/Other',\n       'Australia/Visiting'], dtype=object)\n\n\n\nSplit Train/Test sets\nWe use the final two years (8 quarters) as test set.\n\nY_test_df = Y_df.groupby('unique_id').tail(8)\nY_train_df = Y_df.drop(Y_test_df.index)\n\n\nY_test_df = Y_test_df.set_index('unique_id')\nY_train_df = Y_train_df.set_index('unique_id')\n\n\nY_train_df.groupby('unique_id').size()\n\nunique_id\nAustralia                                                72\nAustralia/ACT                                            72\nAustralia/ACT/Business                                   72\nAustralia/ACT/Canberra                                   72\nAustralia/ACT/Canberra/Business                          72\n                                                         ..\nAustralia/Western Australia/Experience Perth/Other       72\nAustralia/Western Australia/Experience Perth/Visiting    72\nAustralia/Western Australia/Holiday                      72\nAustralia/Western Australia/Other                        72\nAustralia/Western Australia/Visiting                     72\nLength: 425, dtype: int64"
  },
  {
    "objectID": "examples/australiandomestictourism.html#computing-base-forecasts",
    "href": "examples/australiandomestictourism.html#computing-base-forecasts",
    "title": "Geographical Aggregation (Tourism)",
    "section": "2. Computing base forecasts",
    "text": "2. Computing base forecasts\nThe following cell computes the base forecasts for each time series in Y_df using the ETS model. Observe that Y_hat_df contains the forecasts but they are not coherent.\n\nfrom statsforecast.models import ETS\nfrom statsforecast.core import StatsForecast\n\n\nfcst = StatsForecast(df=Y_train_df, \n                     models=[ETS(season_length=4, model='ZZA')], \n                     freq='QS', n_jobs=-1)\nY_hat_df = fcst.forecast(h=8, fitted=True)\nY_fitted_df = fcst.forecast_fitted_values()"
  },
  {
    "objectID": "examples/australiandomestictourism.html#reconcile-forecasts",
    "href": "examples/australiandomestictourism.html#reconcile-forecasts",
    "title": "Geographical Aggregation (Tourism)",
    "section": "3. Reconcile forecasts",
    "text": "3. Reconcile forecasts\nThe following cell makes the previous forecasts coherent using the HierarchicalReconciliation class. Since the hierarchy structure is not strict, we can’t use methods such as TopDown or MiddleOut. In this example we use BottomUp and MinTrace.\n\nfrom hierarchicalforecast.methods import BottomUp, MinTrace\nfrom hierarchicalforecast.core import HierarchicalReconciliation\n\n\nreconcilers = [\n    BottomUp(),\n    MinTrace(method='mint_shrink'),\n    MinTrace(method='ols')\n]\nhrec = HierarchicalReconciliation(reconcilers=reconcilers)\nY_rec_df = hrec.reconcile(Y_hat_df=Y_hat_df, Y_df=Y_fitted_df, S=S_df, tags=tags)\n\nThe dataframe Y_rec_df contains the reconciled forecasts.\n\nY_rec_df.head()\n\n\n\n\n\n\n\n\nds\nETS\nETS/BottomUp\nETS/MinTrace_method-mint_shrink\nETS/MinTrace_method-ols\n\n\nunique_id\n\n\n\n\n\n\n\n\n\nAustralia\n2016-01-01\n25990.068359\n24379.679688\n25438.888351\n25894.418893\n\n\nAustralia\n2016-04-01\n24458.490234\n22902.664062\n23925.188541\n24357.230480\n\n\nAustralia\n2016-07-01\n23974.056641\n22412.984375\n23440.310338\n23865.929521\n\n\nAustralia\n2016-10-01\n24563.455078\n23127.638672\n24101.001833\n24470.783968\n\n\nAustralia\n2017-01-01\n25990.068359\n24516.175781\n25556.667616\n25901.382401"
  },
  {
    "objectID": "examples/australiandomestictourism.html#evaluation",
    "href": "examples/australiandomestictourism.html#evaluation",
    "title": "Geographical Aggregation (Tourism)",
    "section": "4. Evaluation",
    "text": "4. Evaluation\nThe HierarchicalForecast package includes the HierarchicalEvaluation class to evaluate the different hierarchies and also is capable of compute scaled metrics compared to a benchmark model.\n\nfrom hierarchicalforecast.evaluation import HierarchicalEvaluation\n\n\ndef rmse(y, y_hat):\n    return np.mean(np.sqrt(np.mean((y-y_hat)**2, axis=1)))\n\ndef mase(y, y_hat, y_insample, seasonality=4):\n    errors = np.mean(np.abs(y - y_hat), axis=1)\n    scale = np.mean(np.abs(y_insample[:, seasonality:] - y_insample[:, :-seasonality]), axis=1)\n    return np.mean(errors / scale)\n\neval_tags = {}\neval_tags['Total'] = tags['Country']\neval_tags['Purpose'] = tags['Country/Purpose']\neval_tags['State'] = tags['Country/State']\neval_tags['Regions'] = tags['Country/State/Region']\neval_tags['Bottom'] = tags['Country/State/Region/Purpose']\neval_tags['All'] = np.concatenate(list(tags.values()))\n\nevaluator = HierarchicalEvaluation(evaluators=[rmse, mase])\nevaluation = evaluator.evaluate(\n        Y_hat_df=Y_rec_df, Y_test_df=Y_test_df,\n        tags=eval_tags, Y_df=Y_train_df\n)\nevaluation = evaluation.drop('Overall')\nevaluation.columns = ['Base', 'BottomUp', 'MinTrace(mint_shrink)', 'MinTrace(ols)']\nevaluation = evaluation.applymap('{:.2f}'.format)\n\n/var/folders/rp/97y9_3ns23v01hdn0rp9ndw40000gp/T/ipykernel_46857/2768439279.py:22: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.\n  evaluation = evaluation.drop('Overall')\n\n\n\nRMSE\nThe following table shows the performance measured using RMSE across levels for each reconciliation method.\n\nevaluation.query('metric == \"rmse\"')\n\n\n\n\n\n\n\n\n\nBase\nBottomUp\nMinTrace(mint_shrink)\nMinTrace(ols)\n\n\nlevel\nmetric\n\n\n\n\n\n\n\n\nTotal\nrmse\n1743.29\n3028.93\n2102.47\n1818.94\n\n\nPurpose\nrmse\n534.75\n791.28\n574.84\n515.53\n\n\nState\nrmse\n308.15\n413.43\n315.89\n287.34\n\n\nRegions\nrmse\n51.65\n55.14\n46.48\n46.29\n\n\nBottom\nrmse\n19.37\n19.37\n17.78\n18.19\n\n\nAll\nrmse\n45.19\n54.95\n44.59\n42.71\n\n\n\n\n\n\n\n\n\nMASE\nThe following table shows the performance measured using MASE across levels for each reconciliation method.\n\nevaluation.query('metric == \"mase\"')\n\n\n\n\n\n\n\n\n\nBase\nBottomUp\nMinTrace(mint_shrink)\nMinTrace(ols)\n\n\nlevel\nmetric\n\n\n\n\n\n\n\n\nTotal\nmase\n1.59\n3.16\n2.05\n1.67\n\n\nPurpose\nmase\n1.32\n2.28\n1.48\n1.25\n\n\nState\nmase\n1.39\n1.90\n1.39\n1.25\n\n\nRegions\nmase\n1.12\n1.19\n1.01\n0.99\n\n\nBottom\nmase\n0.98\n0.98\n0.94\n1.01\n\n\nAll\nmase\n1.03\n1.08\n0.98\n1.02\n\n\n\n\n\n\n\n\n\nComparison fable\nObserve that we can recover the results reported by the Forecasting: Principles and Practice. The original results were calculated using the R package fable.\n\n\n\nFable’s reconciliation results\n\n\n\n\nReferences\n\nHyndman, R.J., & Athanasopoulos, G. (2021). “Forecasting: principles and practice, 3rd edition: Chapter 11: Forecasting hierarchical and grouped series.”. OTexts: Melbourne, Australia. OTexts.com/fpp3 Accessed on July 2022.\nRob Hyndman, Alan Lee, Earo Wang, Shanika Wickramasuriya, and Maintainer Earo Wang (2021). “hts: Hierarchical and Grouped Time Series”. URL https://CRAN.R-project.org/package=hts. R package version 0.3.1.\nMitchell O’Hara-Wild, Rob Hyndman, Earo Wang, Gabriel Caceres, Tim-Gunnar Hensel, and Timothy Hyndman (2021). “fable: Forecasting Models for Tidy Time Series”. URL https://CRAN.R-project.org/package=fable. R package version 6.0.2."
  },
  {
    "objectID": "utils.html",
    "href": "utils.html",
    "title": "Aggregation/Visualization Utils",
    "section": "",
    "text": "The HierarchicalForecast package contains utility functions to wrangle and visualize hierarchical series datasets. The aggregate function of the module allows you to create a hierarchy from categorical variables representing the structure levels, returning also the aggregation contraints matrix \\(\\mathbf{S}\\).\nIn addition, HierarchicalForecast ensures compatibility of its reconciliation methods with other popular machine-learning libraries via its external forecast adapters that transform output base forecasts from external libraries into a compatible data frame format.\n\nAggregate Function\n\nsource\n\naggregate\n\n aggregate (df:pandas.core.frame.DataFrame, spec:List[List[str]],\n            is_balanced:bool=False, sparse_s:bool=False)\n\nUtils Aggregation Function. Aggregates bottom level series contained in the pandas DataFrame df according to levels defined in the spec list.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\nDataFrame\n\nDataframe with columns ['ds', 'y'] and columns to aggregate.\n\n\nspec\ntyping.List[typing.List[str]]\n\nList of levels. Each element of the list should contain a list of columns of df to aggregate.\n\n\nis_balanced\nbool\nFalse\nDeprecated.\n\n\nsparse_s\nbool\nFalse\nReturn S_df as a sparse dataframe.\n\n\nReturns\npandas DataFrame\n\nHierarchically structured series.\n\n\n\n\n\n\nHierarchical Visualization\n\nsource\n\nHierarchicalPlot\n\n HierarchicalPlot (S:pandas.core.frame.DataFrame,\n                   tags:Dict[str,numpy.ndarray])\n\nHierarchical Plot\nThis class contains a collection of matplotlib visualization methods, suited for small to medium sized hierarchical series.\nParameters: S: pd.DataFrame with summing matrix of size (base, bottom), see aggregate function. tags: np.ndarray, with hierarchical aggregation indexes, where each key is a level and its value contains tags associated to that level.\n\nsource\n\n\nplot_summing_matrix\n\n plot_summing_matrix ()\n\nSummation Constraints plot\nThis method simply plots the hierarchical aggregation constraints matrix \\(\\mathbf{S}\\).\n\nsource\n\n\nplot_series\n\n plot_series (series:str, Y_df:Optional[pandas.core.frame.DataFrame]=None,\n              models:Optional[List[str]]=None,\n              level:Optional[List[int]]=None)\n\nSingle Series plot\nParameters: series: str, string identifying the 'unique_id' any-level series to plot. Y_df: pd.DataFrame, hierarchically structured series (\\(\\mathbf{y}_{[a,b]}\\)). It contains columns ['unique_id', 'ds', 'y'], it may have 'models'. models: List[str], string identifying filtering model columns. level: float list 0-100, confidence levels for prediction intervals available in Y_df.\nReturns: Single series plot with filtered models and prediction interval level.\n\nsource\n\n\nplot_hierarchically_linked_series\n\n plot_hierarchically_linked_series (bottom_series:str,\n                                    Y_df:Optional[pandas.core.frame.DataFr\n                                    ame]=None,\n                                    models:Optional[List[str]]=None,\n                                    level:Optional[List[int]]=None)\n\nHierarchically Linked Series plot\nParameters: bottom_series: str, string identifying the 'unique_id' bottom-level series to plot. Y_df: pd.DataFrame, hierarchically structured series (\\(\\mathbf{y}_{[a,b]}\\)). It contains columns [‘unique_id’, ‘ds’, ‘y’] and models.  models: List[str], string identifying filtering model columns. level: float list 0-100, confidence levels for prediction intervals available in Y_df.\nReturns: Collection of hierarchilly linked series plots associated with the bottom_series and filtered models and prediction interval level.\n\nsource\n\n\nplot_hierarchical_predictions_gap\n\n plot_hierarchical_predictions_gap (Y_df:pandas.core.frame.DataFrame,\n                                    models:Optional[List[str]]=None,\n                                    xlabel:Optional=None,\n                                    ylabel:Optional=None)\n\nHierarchically Predictions Gap plot\nParameters: Y_df: pd.DataFrame, hierarchically structured series (\\(\\mathbf{y}_{[a,b]}\\)). It contains columns [‘unique_id’, ‘ds’, ‘y’] and models.  models: List[str], string identifying filtering model columns. xlabel: str, string for the plot’s x axis label. ylable: str, string for the plot’s y axis label.\nReturns: Plots of aggregated predictions at different levels of the hierarchical structure. The aggregation is performed according to the tag levels see aggregate function.\n\nfrom statsforecast.core import StatsForecast\nfrom statsforecast.models import AutoARIMA, ETS, Naive\nfrom datasetsforecast.hierarchical import HierarchicalData\n\nY_df, S, tags = HierarchicalData.load('./data', 'Labour')\nY_df['ds'] = pd.to_datetime(Y_df['ds'])\n\nY_test_df  = Y_df.groupby('unique_id').tail(24)\nY_train_df = Y_df.drop(Y_test_df.index)\nY_test_df  = Y_test_df.set_index('unique_id')\nY_train_df = Y_train_df.set_index('unique_id')\n\nfcst = StatsForecast(\n    df=Y_train_df, \n    #models=[AutoARIMA(season_length=12), Naive()], \n    models=[ETS(season_length=12, model='AAZ')],\n    freq='MS', \n    n_jobs=-1\n)\nY_hat_df = fcst.forecast(h=24)\n\n# Plot prediction difference of different aggregation\n# Levels Country, Country/Region, Country/Gender/Region ...\nhplots = HierarchicalPlot(S=S, tags=tags)\n\nhplots.plot_hierarchical_predictions_gap(\n    Y_df=Y_hat_df, models='ETS',\n    xlabel='Month', ylabel='Predictions',\n)\n\n\n\n\nExternal Forecast Adapters\n\nsource\n\nsamples_to_quantiles_df\n\n samples_to_quantiles_df (samples:numpy.ndarray, unique_ids:Iterable[str],\n                          dates:Iterable,\n                          quantiles:Optional[Iterable[float]]=None,\n                          level:Optional[Iterable[int]]=None,\n                          model_name:Optional[str]='model')\n\nTransform Random Samples into HierarchicalForecast input. Auxiliary function to create compatible HierarchicalForecast input Y_hat_df dataframe.\nParameters: samples: numpy array. Samples from forecast distribution of shape [n_series, n_samples, horizon]. unique_ids: string list. Unique identifiers for each time series. dates: datetime list. List of forecast dates. quantiles: float list in [0., 1.]. Alternative to level, quantiles to estimate from y distribution. level: int list in [0,100]. Probability levels for prediction intervals. model_name: string. Name of forecasting model.\nReturns: quantiles: float list in [0., 1.]. quantiles to estimate from y distribution . Y_hat_df: pd.DataFrame. With base quantile forecasts with columns ds and models to reconcile indexed by unique_id.\n\n\n\n\n\nIf you find the code useful, please ⭐ us on Github"
  },
  {
    "objectID": "methods.html",
    "href": "methods.html",
    "title": "Reconciliation Methods",
    "section": "",
    "text": "Large collections of time series organized into structures at different aggregation levels often require their forecasts to follow their aggregation constraints, which poses the challenge of creating novel algorithms capable of coherent forecasts.  The HierarchicalForecast package provides the most comprehensive collection of Python implementations of hierarchical forecasting algorithms that follow classic hierarchical reconciliation. All the methods have a reconcile function capable of reconcile base forecasts using numpy arrays.\nMost reconciliation methods can be described by the following convenient linear algebra notation:\n\\[\\tilde{\\mathbf{y}}_{[a,b],\\tau} = \\mathbf{S}_{[a,b][b]} \\mathbf{P}_{[b][a,b]} \\hat{\\mathbf{y}}_{[a,b],\\tau}\\]\nwhere \\(a, b\\) represent the aggregate and bottom levels, \\(\\mathbf{S}_{[a,b][b]}\\) contains the hierarchical aggregation constraints, and \\(\\mathbf{P}_{[b][a,b]}\\) varies across reconciliation methods. The reconciled predictions are \\(\\tilde{\\mathbf{y}}_{[a,b],\\tau}\\), and the base predictions \\(\\hat{\\mathbf{y}}_{[a,b],\\tau}\\).\n\n1. Bottom-Up\n\nsource\n\nBottomUpSparse\n\n BottomUpSparse ()\n\nBottomUpSparse Reconciliation Class.\nThis is the implementation of a Bottom Up reconciliation using the sparse matrix approach. It works much more efficient on datasets with many time series. [makoren: At least I hope so, I only checked up until ~20k time series, and there’s no real improvement, it would be great to check for smth like 1M time series, where the dense S matrix really stops fitting in memory]\nSee the parent class for more details.\n\nsource\n\n\nBottomUp\n\n BottomUp ()\n\nBottom Up Reconciliation Class. The most basic hierarchical reconciliation is performed using an Bottom-Up strategy. It was proposed for the first time by Orcutt in 1968. The corresponding hierarchical “projection” matrix is defined as: \\[\\mathbf{P}_{\\text{BU}} = [\\mathbf{0}_{\\mathrm{[b],[a]}}\\;|\\;\\mathbf{I}_{\\mathrm{[b][b]}}]\\]\nParameters: None\nReferences: - Orcutt, G.H., Watts, H.W., & Edwards, J.B.(1968). “Data aggregation and information loss”. The American Economic Review, 58 , 773(787).\n\nsource\n\n\nBottomUp.fit\n\n BottomUp.fit (S:numpy.ndarray, y_hat:numpy.ndarray,\n               idx_bottom:numpy.ndarray,\n               y_insample:Optional[numpy.ndarray]=None,\n               y_hat_insample:Optional[numpy.ndarray]=None,\n               sigmah:Optional[numpy.ndarray]=None,\n               intervals_method:Optional[str]=None,\n               num_samples:Optional[int]=None, seed:Optional[int]=None,\n               tags:Dict[str,numpy.ndarray]=None)\n\nBottom Up Fit Method.\nParameters: S: Summing matrix of size (base, bottom). y_hat: Forecast values of size (base, horizon). idx_bottom: Indices corresponding to the bottom level of S, size (bottom). level: float list 0-100, confidence levels for prediction intervals. intervals_method: Sampler for prediction intevals, one of normality, bootstrap, permbu. **sampler_kwargs: Coherent sampler instantiation arguments.\nReturns: self: object, fitted reconciler.\n\nsource\n\n\nBottomUp.predict\n\n BottomUp.predict (S:numpy.ndarray, y_hat:numpy.ndarray,\n                   level:Optional[List[int]]=None)\n\nPredict using reconciler.\nPredict using fitted mean and probabilistic reconcilers.\nParameters: S: Summing matrix of size (base, bottom). y_hat: Forecast values of size (base, horizon). level: float list 0-100, confidence levels for prediction intervals.\nReturns: y_tilde: Reconciliated predictions.\n\nsource\n\n\nBottomUp.fit_predict\n\n BottomUp.fit_predict (S:numpy.ndarray, y_hat:numpy.ndarray,\n                       idx_bottom:numpy.ndarray,\n                       y_insample:Optional[numpy.ndarray]=None,\n                       y_hat_insample:Optional[numpy.ndarray]=None,\n                       sigmah:Optional[numpy.ndarray]=None,\n                       level:Optional[List[int]]=None,\n                       intervals_method:Optional[str]=None,\n                       num_samples:Optional[int]=None,\n                       seed:Optional[int]=None,\n                       tags:Dict[str,numpy.ndarray]=None)\n\nBottomUp Reconciliation Method.\nParameters: S: Summing matrix of size (base, bottom). y_hat: Forecast values of size (base, horizon). idx_bottom: Indices corresponding to the bottom level of S, size (bottom). level: float list 0-100, confidence levels for prediction intervals. intervals_method: Sampler for prediction intevals, one of normality, bootstrap, permbu. **sampler_kwargs: Coherent sampler instantiation arguments.\nReturns: y_tilde: Reconciliated y_hat using the Bottom Up approach.\n\nsource\n\n\nBottomUp.sample\n\n BottomUp.sample (num_samples:int)\n\nSample probabilistic coherent distribution.\nGenerates n samples from a probabilistic coherent distribution. The method uses fitted mean and probabilistic reconcilers, defined by the intervals_method selected during the reconciler’s instantiation. Currently available: normality, bootstrap, permbu.\nParameters: num_samples: int, number of samples generated from coherent distribution.\nReturns: samples: Coherent samples of size (num_series, horizon, num_samples).\n\n\n\n2. Top-Down\n\nsource\n\nTopDown\n\n TopDown (method:str)\n\nTop Down Reconciliation Class.\nThe Top Down hierarchical reconciliation method, distributes the total aggregate predictions and decomposes it down the hierarchy using proportions \\(\\mathbf{p}_{\\mathrm{[b]}}\\) that can be actual historical values or estimated.\n\\[\\mathbf{P}=[\\mathbf{p}_{\\mathrm{[b]}}\\;|\\;\\mathbf{0}_{\\mathrm{[b][a,b\\;-1]}}]\\] Parameters: method: One of forecast_proportions, average_proportions and proportion_averages.\nReferences: - CW. Gross (1990). “Disaggregation methods to expedite product line forecasting”. Journal of Forecasting, 9 , 233–254. doi:10.1002/for.3980090304. - G. Fliedner (1999). “An investigation of aggregate variable time series forecast strategies with specific subaggregate time series statistical correlation”. Computers and Operations Research, 26 , 1133–1149. doi:10.1016/S0305-0548(99)00017-9.\n\nsource\n\n\nTopDown.fit\n\n TopDown.fit (S, y_hat, y_insample:Optional[numpy.ndarray]=None,\n              y_hat_insample:Optional[numpy.ndarray]=None,\n              sigmah:Optional[numpy.ndarray]=None,\n              intervals_method:Optional[str]=None,\n              num_samples:Optional[int]=None, seed:Optional[int]=None,\n              tags:Dict[str,numpy.ndarray]=None,\n              idx_bottom:Optional[numpy.ndarray]=None)\n\nTopDown Fit Method.\nParameters: S: Summing matrix of size (base, bottom). y_hat: Forecast values of size (base, horizon). tags: Each key is a level and each value its S indices. y_insample: Insample values of size (base, insample_size). Optional for forecast_proportions method. idx_bottom: Indices corresponding to the bottom level of S, size (bottom). level: float list 0-100, confidence levels for prediction intervals. intervals_method: Sampler for prediction intevals, one of normality, bootstrap, permbu. **sampler_kwargs: Coherent sampler instantiation arguments.\nReturns: self: object, fitted reconciler.\n\nsource\n\n\nTopDown.predict\n\n TopDown.predict (S:numpy.ndarray, y_hat:numpy.ndarray,\n                  level:Optional[List[int]]=None)\n\nPredict using reconciler.\nPredict using fitted mean and probabilistic reconcilers.\nParameters: S: Summing matrix of size (base, bottom). y_hat: Forecast values of size (base, horizon). level: float list 0-100, confidence levels for prediction intervals.\nReturns: y_tilde: Reconciliated predictions.\n\nsource\n\n\nTopDown.fit_predict\n\n TopDown.fit_predict (S:numpy.ndarray, y_hat:numpy.ndarray,\n                      tags:Dict[str,numpy.ndarray],\n                      idx_bottom:numpy.ndarray=None,\n                      y_insample:Optional[numpy.ndarray]=None,\n                      y_hat_insample:Optional[numpy.ndarray]=None,\n                      sigmah:Optional[numpy.ndarray]=None,\n                      level:Optional[List[int]]=None,\n                      intervals_method:Optional[str]=None,\n                      num_samples:Optional[int]=None,\n                      seed:Optional[int]=None)\n\nTop Down Reconciliation Method.\nParameters: S: Summing matrix of size (base, bottom). y_hat: Forecast values of size (base, horizon). tags: Each key is a level and each value its S indices. y_insample: Insample values of size (base, insample_size). Optional for forecast_proportions method. idx_bottom: Indices corresponding to the bottom level of S, size (bottom). level: float list 0-100, confidence levels for prediction intervals. intervals_method: Sampler for prediction intevals, one of normality, bootstrap, permbu. **sampler_kwargs: Coherent sampler instantiation arguments.\nReturns: y_tilde: Reconciliated y_hat using the Top Down approach.\n\nsource\n\n\nTopDown.sample\n\n TopDown.sample (num_samples:int)\n\nSample probabilistic coherent distribution.\nGenerates n samples from a probabilistic coherent distribution. The method uses fitted mean and probabilistic reconcilers, defined by the intervals_method selected during the reconciler’s instantiation. Currently available: normality, bootstrap, permbu.\nParameters: num_samples: int, number of samples generated from coherent distribution.\nReturns: samples: Coherent samples of size (num_series, horizon, num_samples).\n\n\n\n3. Middle-Out\n\nsource\n\nMiddleOut\n\n MiddleOut (middle_level:str, top_down_method:str)\n\nMiddle Out Reconciliation Class.\nThis method is only available for strictly hierarchical structures. It anchors the base predictions in a middle level. The levels above the base predictions use the Bottom-Up approach, while the levels below use a Top-Down.\nParameters: middle_level: Middle level. top_down_method: One of forecast_proportions, average_proportions and proportion_averages.\nReferences: - Hyndman, R.J., & Athanasopoulos, G. (2021). “Forecasting: principles and practice, 3rd edition: Chapter 11: Forecasting hierarchical and grouped series.”. OTexts: Melbourne, Australia. OTexts.com/fpp3 Accessed on July 2022.\n\nsource\n\n\nMiddleOut.fit_predict\n\n MiddleOut.fit_predict (S:numpy.ndarray, y_hat:numpy.ndarray,\n                        tags:Dict[str,numpy.ndarray],\n                        y_insample:Optional[numpy.ndarray]=None,\n                        level:Optional[List[int]]=None,\n                        intervals_method:Optional[str]=None)\n\nMiddle Out Reconciliation Method.\nParameters: S: Summing matrix of size (base, bottom). y_hat: Forecast values of size (base, horizon). tags: Each key is a level and each value its S indices. y_insample: Insample values of size (base, insample_size). Only used for forecast_proportions\nReturns: y_tilde: Reconciliated y_hat using the Middle Out approach.\n\n\n\n4. Min-Trace\n\nsource\n\nMinTraceSparse\n\n MinTraceSparse (method:str, nonnegative:bool=False,\n                 mint_shr_ridge:Optional[float]=2e-08)\n\nMinTraceSparse Reconciliation Class.\nThis is the implementation of a subset of MinTrace features using the sparse matrix approach. It works much more efficient on datasets with many time series.\nSee the parent class for more details.\nCurrently supported: * Methods using diagonal W matrix, i.e. “ols”, “wls_struct”, “wls_var”, * The standard MinT version (non-negative is not supported).\nNote: due to the numerical instability of the matrix inversion when creating the P matrix, the method is NOT guaranteed to give identical results to the non-sparse version.\n\nsource\n\n\nMinTrace\n\n MinTrace (method:str, nonnegative:bool=False,\n           mint_shr_ridge:Optional[float]=2e-08)\n\nMinTrace Reconciliation Class.\nThis reconciliation algorithm proposed by Wickramasuriya et al. depends on a generalized least squares estimator and an estimator of the covariance matrix of the coherency errors \\(\\mathbf{W}_{h}\\). The Min Trace algorithm minimizes the squared errors for the coherent forecasts under an unbiasedness assumption; the solution has a closed form.\n\\[\\mathbf{P}_{\\text{MinT}}=\\left(\\mathbf{S}^{\\intercal}\\mathbf{W}_{h}\\mathbf{S}\\right)^{-1}\n\\mathbf{S}^{\\intercal}\\mathbf{W}^{-1}_{h}\\]\nParameters: method: str, one of ols, wls_struct, wls_var, mint_shrink, mint_cov. nonnegative: bool, reconciled forecasts should be nonnegative? mint_shr_ridge: float=2e-8, ridge numeric protection to MinTrace-shr covariance estimator.\nReferences: - Wickramasuriya, S. L., Athanasopoulos, G., & Hyndman, R. J. (2019). “Optimal forecast reconciliation for hierarchical and grouped time series through trace minimization”. Journal of the American Statistical Association, 114 , 804–819. doi:10.1080/01621459.2018.1448825.. - Wickramasuriya, S.L., Turlach, B.A. & Hyndman, R.J. (2020). “Optimal non-negative forecast reconciliation”. Stat Comput 30, 1167–1182, https://doi.org/10.1007/s11222-020-09930-0.\n\nsource\n\n\nMinTrace.fit\n\n MinTrace.fit (S, y_hat, y_insample:Optional[numpy.ndarray]=None,\n               y_hat_insample:Optional[numpy.ndarray]=None,\n               sigmah:Optional[numpy.ndarray]=None,\n               intervals_method:Optional[str]=None,\n               num_samples:Optional[int]=None, seed:Optional[int]=None,\n               tags:Dict[str,numpy.ndarray]=None,\n               idx_bottom:Optional[numpy.ndarray]=None)\n\nMinTrace Fit Method.\nParameters: S: Summing matrix of size (base, bottom). y_hat: Forecast values of size (base, horizon). tags: Each key is a level and each value its S indices. y_insample: Insample values of size (base, insample_size). Optional for forecast_proportions method. idx_bottom: Indices corresponding to the bottom level of S, size (bottom). level: float list 0-100, confidence levels for prediction intervals. intervals_method: Sampler for prediction intevals, one of normality, bootstrap, permbu. **sampler_kwargs: Coherent sampler instantiation arguments.\nReturns: self: object, fitted reconciler.\n\nsource\n\n\nMinTrace.predict\n\n MinTrace.predict (S:numpy.ndarray, y_hat:numpy.ndarray,\n                   level:Optional[List[int]]=None)\n\nPredict using reconciler.\nPredict using fitted mean and probabilistic reconcilers.\nParameters: S: Summing matrix of size (base, bottom). y_hat: Forecast values of size (base, horizon). level: float list 0-100, confidence levels for prediction intervals.\nReturns: y_tilde: Reconciliated predictions.\n\nsource\n\n\nMinTrace.fit_predict\n\n MinTrace.fit_predict (S:numpy.ndarray, y_hat:numpy.ndarray,\n                       idx_bottom:numpy.ndarray=None,\n                       y_insample:Optional[numpy.ndarray]=None,\n                       y_hat_insample:Optional[numpy.ndarray]=None,\n                       sigmah:Optional[numpy.ndarray]=None,\n                       level:Optional[List[int]]=None,\n                       intervals_method:Optional[str]=None,\n                       num_samples:Optional[int]=None,\n                       seed:Optional[int]=None,\n                       tags:Dict[str,numpy.ndarray]=None)\n\nMinTrace Reconciliation Method.\nParameters: S: Summing matrix of size (base, bottom). y_hat: Forecast values of size (base, horizon). y_insample: Insample values of size (base, insample_size). Only used by wls_var, mint_cov, mint_shrink y_hat_insample: Insample fitted values of size (base, insample_size). Only used by wls_var, mint_cov, mint_shrink idx_bottom: Indices corresponding to the bottom level of S, size (bottom). level: float list 0-100, confidence levels for prediction intervals. sampler: Sampler for prediction intevals, one of normality, bootstrap, permbu.\nReturns: y_tilde: Reconciliated y_hat using the MinTrace approach.\n\nsource\n\n\nMinTrace.sample\n\n MinTrace.sample (num_samples:int)\n\nSample probabilistic coherent distribution.\nGenerates n samples from a probabilistic coherent distribution. The method uses fitted mean and probabilistic reconcilers, defined by the intervals_method selected during the reconciler’s instantiation. Currently available: normality, bootstrap, permbu.\nParameters: num_samples: int, number of samples generated from coherent distribution.\nReturns: samples: Coherent samples of size (num_series, horizon, num_samples).\n\n\n\n5. Optimal Combination\n\nsource\n\nOptimalCombination\n\n OptimalCombination (method:str, nonnegative:bool=False)\n\nOptimal Combination Reconciliation Class.\nThis reconciliation algorithm was proposed by Hyndman et al. 2011, the method uses generalized least squares estimator using the coherency errors covariance matrix. Consider the covariance of the base forecast \\(\\textrm{Var}(\\epsilon_{h}) = \\Sigma_{h}\\), the \\(\\mathbf{P}\\) matrix of this method is defined by: \\[ \\mathbf{P} = \\left(\\mathbf{S}^{\\intercal}\\Sigma_{h}^{\\dagger}\\mathbf{S}\\right)^{-1}\\mathbf{S}^{\\intercal}\\Sigma^{\\dagger}_{h}\\] where \\(\\Sigma_{h}^{\\dagger}\\) denotes the variance pseudo-inverse. The method was later proven equivalent to MinTrace variants.\nParameters: method: str, allowed optimal combination methods: ‘ols’, ‘wls_struct’. nonnegative: bool, reconciled forecasts should be nonnegative?\nReferences: - Rob J. Hyndman, Roman A. Ahmed, George Athanasopoulos, Han Lin Shang (2010). “Optimal Combination Forecasts for Hierarchical Time Series”.. - Shanika L. Wickramasuriya, George Athanasopoulos and Rob J. Hyndman (2010). “Optimal Combination Forecasts for Hierarchical Time Series”.. - Wickramasuriya, S.L., Turlach, B.A. & Hyndman, R.J. (2020). “Optimal non-negative forecast reconciliation”. Stat Comput 30, 1167–1182, https://doi.org/10.1007/s11222-020-09930-0.\n\nsource\n\n\nOptimalCombination.fit\n\n OptimalCombination.fit (S, y_hat,\n                         y_insample:Optional[numpy.ndarray]=None,\n                         y_hat_insample:Optional[numpy.ndarray]=None,\n                         sigmah:Optional[numpy.ndarray]=None,\n                         intervals_method:Optional[str]=None,\n                         num_samples:Optional[int]=None,\n                         seed:Optional[int]=None,\n                         tags:Dict[str,numpy.ndarray]=None,\n                         idx_bottom:Optional[numpy.ndarray]=None)\n\nMinTrace Fit Method.\nParameters: S: Summing matrix of size (base, bottom). y_hat: Forecast values of size (base, horizon). tags: Each key is a level and each value its S indices. y_insample: Insample values of size (base, insample_size). Optional for forecast_proportions method. idx_bottom: Indices corresponding to the bottom level of S, size (bottom). level: float list 0-100, confidence levels for prediction intervals. intervals_method: Sampler for prediction intevals, one of normality, bootstrap, permbu. **sampler_kwargs: Coherent sampler instantiation arguments.\nReturns: self: object, fitted reconciler.\n\nsource\n\n\nOptimalCombination.predict\n\n OptimalCombination.predict (S:numpy.ndarray, y_hat:numpy.ndarray,\n                             level:Optional[List[int]]=None)\n\nPredict using reconciler.\nPredict using fitted mean and probabilistic reconcilers.\nParameters: S: Summing matrix of size (base, bottom). y_hat: Forecast values of size (base, horizon). level: float list 0-100, confidence levels for prediction intervals.\nReturns: y_tilde: Reconciliated predictions.\n\nsource\n\n\nOptimalCombination.fit_predict\n\n OptimalCombination.fit_predict (S:numpy.ndarray, y_hat:numpy.ndarray,\n                                 idx_bottom:numpy.ndarray=None,\n                                 y_insample:Optional[numpy.ndarray]=None, \n                                 y_hat_insample:Optional[numpy.ndarray]=No\n                                 ne, sigmah:Optional[numpy.ndarray]=None,\n                                 level:Optional[List[int]]=None,\n                                 intervals_method:Optional[str]=None,\n                                 num_samples:Optional[int]=None,\n                                 seed:Optional[int]=None,\n                                 tags:Dict[str,numpy.ndarray]=None)\n\nMinTrace Reconciliation Method.\nParameters: S: Summing matrix of size (base, bottom). y_hat: Forecast values of size (base, horizon). y_insample: Insample values of size (base, insample_size). Only used by wls_var, mint_cov, mint_shrink y_hat_insample: Insample fitted values of size (base, insample_size). Only used by wls_var, mint_cov, mint_shrink idx_bottom: Indices corresponding to the bottom level of S, size (bottom). level: float list 0-100, confidence levels for prediction intervals. sampler: Sampler for prediction intevals, one of normality, bootstrap, permbu.\nReturns: y_tilde: Reconciliated y_hat using the MinTrace approach.\n\nsource\n\n\nOptimalCombination.sample\n\n OptimalCombination.sample (num_samples:int)\n\nSample probabilistic coherent distribution.\nGenerates n samples from a probabilistic coherent distribution. The method uses fitted mean and probabilistic reconcilers, defined by the intervals_method selected during the reconciler’s instantiation. Currently available: normality, bootstrap, permbu.\nParameters: num_samples: int, number of samples generated from coherent distribution.\nReturns: samples: Coherent samples of size (num_series, horizon, num_samples).\n\n\n\n6. Emp. Risk Minimization\n\nsource\n\nERM\n\n ERM (method:str, lambda_reg:float=0.01)\n\nOptimal Combination Reconciliation Class.\nThe Empirical Risk Minimization reconciliation strategy relaxes the unbiasedness assumptions from previous reconciliation methods like MinT and optimizes square errors between the reconciled predictions and the validation data to obtain an optimal reconciliation matrix P.\nThe exact solution for \\(\\mathbf{P}\\) (method='closed') follows the expression: \\[\\mathbf{P}^{*} = \\left(\\mathbf{S}^{\\intercal}\\mathbf{S}\\right)^{-1}\\mathbf{Y}^{\\intercal}\\hat{\\mathbf{Y}}\\left(\\hat{\\mathbf{Y}}\\hat{\\mathbf{Y}}\\right)^{-1}\\]\nThe alternative Lasso regularized \\(\\mathbf{P}\\) solution (method='reg_bu') is useful when the observations of validation data is limited or the exact solution has low numerical stability. \\[\\mathbf{P}^{*} = \\text{argmin}_{\\mathbf{P}} ||\\mathbf{Y}-\\mathbf{S} \\mathbf{P} \\hat{Y} ||^{2}_{2} + \\lambda ||\\mathbf{P}-\\mathbf{P}_{\\text{BU}}||_{1}\\]\nParameters: method: str, one of closed, reg and reg_bu. lambda_reg: float, l1 regularizer for reg and reg_bu.\nReferences: - Ben Taieb, S., & Koo, B. (2019). Regularized regression for hierarchical forecasting without unbiasedness conditions. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining KDD ’19 (p. 1337{1347). New York, NY, USA: Association for Computing Machinery..\n\nsource\n\n\nERM.fit\n\n ERM.fit (S, y_hat, y_insample, y_hat_insample,\n          sigmah:Optional[numpy.ndarray]=None,\n          intervals_method:Optional[str]=None,\n          num_samples:Optional[int]=None, seed:Optional[int]=None,\n          tags:Dict[str,numpy.ndarray]=None,\n          idx_bottom:Optional[numpy.ndarray]=None)\n\nERM Fit Method.\nParameters: S: Summing matrix of size (base, bottom). y_hat: Forecast values of size (base, horizon). y_insample: Train values of size (base, insample_size). y_hat_insample: Insample train predictions of size (base, insample_size). idx_bottom: Indices corresponding to the bottom level of S, size (bottom). level: float list 0-100, confidence levels for prediction intervals. intervals_method: Sampler for prediction intevals, one of normality, bootstrap, permbu. **sampler_kwargs: Coherent sampler instantiation arguments.\nReturns: self: object, fitted reconciler.\n\nsource\n\n\nERM.predict\n\n ERM.predict (S:numpy.ndarray, y_hat:numpy.ndarray,\n              level:Optional[List[int]]=None)\n\nPredict using reconciler.\nPredict using fitted mean and probabilistic reconcilers.\nParameters: S: Summing matrix of size (base, bottom). y_hat: Forecast values of size (base, horizon). level: float list 0-100, confidence levels for prediction intervals.\nReturns: y_tilde: Reconciliated predictions.\n\nsource\n\n\nERM.fit_predict\n\n ERM.fit_predict (S:numpy.ndarray, y_hat:numpy.ndarray,\n                  idx_bottom:numpy.ndarray=None,\n                  y_insample:Optional[numpy.ndarray]=None,\n                  y_hat_insample:Optional[numpy.ndarray]=None,\n                  sigmah:Optional[numpy.ndarray]=None,\n                  level:Optional[List[int]]=None,\n                  intervals_method:Optional[str]=None,\n                  num_samples:Optional[int]=None, seed:Optional[int]=None,\n                  tags:Dict[str,numpy.ndarray]=None)\n\nERM Reconciliation Method.\nParameters: S: Summing matrix of size (base, bottom). y_hat: Forecast values of size (base, horizon). y_insample: Train values of size (base, insample_size). y_hat_insample: Insample train predictions of size (base, insample_size). idx_bottom: Indices corresponding to the bottom level of S, size (bottom). level: float list 0-100, confidence levels for prediction intervals. intervals_method: Sampler for prediction intevals, one of normality, bootstrap, permbu.\nReturns: y_tilde: Reconciliated y_hat using the ERM approach.\n\nsource\n\n\nERM.sample\n\n ERM.sample (num_samples:int)\n\nSample probabilistic coherent distribution.\nGenerates n samples from a probabilistic coherent distribution. The method uses fitted mean and probabilistic reconcilers, defined by the intervals_method selected during the reconciler’s instantiation. Currently available: normality, bootstrap, permbu.\nParameters: num_samples: int, number of samples generated from coherent distribution.\nReturns: samples: Coherent samples of size (num_series, horizon, num_samples).\n\nreconciler_args = dict(S=S, \n                       y_hat=y_hat_base,\n                       y_insample=y_base,\n                       y_hat_insample=y_hat_base_insample,\n                       sigmah=sigmah,\n                       level=[80, 90],\n                       intervals_method='normality',\n                       num_samples=200,\n                       seed=0,\n                       tags=tags,\n                       idx_bottom=idx_bottom\n                       )\n\n\n\n\nReferences\n\nGeneral Reconciliation\n\nOrcutt, G.H., Watts, H.W., & Edwards, J.B.(1968). Data aggregation and information loss. The American Economic Review, 58 , 773(787).\nDisaggregation methods to expedite product line forecasting. Journal of Forecasting, 9 , 233–254. doi:10.1002/for.3980090304.\nAn investigation of aggregate variable time series forecast strategies with specific subaggregate time series statistical correlation. Computers and Operations Research, 26 , 1133–1149. doi:10.1016/S0305-0548(99)00017-9.\nHyndman, R.J., & Athanasopoulos, G. (2021). “Forecasting: principles and practice, 3rd edition: Chapter 11: Forecasting hierarchical and grouped series.”. OTexts: Melbourne, Australia. OTexts.com/fpp3 Accessed on July 2022.\n\n\n\nOptimal Reconciliation\n\nRob J. Hyndman, Roman A. Ahmed, George Athanasopoulos, Han Lin Shang. “Optimal Combination Forecasts for Hierarchical Time Series” (2010).\nShanika L. Wickramasuriya, George Athanasopoulos and Rob J. Hyndman. “Optimal Combination Forecasts for Hierarchical Time Series” (2010).\nBen Taieb, S., & Koo, B. (2019). Regularized regression for hierarchical forecasting without unbiasedness conditions. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining KDD ’19 (p. 1337{1347). New York, NY, USA: Association for Computing Machinery.\n\n\n\nHierarchical Probabilistic Coherent Predictions\n\nPuwasala Gamakumara Ph. D. dissertation. Monash University, Econometrics and Business Statistics. “Probabilistic Forecast Reconciliation”.\nTaieb, Souhaib Ben and Taylor, James W and Hyndman, Rob J. (2017). Coherent probabilistic forecasts for hierarchical time series. International conference on machine learning ICML.\n\n\n\n\n\n\nIf you find the code useful, please ⭐ us on Github"
  },
  {
    "objectID": "probabilistic_methods.html",
    "href": "probabilistic_methods.html",
    "title": "Probabilistic Methods",
    "section": "",
    "text": "Here we provide a collection of methods designed to provide hierarchically coherent probabilistic distributions, which means that they generate samples of multivariate time series with hierarchical linear constraints.\nWe designed these methods to extend the core.HierarchicalForecast capabilities class. Check their usage example here.\n\n1. Normality\n\nsource\n\nNormality\n\n Normality (S:numpy.ndarray, P:numpy.ndarray, y_hat:numpy.ndarray,\n            sigmah:numpy.ndarray, W:numpy.ndarray, seed:int=0)\n\nNormality Probabilistic Reconciliation Class.\nThe Normality method leverages the Gaussian Distribution linearity, to generate hierarchically coherent prediction distributions. This class is meant to be used as the sampler input as other HierarchicalForecast reconciliation classes.\nGiven base forecasts under a normal distribution: \\[\\hat{y}_{h} \\sim \\mathrm{N}(\\hat{\\boldsymbol{\\mu}}, \\hat{\\mathbf{W}}_{h})\\]\nThe reconciled forecasts are also normally distributed: \\[\\tilde{y}_{h} \\sim \\mathrm{N}(\\mathbf{S}\\mathbf{P}\\hat{\\boldsymbol{\\mu}},\n\\mathbf{S}\\mathbf{P}\\hat{\\mathbf{W}}_{h} \\mathbf{P}^{\\intercal} \\mathbf{S}^{\\intercal})\\]\nParameters: S: np.array, summing matrix of size (base, bottom). P: np.array, reconciliation matrix of size (bottom, base). y_hat: Point forecasts values of size (base, horizon). W: np.array, hierarchical covariance matrix of size (base, base). sigmah: np.array, forecast standard dev. of size (base, horizon). num_samples: int, number of bootstraped samples generated. seed: int, random seed for numpy generator’s replicability.\nReferences: - Panagiotelis A., Gamakumara P. Athanasopoulos G., and Hyndman R. J. (2022). “Probabilistic forecast reconciliation: Properties, evaluation and score optimisation”. European Journal of Operational Research.\n\nsource\n\n\nNormality.get_samples\n\n Normality.get_samples (num_samples:int=None)\n\nNormality Coherent Samples.\nObtains coherent samples under the Normality assumptions.\nParameters: num_samples: int, number of samples generated from coherent distribution.\nReturns: samples: Coherent samples of size (base, horizon, num_samples).\n\n\n\n2. Bootstrap\n\nsource\n\nBootstrap\n\n Bootstrap (S:numpy.ndarray, P:numpy.ndarray, y_hat:numpy.ndarray,\n            y_insample:numpy.ndarray, y_hat_insample:numpy.ndarray,\n            num_samples:int=100, seed:int=0, W:numpy.ndarray=None)\n\nBootstrap Probabilistic Reconciliation Class.\nThis method goes beyond the normality assumption for the base forecasts, the technique simulates future sample paths and uses them to generate base sample paths that are latered reconciled. This clever idea and its simplicity allows to generate coherent bootstraped prediction intervals for any reconciliation strategy. This class is meant to be used as the sampler input as other HierarchicalForecast reconciliation classes.\nGiven a boostraped set of simulated sample paths: \\[(\\hat{\\mathbf{y}}^{[1]}_{\\tau}, \\dots ,\\hat{\\mathbf{y}}^{[B]}_{\\tau})\\]\nThe reconciled sample paths allow for reconciled distributional forecasts: \\[(\\mathbf{S}\\mathbf{P}\\hat{\\mathbf{y}}^{[1]}_{\\tau}, \\dots ,\\mathbf{S}\\mathbf{P}\\hat{\\mathbf{y}}^{[B]}_{\\tau})\\]\nParameters: S: np.array, summing matrix of size (base, bottom). P: np.array, reconciliation matrix of size (bottom, base). y_hat: Point forecasts values of size (base, horizon). y_insample: Insample values of size (base, insample_size). y_hat_insample: Insample point forecasts of size (base, insample_size). num_samples: int, number of bootstraped samples generated. seed: int, random seed for numpy generator’s replicability.\nReferences: - Puwasala Gamakumara Ph. D. dissertation. Monash University, Econometrics and Business Statistics (2020). “Probabilistic Forecast Reconciliation” - Panagiotelis A., Gamakumara P. Athanasopoulos G., and Hyndman R. J. (2022). “Probabilistic forecast reconciliation: Properties, evaluation and score optimisation”. European Journal of Operational Research.\n\nsource\n\n\nBootstrap.get_samples\n\n Bootstrap.get_samples (num_samples:int=None)\n\nBootstrap Sample Reconciliation Method.\nApplies Bootstrap sample reconciliation method as defined by Gamakumara 2020. Generating independent sample paths and reconciling them with Bootstrap.\nParameters: num_samples: int, number of samples generated from coherent distribution.\nReturns: samples: Coherent samples of size (base, horizon, num_samples).\n\n\n\n3. PERMBU\n\nsource\n\nPERMBU\n\n PERMBU (S:numpy.ndarray, tags:Dict[str,numpy.ndarray],\n         y_hat:numpy.ndarray, y_insample:numpy.ndarray,\n         y_hat_insample:numpy.ndarray, sigmah:numpy.ndarray,\n         num_samples:int=None, seed:int=0, P:numpy.ndarray=None)\n\nPERMBU Probabilistic Reconciliation Class.\nThe PERMBU method leverages empirical bottom-level marginal distributions with empirical copula functions (describing bottom-level dependencies) to generate the distribution of aggregate-level distributions using BottomUp reconciliation. The sample reordering technique in the PERMBU method reinjects multivariate dependencies into independent bottom-level samples.\nAlgorithm:\n1.   For all series compute conditional marginals distributions.\n2.   Compute residuals $\\hat{\\epsilon}_{i,t}$ and obtain rank permutations.\n2.   Obtain K-sample from the bottom-level series predictions.\n3.   Apply recursively through the hierarchical structure:&lt;br&gt;\n    3.1.   For a given aggregate series $i$ and its children series:&lt;br&gt;\n    3.2.   Obtain children's empirical joint using sample reordering copula.&lt;br&gt;\n    3.2.   From the children's joint obtain the aggregate series's samples.    \nParameters: S: np.array, summing matrix of size (base, bottom). tags: Each key is a level and each value its S indices. y_insample: Insample values of size (base, insample_size). y_hat_insample: Insample point forecasts of size (base, insample_size). sigmah: np.array, forecast standard dev. of size (base, horizon). num_samples: int, number of normal prediction samples generated. seed: int, random seed for numpy generator’s replicability.\nReferences: - Taieb, Souhaib Ben and Taylor, James W and Hyndman, Rob J. (2017). Coherent probabilistic forecasts for hierarchical time series. International conference on machine learning ICML.\n\nsource\n\n\nPERMBU.get_samples\n\n PERMBU.get_samples (num_samples:int=None)\n\nPERMBU Sample Reconciliation Method.\nApplies PERMBU reconciliation method as defined by Taieb et. al 2017. Generating independent base prediction samples, restoring its multivariate dependence using estimated copula with reordering and applying the BottomUp aggregation to the new samples.\nParameters: num_samples: int, number of samples generated from coherent distribution.\nReturns: samples: Coherent samples of size (base, horizon, num_samples).\n\n\n\nReferences\n\nRob J. Hyndman and George Athanasopoulos (2018). “Forecasting principles and practice, Reconciled distributional forecasts”.\nPuwasala Gamakumara Ph. D. dissertation. Monash University, Econometrics and Business Statistics (2020). “Probabilistic Forecast Reconciliation”\nPanagiotelis A., Gamakumara P. Athanasopoulos G., and Hyndman R. J. (2022). “Probabilistic forecast reconciliation: Properties, evaluation and score optimisation”. European Journal of Operational Research.\nTaieb, Souhaib Ben and Taylor, James W and Hyndman, Rob J. (2017). Coherent probabilistic forecasts for hierarchical time series. International conference on machine learning ICML.\n\n\n\n\n\nIf you find the code useful, please ⭐ us on Github"
  },
  {
    "objectID": "examples/australiandomestictourism-intervals.html",
    "href": "examples/australiandomestictourism-intervals.html",
    "title": "Normality",
    "section": "",
    "text": "In many cases, only the time series at the lowest level of the hierarchies (bottom time series) are available. HierarchicalForecast has tools to create time series for all hierarchies and also allows you to calculate prediction intervals for all hierarchies. In this notebook we will see how to do it.\n!pip install hierarchicalforecast statsforecast\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# compute base forecast no coherent\nfrom statsforecast.models import AutoARIMA\nfrom statsforecast.core import StatsForecast\n\n#obtain hierarchical reconciliation methods and evaluation\nfrom hierarchicalforecast.methods import BottomUp, MinTrace\nfrom hierarchicalforecast.utils import aggregate, HierarchicalPlot\nfrom hierarchicalforecast.core import HierarchicalReconciliation\nfrom hierarchicalforecast.evaluation import HierarchicalEvaluation\n\n/Users/fedex/miniconda3/envs/hierarchicalforecast/lib/python3.10/site-packages/statsforecast/core.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from tqdm.autonotebook import tqdm\nIf you find the code useful, please ⭐ us on Github"
  },
  {
    "objectID": "examples/australiandomestictourism-intervals.html#aggregate-bottom-time-series",
    "href": "examples/australiandomestictourism-intervals.html#aggregate-bottom-time-series",
    "title": "Normality",
    "section": "Aggregate bottom time series",
    "text": "Aggregate bottom time series\nIn this example we will use the Tourism dataset from the Forecasting: Principles and Practice book. The dataset only contains the time series at the lowest level, so we need to create the time series for all hierarchies.\n\nY_df = pd.read_csv('https://raw.githubusercontent.com/Nixtla/transfer-learning-time-series/main/datasets/tourism.csv')\nY_df = Y_df.rename({'Trips': 'y', 'Quarter': 'ds'}, axis=1)\nY_df.insert(0, 'Country', 'Australia')\nY_df = Y_df[['Country', 'Region', 'State', 'Purpose', 'ds', 'y']]\nY_df['ds'] = Y_df['ds'].str.replace(r'(\\d+) (Q\\d)', r'\\1-\\2', regex=True)\nY_df['ds'] = pd.to_datetime(Y_df['ds'])\nY_df.head()\n\n\n\n\n\n\n\n\nCountry\nRegion\nState\nPurpose\nds\ny\n\n\n\n\n0\nAustralia\nAdelaide\nSouth Australia\nBusiness\n1998-01-01\n135.077690\n\n\n1\nAustralia\nAdelaide\nSouth Australia\nBusiness\n1998-04-01\n109.987316\n\n\n2\nAustralia\nAdelaide\nSouth Australia\nBusiness\n1998-07-01\n166.034687\n\n\n3\nAustralia\nAdelaide\nSouth Australia\nBusiness\n1998-10-01\n127.160464\n\n\n4\nAustralia\nAdelaide\nSouth Australia\nBusiness\n1999-01-01\n137.448533\n\n\n\n\n\n\n\nThe dataset can be grouped in the following non-strictly hierarchical structure.\n\nspec = [\n    ['Country'],\n    ['Country', 'State'], \n    ['Country', 'Purpose'], \n    ['Country', 'State', 'Region'], \n    ['Country', 'State', 'Purpose'], \n    ['Country', 'State', 'Region', 'Purpose']\n]\n\nUsing the aggregate function from HierarchicalForecast we can generate: 1. Y_df: the hierarchical structured series \\(\\mathbf{y}_{[a,b]\\tau}\\) 2. S_df: the aggregation constraings dataframe with \\(S_{[a,b]}\\) 3. tags: a list with the ‘unique_ids’ conforming each aggregation level.\n\nY_df, S_df, tags = aggregate(df=Y_df, spec=spec)\nY_df = Y_df.reset_index()\n\n/Users/fedex/miniconda3/envs/hierarchicalforecast/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:828: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n  warnings.warn(\n\n\n\nY_df.head()\n\n\n\n\n\n\n\n\nunique_id\nds\ny\n\n\n\n\n0\nAustralia\n1998-01-01\n23182.197269\n\n\n1\nAustralia\n1998-04-01\n20323.380067\n\n\n2\nAustralia\n1998-07-01\n19826.640511\n\n\n3\nAustralia\n1998-10-01\n20830.129891\n\n\n4\nAustralia\n1999-01-01\n22087.353380\n\n\n\n\n\n\n\n\nS_df.iloc[:5, :5]\n\n\n\n\n\n\n\n\nAustralia/ACT/Canberra/Business\nAustralia/ACT/Canberra/Holiday\nAustralia/ACT/Canberra/Other\nAustralia/ACT/Canberra/Visiting\nAustralia/New South Wales/Blue Mountains/Business\n\n\n\n\nAustralia\n1.0\n1.0\n1.0\n1.0\n1.0\n\n\nAustralia/ACT\n1.0\n1.0\n1.0\n1.0\n0.0\n\n\nAustralia/New South Wales\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\nAustralia/Northern Territory\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\nAustralia/Queensland\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n\n\n\n\ntags['Country/Purpose']\n\narray(['Australia/Business', 'Australia/Holiday', 'Australia/Other',\n       'Australia/Visiting'], dtype=object)\n\n\nWe can visualize the S matrix and the data using the HierarchicalPlot class as follows.\n\nhplot = HierarchicalPlot(S=S_df, tags=tags)\n\n\nhplot.plot_summing_matrix()\n\n\n\n\n\nhplot.plot_hierarchically_linked_series(\n    bottom_series='Australia/ACT/Canberra/Holiday',\n    Y_df=Y_df.set_index('unique_id')\n)\n\n\n\n\n\nSplit Train/Test sets\nWe use the final two years (8 quarters) as test set.\n\nY_test_df = Y_df.groupby('unique_id').tail(8)\nY_train_df = Y_df.drop(Y_test_df.index)\n\n\nY_test_df = Y_test_df.set_index('unique_id')\nY_train_df = Y_train_df.set_index('unique_id')\n\n\nY_train_df.groupby('unique_id').size()\n\nunique_id\nAustralia                                                72\nAustralia/ACT                                            72\nAustralia/ACT/Business                                   72\nAustralia/ACT/Canberra                                   72\nAustralia/ACT/Canberra/Business                          72\n                                                         ..\nAustralia/Western Australia/Experience Perth/Other       72\nAustralia/Western Australia/Experience Perth/Visiting    72\nAustralia/Western Australia/Holiday                      72\nAustralia/Western Australia/Other                        72\nAustralia/Western Australia/Visiting                     72\nLength: 425, dtype: int64"
  },
  {
    "objectID": "examples/australiandomestictourism-intervals.html#computing-base-forecasts",
    "href": "examples/australiandomestictourism-intervals.html#computing-base-forecasts",
    "title": "Normality",
    "section": "Computing base forecasts",
    "text": "Computing base forecasts\nThe following cell computes the base forecasts for each time series in Y_df using the AutoARIMA and model. Observe that Y_hat_df contains the forecasts but they are not coherent. To reconcile the prediction intervals we need to calculate the uncoherent intervals using the level argument of StatsForecast.\n\nfcst = StatsForecast(df=Y_train_df,\n                     models=[AutoARIMA(season_length=4)], \n                     freq='QS', n_jobs=-1)\nY_hat_df = fcst.forecast(h=8, fitted=True, level=[80, 90])\nY_fitted_df = fcst.forecast_fitted_values()"
  },
  {
    "objectID": "examples/australiandomestictourism-intervals.html#reconcile-forecasts",
    "href": "examples/australiandomestictourism-intervals.html#reconcile-forecasts",
    "title": "Normality",
    "section": "Reconcile forecasts",
    "text": "Reconcile forecasts\nThe following cell makes the previous forecasts coherent using the HierarchicalReconciliation class. Since the hierarchy structure is not strict, we can’t use methods such as TopDown or MiddleOut. In this example we use BottomUp and MinTrace. If you want to calculate prediction intervals, you have to use the level argument as follows.\n\nreconcilers = [\n    BottomUp(),\n    MinTrace(method='mint_shrink'),\n    MinTrace(method='ols')\n]\nhrec = HierarchicalReconciliation(reconcilers=reconcilers)\nY_rec_df = hrec.reconcile(Y_hat_df=Y_hat_df, Y_df=Y_fitted_df, \n                          S=S_df, tags=tags, level=[80, 90])\n\nThe dataframe Y_rec_df contains the reconciled forecasts.\n\nY_rec_df.head()\n\n\n\n\n\n\n\n\nds\nAutoARIMA\nAutoARIMA-lo-90\nAutoARIMA-lo-80\nAutoARIMA-hi-80\nAutoARIMA-hi-90\nAutoARIMA/BottomUp\nAutoARIMA/BottomUp-lo-90\nAutoARIMA/BottomUp-lo-80\nAutoARIMA/BottomUp-hi-80\n...\nAutoARIMA/MinTrace_method-mint_shrink\nAutoARIMA/MinTrace_method-mint_shrink-lo-90\nAutoARIMA/MinTrace_method-mint_shrink-lo-80\nAutoARIMA/MinTrace_method-mint_shrink-hi-80\nAutoARIMA/MinTrace_method-mint_shrink-hi-90\nAutoARIMA/MinTrace_method-ols\nAutoARIMA/MinTrace_method-ols-lo-90\nAutoARIMA/MinTrace_method-ols-lo-80\nAutoARIMA/MinTrace_method-ols-hi-80\nAutoARIMA/MinTrace_method-ols-hi-90\n\n\nunique_id\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAustralia\n2016-01-01\n26212.554688\n24694.224609\n25029.580078\n27395.527344\n27730.884766\n24368.099609\n23674.076441\n23827.366706\n24908.832513\n...\n25205.749397\n24453.417115\n24619.586229\n25791.912565\n25958.081679\n26059.047512\n24978.608364\n25217.247087\n26900.847937\n27139.486661\n\n\nAustralia\n2016-04-01\n25033.667969\n23324.066406\n23701.669922\n26365.666016\n26743.269531\n22395.921875\n21629.482078\n21798.767146\n22993.076604\n...\n23720.833190\n22915.772233\n23093.587632\n24348.078748\n24525.894148\n24769.464257\n23554.946551\n23823.199470\n25715.729045\n25983.981963\n\n\nAustralia\n2016-07-01\n24507.027344\n22625.500000\n23041.076172\n25972.978516\n26388.554688\n22004.169922\n21182.945074\n21364.330624\n22644.009219\n...\n23167.123691\n22316.298074\n22504.221604\n23830.025777\n24017.949308\n24205.855344\n22870.661086\n23165.568073\n25246.142616\n25541.049603\n\n\nAustralia\n2016-10-01\n25598.929688\n23559.919922\n24010.281250\n27187.578125\n27637.937500\n22325.056641\n21456.892977\n21648.645996\n23001.467285\n...\n23982.251913\n23087.313715\n23284.980478\n24679.523348\n24877.190111\n25271.861336\n23825.782311\n24145.180634\n26398.542038\n26717.940362\n\n\nAustralia\n2017-01-01\n26982.578125\n24651.535156\n25166.396484\n28798.757812\n29313.619141\n23258.001953\n22296.178714\n22508.618508\n24007.385398\n...\n25002.243615\n24016.747195\n24234.415731\n25770.071498\n25987.740034\n26611.143736\n24959.636647\n25324.408272\n27897.879201\n28262.650825\n\n\n\n\n5 rows × 21 columns"
  },
  {
    "objectID": "examples/australiandomestictourism-intervals.html#plot-forecasts",
    "href": "examples/australiandomestictourism-intervals.html#plot-forecasts",
    "title": "Normality",
    "section": "Plot forecasts",
    "text": "Plot forecasts\nThen we can plot the probabilistic forecasts using the following function.\n\nplot_df = pd.concat([Y_df.set_index(['unique_id', 'ds']), \n                     Y_rec_df.set_index('ds', append=True)], axis=1)\nplot_df = plot_df.reset_index('ds')\n\n\nPlot single time series\n\nhplot.plot_series(\n    series='Australia',\n    Y_df=plot_df, \n    models=['y', 'AutoARIMA', 'AutoARIMA/MinTrace_method-ols'],\n    level=[80]\n)\n\n\n\n\n\n# Since we are plotting a bottom time series\n# the probabilistic and mean forecasts\n# are the same\nhplot.plot_series(\n    series='Australia/Western Australia/Experience Perth/Visiting',\n    Y_df=plot_df, \n    models=['y', 'AutoARIMA', 'AutoARIMA/BottomUp'],\n    level=[80]\n)\n\n\n\n\n\n\nPlot hierarchichally linked time series\n\nhplot.plot_hierarchically_linked_series(\n    bottom_series='Australia/Western Australia/Experience Perth/Visiting',\n    Y_df=plot_df, \n    models=['y', 'AutoARIMA', 'AutoARIMA/MinTrace_method-ols', 'AutoARIMA/BottomUp'],\n    level=[80]\n)\n\n\n\n\n\n# ACT only has Canberra\nhplot.plot_hierarchically_linked_series(\n    bottom_series='Australia/ACT/Canberra/Other',\n    Y_df=plot_df, \n    models=['y', 'AutoARIMA/MinTrace_method-mint_shrink'],\n    level=[80, 90]\n)\n\n\n\n\n\n\nReferences\n\nHyndman, R.J., & Athanasopoulos, G. (2021). “Forecasting: principles and practice, 3rd edition: Chapter 11: Forecasting hierarchical and grouped series.”. OTexts: Melbourne, Australia. OTexts.com/fpp3 Accessed on July 2022.\nShanika L. Wickramasuriya, George Athanasopoulos, and Rob J. Hyndman. Optimal forecast reconciliation for hierarchical and grouped time series through trace minimization.Journal of the American Statistical Association, 114(526):804–819, 2019. doi: 10.1080/01621459.2018.1448825. URL https://robjhyndman.com/publications/mint/."
  },
  {
    "objectID": "examples/tourismsmall.html",
    "href": "examples/tourismsmall.html",
    "title": "Reconciliation Quick Start",
    "section": "",
    "text": "Large collections of time series organized into structures at different aggregation levels often require their forecasts to follow their aggregation constraints, which poses the challenge of creating novel algorithms capable of coherent forecasts.\nThe HierarchicalForecast package provides a wide collection of Python implementations of hierarchical forecasting algorithms that follow classic hierarchical reconciliation.\nIn this notebook we will show how to use the StatsForecast library to produce base forecasts, and use HierarchicalForecast package to perform hierarchical reconciliation.\nYou can run these experiments using CPU or GPU with Google Colab.\nIf you find the code useful, please ⭐ us on Github"
  },
  {
    "objectID": "examples/tourismsmall.html#libraries",
    "href": "examples/tourismsmall.html#libraries",
    "title": "Reconciliation Quick Start",
    "section": "1. Libraries",
    "text": "1. Libraries\n\n# !pip install hierarchicalforecast\n# !pip install -U numba statsforecast datasetsforecast"
  },
  {
    "objectID": "examples/tourismsmall.html#load-data",
    "href": "examples/tourismsmall.html#load-data",
    "title": "Reconciliation Quick Start",
    "section": "2. Load Data",
    "text": "2. Load Data\nIn this example we will use the TourismSmall dataset. The following cell gets the time series for the different levels in the hierarchy, the summing matrix S which recovers the full dataset from the bottom level hierarchy and the indices of each hierarchy denoted by tags.\n\nimport numpy as np\nimport pandas as pd\n\nfrom datasetsforecast.hierarchical import HierarchicalData, HierarchicalInfo\n\n\ngroup_name = 'TourismSmall'\ngroup = HierarchicalInfo.get_group(group_name)\nY_df, S_df, tags = HierarchicalData.load('./data', group_name)\nY_df['ds'] = pd.to_datetime(Y_df['ds'])\n\n\nY_df.head()\n\n\n\n\n\n\n\n\nunique_id\nds\ny\n\n\n\n\n0\ntotal\n1998-03-31\n84503\n\n\n1\ntotal\n1998-06-30\n65312\n\n\n2\ntotal\n1998-09-30\n72753\n\n\n3\ntotal\n1998-12-31\n70880\n\n\n4\ntotal\n1999-03-31\n86893\n\n\n\n\n\n\n\n\nS_df.iloc[:5, :5]\n\n\n\n\n\n\n\n\nnsw-hol-city\nnsw-hol-noncity\nvic-hol-city\nvic-hol-noncity\nqld-hol-city\n\n\n\n\ntotal\n1.0\n1.0\n1.0\n1.0\n1.0\n\n\nhol\n1.0\n1.0\n1.0\n1.0\n1.0\n\n\nvfr\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\nbus\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\noth\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n\n\n\n\ntags\n\n{'Country': array(['total'], dtype=object),\n 'Country/Purpose': array(['hol', 'vfr', 'bus', 'oth'], dtype=object),\n 'Country/Purpose/State': array(['nsw-hol', 'vic-hol', 'qld-hol', 'sa-hol', 'wa-hol', 'tas-hol',\n        'nt-hol', 'nsw-vfr', 'vic-vfr', 'qld-vfr', 'sa-vfr', 'wa-vfr',\n        'tas-vfr', 'nt-vfr', 'nsw-bus', 'vic-bus', 'qld-bus', 'sa-bus',\n        'wa-bus', 'tas-bus', 'nt-bus', 'nsw-oth', 'vic-oth', 'qld-oth',\n        'sa-oth', 'wa-oth', 'tas-oth', 'nt-oth'], dtype=object),\n 'Country/Purpose/State/CityNonCity': array(['nsw-hol-city', 'nsw-hol-noncity', 'vic-hol-city',\n        'vic-hol-noncity', 'qld-hol-city', 'qld-hol-noncity',\n        'sa-hol-city', 'sa-hol-noncity', 'wa-hol-city', 'wa-hol-noncity',\n        'tas-hol-city', 'tas-hol-noncity', 'nt-hol-city', 'nt-hol-noncity',\n        'nsw-vfr-city', 'nsw-vfr-noncity', 'vic-vfr-city',\n        'vic-vfr-noncity', 'qld-vfr-city', 'qld-vfr-noncity',\n        'sa-vfr-city', 'sa-vfr-noncity', 'wa-vfr-city', 'wa-vfr-noncity',\n        'tas-vfr-city', 'tas-vfr-noncity', 'nt-vfr-city', 'nt-vfr-noncity',\n        'nsw-bus-city', 'nsw-bus-noncity', 'vic-bus-city',\n        'vic-bus-noncity', 'qld-bus-city', 'qld-bus-noncity',\n        'sa-bus-city', 'sa-bus-noncity', 'wa-bus-city', 'wa-bus-noncity',\n        'tas-bus-city', 'tas-bus-noncity', 'nt-bus-city', 'nt-bus-noncity',\n        'nsw-oth-city', 'nsw-oth-noncity', 'vic-oth-city',\n        'vic-oth-noncity', 'qld-oth-city', 'qld-oth-noncity',\n        'sa-oth-city', 'sa-oth-noncity', 'wa-oth-city', 'wa-oth-noncity',\n        'tas-oth-city', 'tas-oth-noncity', 'nt-oth-city', 'nt-oth-noncity'],\n       dtype=object)}\n\n\nWe split the dataframe in train/test splits.\n\nY_test_df = Y_df.groupby('unique_id').tail(group.horizon)\nY_train_df = Y_df.drop(Y_test_df.index)"
  },
  {
    "objectID": "examples/tourismsmall.html#base-forecasts",
    "href": "examples/tourismsmall.html#base-forecasts",
    "title": "Reconciliation Quick Start",
    "section": "3. Base forecasts",
    "text": "3. Base forecasts\nThe following cell computes the base forecast for each time series using the auto_arima and naive models. Observe that Y_hat_df contains the forecasts but they are not coherent.\n\nfrom statsforecast.core import StatsForecast\nfrom statsforecast.models import AutoARIMA, Naive\n\n\nfcst = StatsForecast(\n    df=Y_train_df, \n    models=[AutoARIMA(season_length=group.seasonality), Naive()], \n    freq=group.freq, \n    n_jobs=-1\n)\nY_hat_df = fcst.forecast(h=group.horizon)"
  },
  {
    "objectID": "examples/tourismsmall.html#hierarchical-reconciliation",
    "href": "examples/tourismsmall.html#hierarchical-reconciliation",
    "title": "Reconciliation Quick Start",
    "section": "4. Hierarchical reconciliation",
    "text": "4. Hierarchical reconciliation\nThe following cell makes the previous forecasts coherent using the HierarchicalReconciliation class. The used methods to make the forecasts coherent are:\n\nBottomUp: The reconciliation of the method is a simple addition to the upper levels.\nTopDown: The second method constrains the base-level predictions to the top-most aggregate-level serie and then distributes it to the disaggregate series through the use of proportions.\nMiddleOut: Anchors the base predictions in a middle level.\n\n\nfrom hierarchicalforecast.core import HierarchicalReconciliation\nfrom hierarchicalforecast.methods import BottomUp, TopDown, MiddleOut\n\n\nreconcilers = [\n    BottomUp(),\n    TopDown(method='forecast_proportions'),\n    MiddleOut(middle_level='Country/Purpose/State', \n              top_down_method='forecast_proportions')\n]\nhrec = HierarchicalReconciliation(reconcilers=reconcilers)\nY_rec_df = hrec.reconcile(Y_hat_df=Y_hat_df, Y_df=Y_train_df, S=S_df, tags=tags)"
  },
  {
    "objectID": "examples/tourismsmall.html#evaluation",
    "href": "examples/tourismsmall.html#evaluation",
    "title": "Reconciliation Quick Start",
    "section": "5. Evaluation",
    "text": "5. Evaluation\nThe HierarchicalForecast package includes the HierarchicalEvaluation class to evaluate the different hierarchies and also is capable of compute scaled metrics compared to a benchmark model.\n\nfrom hierarchicalforecast.evaluation import HierarchicalEvaluation\n\n\ndef mse(y, y_hat):\n    return np.mean((y-y_hat)**2)\n\nevaluator = HierarchicalEvaluation(evaluators=[mse])\nevaluation = evaluator.evaluate(\n        Y_hat_df=Y_rec_df, Y_test_df=Y_test_df.set_index('unique_id'),\n        tags=tags, benchmark='Naive'\n)\nevaluation.filter(like='ARIMA', axis=1).T\n\n\n\n\n\n\n\nlevel\nOverall\nCountry\nCountry/Purpose\nCountry/Purpose/State\nCountry/Purpose/State/CityNonCity\n\n\nmetric\nmse-scaled\nmse-scaled\nmse-scaled\nmse-scaled\nmse-scaled\n\n\n\n\nAutoARIMA\n0.168957\n0.132836\n0.193802\n0.182351\n0.199594\n\n\nAutoARIMA/BottomUp\n0.100188\n0.08518\n0.077097\n0.155771\n0.199594\n\n\nAutoARIMA/TopDown_method-forecast_proportions\n0.144239\n0.132836\n0.11976\n0.204426\n0.227395\n\n\nAutoARIMA/MiddleOut_middle_level-Country/Purpose/State_top_down_method-forecast_proportions\n0.134219\n0.145776\n0.091276\n0.182351\n0.215141\n\n\n\n\n\n\n\n\nReferences\n\nOrcutt, G.H., Watts, H.W., & Edwards, J.B.(1968). Data aggregation and information loss. The American Economic Review, 58 , 773(787).\nDisaggregation methods to expedite product line forecasting. Journal of Forecasting, 9 , 233–254. doi:10.1002/for.3980090304.\nAn investigation of aggregate variable time series forecast strategies with specific subaggregate time series statistical correlation. Computers and Operations Research, 26 , 1133–1149. doi:10.1016/S0305-0548(99)00017-9.\nHyndman, R.J., & Athanasopoulos, G. (2021). “Forecasting: principles and practice, 3rd edition: Chapter 11: Forecasting hierarchical and grouped series.”. OTexts: Melbourne, Australia. OTexts.com/fpp3 Accessed on July 2022."
  },
  {
    "objectID": "examples/introduction.html",
    "href": "examples/introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "You can run these experiments using CPU or GPU with Google Colab.\nIf you find the code useful, please ⭐ us on Github"
  },
  {
    "objectID": "examples/introduction.html#hierarchical-series",
    "href": "examples/introduction.html#hierarchical-series",
    "title": "Introduction",
    "section": "1. Hierarchical Series",
    "text": "1. Hierarchical Series\nIn many applications, a set of time series is hierarchically organized. Examples include the presence of geographic levels, products, or categories that define different types of aggregations.\nIn such scenarios, forecasters are often required to provide predictions for all disaggregate and aggregate series. A natural desire is for those predictions to be “coherent”, that is, for the bottom series to add up precisely to the forecasts of the aggregated series.\n\n\n\nFigure 1. A two level time series hierarchical structure, with four bottom level variables.\n\n\nFigure 1. shows a simple hierarchical structure where we have four bottom-level series, two middle-level series, and the top level representing the total aggregation. Its hierarchical aggregations or coherency constraints are:\n\\[\\begin{align}\n        y_{\\mathrm{Total},\\tau} = y_{\\beta_{1},\\tau}+y_{\\beta_{2},\\tau}+y_{\\beta_{3},\\tau}+y_{\\beta_{4},\\tau}\n        \\qquad \\qquad \\qquad \\qquad \\qquad \\\\\n        \\mathbf{y}_{[a],\\tau}=\\left[y_{\\mathrm{Total},\\tau},\\; y_{\\beta_{1},\\tau}+y_{\\beta_{2},\\tau},\\;y_{\\beta_{3},\\tau}+y_{\\beta_{4},\\tau}\\right]^{\\intercal}\n        \\qquad\n        \\mathbf{y}_{[b],\\tau}=\\left[ y_{\\beta_{1},\\tau},\\; y_{\\beta_{2},\\tau},\\; y_{\\beta_{3},\\tau},\\; y_{\\beta_{4},\\tau} \\right]^{\\intercal}\n\\end{align}\\]\nLuckily these constraints can be compactly expressed with the following matrices:\n\\[\\begin{align}\n\\mathbf{S}_{[a,b][b]}\n=\n\\begin{bmatrix}\n\\mathbf{A}_{\\mathrm{[a][b]}} \\\\\n           \\\\\n           \\\\\n\\mathbf{I}_{\\mathrm{[b][b]}} \\\\\n           \\\\\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1 & 1 & 1 & 1 \\\\\n1 & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 1 \\\\\n1 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1 \\\\\n\\end{bmatrix}\n\\end{align}\\]\nwhere \\(\\mathbf{A}_{[a,b][b]}\\) aggregates the bottom series to the upper levels, and \\(\\mathbf{I}_{\\mathrm{[b][b]}}\\) is an identity matrix. The representation of the hierarchical series is then:\n\\[\\begin{align}\n\\mathbf{y}_{[a,b],\\tau} = \\mathbf{S}_{[a,b][b]} \\mathbf{y}_{[b],\\tau}\n\\end{align}\\]\nTo visualize an example, in Figure 2. One can think of the hierarchical time series structure levels to represent different geographical aggregations. For example, in Figure 2. the top level is the total aggregation of series within a country, the middle level being its states and the bottom level its regions.\n\n\n\nFigure 2. A hierarchy can be composed of geographic levels. In this example the top level corresponds to country aggregation, middle level to states, and bottom level to regions."
  },
  {
    "objectID": "examples/introduction.html#hierarchical-forecast",
    "href": "examples/introduction.html#hierarchical-forecast",
    "title": "Introduction",
    "section": "2. Hierarchical Forecast",
    "text": "2. Hierarchical Forecast\nTo achieve “coherency”, most statistical solutions to the hierarchical forecasting challenge implement a two-stage reconciliation process.\n1. First, we obtain a set of the base forecast \\(\\mathbf{\\hat{y}}_{[a,b],\\tau}\\) 2. Later, we reconcile them into coherent forecasts \\(\\mathbf{\\tilde{y}}_{[a,b],\\tau}\\).\nMost hierarchical reconciliation methods can be expressed by the following transformations:\n\\[\\begin{align}\n\\tilde{\\mathbf{y}}_{[a,b],\\tau} = \\mathbf{S}_{[a,b][b]} \\mathbf{P}_{[b][a,b]} \\hat{\\mathbf{y}}_{[a,b],\\tau}\n\\end{align}\\]\nThe HierarchicalForecast library offers a Python collection of reconciliation methods, datasets, evaluation and visualization tools for the task. Among its available reconciliation methods we have BottomUp, TopDown, MiddleOut, MinTrace, ERM. Among its probabilistic coherent methods we have Normality, Bootstrap, PERMBU."
  },
  {
    "objectID": "examples/introduction.html#minimal-example",
    "href": "examples/introduction.html#minimal-example",
    "title": "Introduction",
    "section": "3. Minimal Example",
    "text": "3. Minimal Example\n\n!pip install hierarchicalforecast\n!pip install -U numba statsforecast datasetsforecast\n\n\nWrangling Data\n\nimport numpy as np\nimport pandas as pd\n\nWe are going to creat a synthetic data set to illustrate a hierarchical time series structure like the one in Figure 1.\nWe will create a two level structure with four bottom series where aggregations of the series are self evident.\n\n# Create Figure 1. synthetic bottom data\nds = pd.date_range(start='2000-01-01', end='2000-08-01', freq='MS')\ny_base = np.arange(1,9)\nr1 = y_base * (10**1)\nr2 = y_base * (10**1)\nr3 = y_base * (10**2)\nr4 = y_base * (10**2)\n\nys = np.concatenate([r1, r2, r3, r4])\nds = np.tile(ds, 4)\nunique_ids = ['r1'] * 8 + ['r2'] * 8 + ['r3'] * 8 + ['r4'] * 8\ntop_level = 'Australia'\nmiddle_level = ['State1'] * 16 + ['State2'] * 16\nbottom_level = unique_ids\n\nbottom_df = dict(ds=ds,\n                 top_level=top_level, \n                 middle_level=middle_level, \n                 bottom_level=bottom_level,\n                 y=ys)\nbottom_df = pd.DataFrame(bottom_df)\nbottom_df.groupby('bottom_level').head(2)\n\n\n\n\n\n\n\n\nds\ntop_level\nmiddle_level\nbottom_level\ny\n\n\n\n\n0\n2000-01-01\nAustralia\nState1\nr1\n10\n\n\n1\n2000-02-01\nAustralia\nState1\nr1\n20\n\n\n8\n2000-01-01\nAustralia\nState1\nr2\n10\n\n\n9\n2000-02-01\nAustralia\nState1\nr2\n20\n\n\n16\n2000-01-01\nAustralia\nState2\nr3\n100\n\n\n17\n2000-02-01\nAustralia\nState2\nr3\n200\n\n\n24\n2000-01-01\nAustralia\nState2\nr4\n100\n\n\n25\n2000-02-01\nAustralia\nState2\nr4\n200\n\n\n\n\n\n\n\nThe previously introduced hierarchical series \\(\\mathbf{y}_{[a,b]\\tau}\\) is captured within the Y_hier_df dataframe.\nThe aggregation constraints matrix \\(\\mathbf{S}_{[a][b]}\\) is captured within the S_df dataframe.\nFinally the tags contains a list within Y_hier_df composing each hierarchical level, for example the tags['top_level'] contains Australia’s aggregated series index.\n\nfrom hierarchicalforecast.utils import aggregate\n\n\n# Create hierarchical structure and constraints\nhierarchy_levels = [['top_level'],\n                    ['top_level', 'middle_level'],\n                    ['top_level', 'middle_level', 'bottom_level']]\nY_hier_df, S_df, tags = aggregate(df=bottom_df, spec=hierarchy_levels)\nY_hier_df = Y_hier_df.reset_index()\nprint('S_df.shape', S_df.shape)\nprint('Y_hier_df.shape', Y_hier_df.shape)\nprint(\"tags['top_level']\", tags['top_level'])\n\nS_df.shape (7, 4)\nY_hier_df.shape (56, 3)\ntags['top_level'] ['Australia']\n\n\n/Users/cchallu/opt/anaconda3/envs/hierarchicalforecast/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:828: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n  warnings.warn(\n\n\n\nY_hier_df.groupby('unique_id').head(2)\n\n\n\n\n\n\n\n\nunique_id\nds\ny\n\n\n\n\n0\nAustralia\n2000-01-01\n220.0\n\n\n1\nAustralia\n2000-02-01\n440.0\n\n\n8\nAustralia/State1\n2000-01-01\n20.0\n\n\n9\nAustralia/State1\n2000-02-01\n40.0\n\n\n16\nAustralia/State2\n2000-01-01\n200.0\n\n\n17\nAustralia/State2\n2000-02-01\n400.0\n\n\n24\nAustralia/State1/r1\n2000-01-01\n10.0\n\n\n25\nAustralia/State1/r1\n2000-02-01\n20.0\n\n\n32\nAustralia/State1/r2\n2000-01-01\n10.0\n\n\n33\nAustralia/State1/r2\n2000-02-01\n20.0\n\n\n40\nAustralia/State2/r3\n2000-01-01\n100.0\n\n\n41\nAustralia/State2/r3\n2000-02-01\n200.0\n\n\n48\nAustralia/State2/r4\n2000-01-01\n100.0\n\n\n49\nAustralia/State2/r4\n2000-02-01\n200.0\n\n\n\n\n\n\n\n\nS_df\n\n\n\n\n\n\n\n\nAustralia/State1/r1\nAustralia/State1/r2\nAustralia/State2/r3\nAustralia/State2/r4\n\n\n\n\nAustralia\n1.0\n1.0\n1.0\n1.0\n\n\nAustralia/State1\n1.0\n1.0\n0.0\n0.0\n\n\nAustralia/State2\n0.0\n0.0\n1.0\n1.0\n\n\nAustralia/State1/r1\n1.0\n0.0\n0.0\n0.0\n\n\nAustralia/State1/r2\n0.0\n1.0\n0.0\n0.0\n\n\nAustralia/State2/r3\n0.0\n0.0\n1.0\n0.0\n\n\nAustralia/State2/r4\n0.0\n0.0\n0.0\n1.0\n\n\n\n\n\n\n\n\n\nBase Predictions\nNext, we compute the base forecast for each time series using the naive model. Observe that Y_hat_df contains the forecasts but they are not coherent.\n\nfrom statsforecast.models import Naive\nfrom statsforecast.core import StatsForecast\n\n\n# Split train/test sets\nY_test_df  = Y_hier_df.groupby('unique_id').tail(4)\nY_train_df = Y_hier_df.drop(Y_test_df.index)\n\n# Compute base Naive predictions\n# Careful identifying correct data freq, this data quarterly 'Q'\nfcst = StatsForecast(df=Y_train_df,\n                     models=[Naive()],\n                     freq='Q', n_jobs=-1)\nY_hat_df = fcst.forecast(h=4, fitted=True)\nY_fitted_df = fcst.forecast_fitted_values()\n\n\n\nReconciliation\n\nfrom hierarchicalforecast.methods import BottomUp\nfrom hierarchicalforecast.core import HierarchicalReconciliation\n\n\n# You can select a reconciler from our collection\nreconcilers = [BottomUp()] # MinTrace(method='mint_shrink')\nhrec = HierarchicalReconciliation(reconcilers=reconcilers)\n\nY_rec_df = hrec.reconcile(Y_hat_df=Y_hat_df, \n                          Y_df=Y_fitted_df,\n                          S=S_df, tags=tags)\nY_rec_df.groupby('unique_id').head(2)\n\n\n\n\n\n\n\n\nds\nNaive\nNaive/BottomUp\n\n\nunique_id\n\n\n\n\n\n\n\nAustralia\n2000-06-30\n880.0\n880.0\n\n\nAustralia\n2000-09-30\n880.0\n880.0\n\n\nAustralia/State1\n2000-06-30\n80.0\n80.0\n\n\nAustralia/State1\n2000-09-30\n80.0\n80.0\n\n\nAustralia/State2\n2000-06-30\n800.0\n800.0\n\n\nAustralia/State2\n2000-09-30\n800.0\n800.0\n\n\nAustralia/State1/r1\n2000-06-30\n40.0\n40.0\n\n\nAustralia/State1/r1\n2000-09-30\n40.0\n40.0\n\n\nAustralia/State1/r2\n2000-06-30\n40.0\n40.0\n\n\nAustralia/State1/r2\n2000-09-30\n40.0\n40.0\n\n\nAustralia/State2/r3\n2000-06-30\n400.0\n400.0\n\n\nAustralia/State2/r3\n2000-09-30\n400.0\n400.0\n\n\nAustralia/State2/r4\n2000-06-30\n400.0\n400.0\n\n\nAustralia/State2/r4\n2000-09-30\n400.0\n400.0"
  },
  {
    "objectID": "examples/introduction.html#references",
    "href": "examples/introduction.html#references",
    "title": "Introduction",
    "section": "References",
    "text": "References\n\nHyndman, R.J., & Athanasopoulos, G. (2021). “Forecasting: principles and practice, 3rd edition: Chapter 11: Forecasting hierarchical and grouped series.”. OTexts: Melbourne, Australia. OTexts.com/fpp3 Accessed on July 2022.\nOrcutt, G.H., Watts, H.W., & Edwards, J.B.(1968). Data aggregation and information loss. The American Economic Review, 58 , 773(787).\nDisaggregation methods to expedite product line forecasting. Journal of Forecasting, 9 , 233–254. doi:10.1002/for.3980090304.\nWickramasuriya, S. L., Athanasopoulos, G., & Hyndman, R. J. (2019). \"Optimal forecast reconciliation for hierarchical and grouped time series through trace minimization\". Journal of the American Statistical Association, 114 , 804–819. doi:10.1080/01621459.2018.1448825.\nBen Taieb, S., & Koo, B. (2019). Regularized regression for hierarchical forecasting without unbiasedness conditions. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining KDD ’19 (p. 1337(1347). New York, NY, USA: Association for Computing Machinery."
  },
  {
    "objectID": "examples/australiandomestictourism-bootstraped-intervals.html",
    "href": "examples/australiandomestictourism-bootstraped-intervals.html",
    "title": "Bootstrap",
    "section": "",
    "text": "In many cases, only the time series at the lowest level of the hierarchies (bottom time series) are available. HierarchicalForecast has tools to create time series for all hierarchies and also allows you to calculate prediction intervals for all hierarchies. In this notebook we will see how to do it.\n!pip install hierarchicalforecast statsforecast\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# compute base forecast no coherent\nfrom statsforecast.models import ETS, Naive\nfrom statsforecast.core import StatsForecast\n\n#obtain hierarchical reconciliation methods and evaluation\nfrom hierarchicalforecast.methods import BottomUp, MinTrace\nfrom hierarchicalforecast.utils import aggregate, HierarchicalPlot\nfrom hierarchicalforecast.core import HierarchicalReconciliation\nfrom hierarchicalforecast.evaluation import HierarchicalEvaluation\n\n/Users/fedex/miniconda3/envs/hierarchicalforecast/lib/python3.10/site-packages/statsforecast/core.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from tqdm.autonotebook import tqdm\nIf you find the code useful, please ⭐ us on Github"
  },
  {
    "objectID": "examples/australiandomestictourism-bootstraped-intervals.html#aggregate-bottom-time-series",
    "href": "examples/australiandomestictourism-bootstraped-intervals.html#aggregate-bottom-time-series",
    "title": "Bootstrap",
    "section": "Aggregate bottom time series",
    "text": "Aggregate bottom time series\nIn this example we will use the Tourism dataset from the Forecasting: Principles and Practice book. The dataset only contains the time series at the lowest level, so we need to create the time series for all hierarchies.\n\nY_df = pd.read_csv('https://raw.githubusercontent.com/Nixtla/transfer-learning-time-series/main/datasets/tourism.csv')\nY_df = Y_df.rename({'Trips': 'y', 'Quarter': 'ds'}, axis=1)\nY_df.insert(0, 'Country', 'Australia')\nY_df = Y_df[['Country', 'Region', 'State', 'Purpose', 'ds', 'y']]\nY_df['ds'] = Y_df['ds'].str.replace(r'(\\d+) (Q\\d)', r'\\1-\\2', regex=True)\nY_df['ds'] = pd.to_datetime(Y_df['ds'])\nY_df.head()\n\n\n\n\n\n\n\n\nCountry\nRegion\nState\nPurpose\nds\ny\n\n\n\n\n0\nAustralia\nAdelaide\nSouth Australia\nBusiness\n1998-01-01\n135.077690\n\n\n1\nAustralia\nAdelaide\nSouth Australia\nBusiness\n1998-04-01\n109.987316\n\n\n2\nAustralia\nAdelaide\nSouth Australia\nBusiness\n1998-07-01\n166.034687\n\n\n3\nAustralia\nAdelaide\nSouth Australia\nBusiness\n1998-10-01\n127.160464\n\n\n4\nAustralia\nAdelaide\nSouth Australia\nBusiness\n1999-01-01\n137.448533\n\n\n\n\n\n\n\nThe dataset can be grouped in the following non-strictly hierarchical structure.\n\nspec = [\n    ['Country'],\n    ['Country', 'State'], \n    ['Country', 'Purpose'], \n    ['Country', 'State', 'Region'], \n    ['Country', 'State', 'Purpose'], \n    ['Country', 'State', 'Region', 'Purpose']\n]\n\nUsing the aggregate function from HierarchicalForecast we can generate: 1. Y_df: the hierarchical structured series \\(\\mathbf{y}_{[a,b]\\tau}\\) 2. S_df: the aggregation constraings dataframe with \\(S_{[a,b]}\\) 3. tags: a list with the ‘unique_ids’ conforming each aggregation level.\n\nY_df, S_df, tags = aggregate(df=Y_df, spec=spec)\nY_df = Y_df.reset_index()\n\n/Users/fedex/miniconda3/envs/hierarchicalforecast/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:828: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n  warnings.warn(\n\n\n\nY_df.head()\n\n\n\n\n\n\n\n\nunique_id\nds\ny\n\n\n\n\n0\nAustralia\n1998-01-01\n23182.197269\n\n\n1\nAustralia\n1998-04-01\n20323.380067\n\n\n2\nAustralia\n1998-07-01\n19826.640511\n\n\n3\nAustralia\n1998-10-01\n20830.129891\n\n\n4\nAustralia\n1999-01-01\n22087.353380\n\n\n\n\n\n\n\n\nS_df.iloc[:5, :5]\n\n\n\n\n\n\n\n\nAustralia/ACT/Canberra/Business\nAustralia/ACT/Canberra/Holiday\nAustralia/ACT/Canberra/Other\nAustralia/ACT/Canberra/Visiting\nAustralia/New South Wales/Blue Mountains/Business\n\n\n\n\nAustralia\n1.0\n1.0\n1.0\n1.0\n1.0\n\n\nAustralia/ACT\n1.0\n1.0\n1.0\n1.0\n0.0\n\n\nAustralia/New South Wales\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\nAustralia/Northern Territory\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\nAustralia/Queensland\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n\n\n\n\ntags['Country/Purpose']\n\narray(['Australia/Business', 'Australia/Holiday', 'Australia/Other',\n       'Australia/Visiting'], dtype=object)\n\n\nWe can visualize the S_df dataframe and Y_df using the HierarchicalPlot class as follows.\n\nhplot = HierarchicalPlot(S=S_df, tags=tags)\n\n\nhplot.plot_summing_matrix()\n\n\n\n\n\nhplot.plot_hierarchically_linked_series(\n    bottom_series='Australia/ACT/Canberra/Holiday',\n    Y_df=Y_df.set_index('unique_id')\n)\n\n\n\n\n\nSplit Train/Test sets\nWe use the final two years (8 quarters) as test set.\n\nY_test_df = Y_df.groupby('unique_id').tail(8)\nY_train_df = Y_df.drop(Y_test_df.index)\n\n\nY_test_df = Y_test_df.set_index('unique_id')\nY_train_df = Y_train_df.set_index('unique_id')\n\n\nY_train_df.groupby('unique_id').size()\n\nunique_id\nAustralia                                                72\nAustralia/ACT                                            72\nAustralia/ACT/Business                                   72\nAustralia/ACT/Canberra                                   72\nAustralia/ACT/Canberra/Business                          72\n                                                         ..\nAustralia/Western Australia/Experience Perth/Other       72\nAustralia/Western Australia/Experience Perth/Visiting    72\nAustralia/Western Australia/Holiday                      72\nAustralia/Western Australia/Other                        72\nAustralia/Western Australia/Visiting                     72\nLength: 425, dtype: int64"
  },
  {
    "objectID": "examples/australiandomestictourism-bootstraped-intervals.html#computing-base-forecasts",
    "href": "examples/australiandomestictourism-bootstraped-intervals.html#computing-base-forecasts",
    "title": "Bootstrap",
    "section": "Computing Base Forecasts",
    "text": "Computing Base Forecasts\nThe following cell computes the base forecasts for each time series in Y_df using the AutoETS and model. Observe that Y_hat_df contains the forecasts but they are not coherent. Since we are computing prediction intervals using bootstrapping, we only need the fitted values of the models.\n\nfcst = StatsForecast(df=Y_train_df, \n                     models=[ETS(season_length=4, model='ZAA')],\n                     freq='QS', n_jobs=-1)\nY_hat_df = fcst.forecast(h=8, fitted=True)\nY_fitted_df = fcst.forecast_fitted_values()\n\n/Users/fedex/miniconda3/envs/hierarchicalforecast/lib/python3.10/site-packages/statsforecast/models.py:526: FutureWarning: `ETS` will be deprecated in future versions of `StatsForecast`. Please use `AutoETS` instead.\n  ETS._warn()"
  },
  {
    "objectID": "examples/australiandomestictourism-bootstraped-intervals.html#reconcile-base-forecasts",
    "href": "examples/australiandomestictourism-bootstraped-intervals.html#reconcile-base-forecasts",
    "title": "Bootstrap",
    "section": "Reconcile Base Forecasts",
    "text": "Reconcile Base Forecasts\nThe following cell makes the previous forecasts coherent using the HierarchicalReconciliation class. Since the hierarchy structure is not strict, we can’t use methods such as TopDown or MiddleOut. In this example we use BottomUp and MinTrace. If you want to calculate prediction intervals, you have to use the level argument as follows and set intervals_method='bootstrap'.\n\nreconcilers = [\n    BottomUp(),\n    MinTrace(method='mint_shrink'),\n    MinTrace(method='ols')\n]\nhrec = HierarchicalReconciliation(reconcilers=reconcilers)\nY_rec_df = hrec.reconcile(Y_hat_df=Y_hat_df, Y_df=Y_fitted_df, S=S_df, \n                          tags=tags, level=[80, 90], \n                          intervals_method='bootstrap')\n\nThe dataframe Y_rec_df contains the reconciled forecasts.\n\nY_rec_df.head()\n\n\n\n\n\n\n\n\nds\nETS\nETS/BottomUp\nETS/BottomUp-lo-90\nETS/BottomUp-lo-80\nETS/BottomUp-hi-80\nETS/BottomUp-hi-90\nETS/MinTrace_method-mint_shrink\nETS/MinTrace_method-mint_shrink-lo-90\nETS/MinTrace_method-mint_shrink-lo-80\nETS/MinTrace_method-mint_shrink-hi-80\nETS/MinTrace_method-mint_shrink-hi-90\nETS/MinTrace_method-ols\nETS/MinTrace_method-ols-lo-90\nETS/MinTrace_method-ols-lo-80\nETS/MinTrace_method-ols-hi-80\nETS/MinTrace_method-ols-hi-90\n\n\nunique_id\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAustralia\n2016-01-01\n26080.878906\n24487.349609\n23244.120996\n23333.694727\n25381.792969\n25426.333984\n25532.523559\n24428.911701\n24709.210638\n26365.606934\n26476.255501\n26034.114241\n24914.136375\n25100.462938\n27102.735022\n27176.416922\n\n\nAustralia\n2016-04-01\n24587.011719\n23069.744141\n21826.519434\n21912.962891\n23946.606250\n24281.447266\n24118.557177\n23199.968626\n23295.244252\n25108.470410\n25489.383606\n24567.460995\n23484.050568\n23640.638423\n25709.763678\n25809.249492\n\n\nAustralia\n2016-07-01\n24147.308594\n22689.777344\n21297.136719\n21530.438281\n23701.173828\n24155.820312\n23731.251387\n22627.639669\n22818.729182\n24821.488458\n25246.867432\n24150.134898\n23030.156834\n23155.025556\n25359.992376\n25404.841402\n\n\nAustralia\n2016-10-01\n24794.041016\n23429.757812\n22037.123047\n22276.453125\n24241.417969\n24441.160156\n24486.549344\n23385.927232\n23600.704525\n25353.555625\n25481.478557\n24831.584516\n23725.924464\n23836.475174\n25900.205254\n25977.265089\n\n\nAustralia\n2017-01-01\n26284.000000\n24940.042969\n23696.722754\n23904.382812\n25814.941406\n25974.169922\n26041.867488\n24972.077858\n25158.986710\n26918.104747\n27135.580845\n26348.203335\n25254.659324\n25487.502291\n27410.873035\n27477.334507"
  },
  {
    "objectID": "examples/australiandomestictourism-bootstraped-intervals.html#plot-predictions",
    "href": "examples/australiandomestictourism-bootstraped-intervals.html#plot-predictions",
    "title": "Bootstrap",
    "section": "Plot Predictions",
    "text": "Plot Predictions\nThen we can plot the probabilist forecasts using the following function.\n\nplot_df = pd.concat([Y_df.set_index(['unique_id', 'ds']), \n                     Y_rec_df.set_index('ds', append=True)], axis=1)\nplot_df = plot_df.reset_index('ds')\n\n\nPlot single time series\n\nhplot.plot_series(\n    series='Australia',\n    Y_df=plot_df, \n    models=['y', 'ETS', 'ETS/MinTrace_method-ols', 'ETS/MinTrace_method-mint_shrink'],\n    level=[80]\n)\n\n\n\n\n\n# Since we are plotting a bottom time series\n# the probabilistic and mean forecasts\n# differ due to bootstrapping\nhplot.plot_series(\n    series='Australia/Western Australia/Experience Perth/Visiting',\n    Y_df=plot_df, \n    models=['y', 'ETS', 'ETS/BottomUp'],\n    level=[80]\n)\n\n\n\n\n\n\nPlot hierarchichally linked time series\n\nhplot.plot_hierarchically_linked_series(\n    bottom_series='Australia/Western Australia/Experience Perth/Visiting',\n    Y_df=plot_df, \n    models=['y', 'ETS', 'ETS/MinTrace_method-ols', 'ETS/BottomUp'],\n    level=[80]\n)\n\n\n\n\n\n# ACT only has Canberra\nhplot.plot_hierarchically_linked_series(\n    bottom_series='Australia/ACT/Canberra/Other',\n    Y_df=plot_df, \n    models=['y', 'ETS/MinTrace_method-mint_shrink'],\n    level=[80, 90]\n)\n\n\n\n\n\n\nReferences\n\nHyndman, R.J., & Athanasopoulos, G. (2021). “Forecasting: principles and practice, 3rd edition: Chapter 11: Forecasting hierarchical and grouped series.”. OTexts: Melbourne, Australia. OTexts.com/fpp3 Accessed on July 2022.\nShanika L. Wickramasuriya, George Athanasopoulos, and Rob J. Hyndman. Optimal forecast reconciliation for hierarchical and grouped time series through trace minimization.Journal of the American Statistical Association, 114(526):804–819, 2019. doi: 10.1080/01621459.2018.1448825. URL https://robjhyndman.com/publications/mint/.\nPuwasala Gamakumara Ph. D. dissertation. Monash University, Econometrics and Business Statistics (2020). “Probabilistic Forecast Reconciliation”"
  },
  {
    "objectID": "examples/mlframeworksexample.html",
    "href": "examples/mlframeworksexample.html",
    "title": "Neural/MLForecast",
    "section": "",
    "text": "This example notebook demonstrates the compatibility of HierarchicalForecast’s reconciliation methods with popular machine-learning libraries, specifically NeuralForecast and MLForecast.\nThe notebook utilizes NBEATS and XGBRegressor models to create base forecasts for the TourismLarge Hierarchical Dataset. After that, we use HierarchicalForecast to reconcile the base predictions.\nReferences - Boris N. Oreshkin, Dmitri Carpov, Nicolas Chapados, Yoshua Bengio (2019). “N-BEATS: Neural basis expansion analysis for interpretable time series forecasting”. url: https://arxiv.org/abs/1905.10437 - Tianqi Chen and Carlos Guestrin. “XGBoost: A Scalable Tree Boosting System”. In: Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. KDD ’16. San Francisco, California, USA: Association for Computing Machinery, 2016, pp. 785–794. isbn: 9781450342322. doi: 10.1145/2939672.2939785. url: https://doi.org/10.1145/2939672.2939785 (cit. on p. 26).\nYou can run these experiments using CPU or GPU with Google Colab.\nIf you find the code useful, please ⭐ us on Github"
  },
  {
    "objectID": "examples/mlframeworksexample.html#installing-packages",
    "href": "examples/mlframeworksexample.html#installing-packages",
    "title": "Neural/MLForecast",
    "section": "1. Installing packages",
    "text": "1. Installing packages\n\n!pip install datasetsforecast mlforecast \n!pip install git+https://github.com/Nixtla/neuralforecast.git\n!pip install git+https://github.com/Nixtla/hierarchicalforecast.git\n\n\nimport numpy as np\nimport pandas as pd\n\nfrom datasetsforecast.hierarchical import HierarchicalData\n\nfrom neuralforecast import NeuralForecast\nfrom neuralforecast.models import NBEATS\nfrom neuralforecast.losses.pytorch import GMM\n\nfrom mlforecast import MLForecast\nfrom window_ops.expanding import expanding_mean\nfrom mlforecast.utils import PredictionIntervals\nfrom mlforecast.target_transforms import Differences\nimport xgboost as xgb\n\n#obtain hierarchical reconciliation methods and evaluation\nfrom hierarchicalforecast.methods import BottomUp, MinTrace\nfrom hierarchicalforecast.utils import HierarchicalPlot\nfrom hierarchicalforecast.core import HierarchicalReconciliation\nfrom hierarchicalforecast.evaluation import scaled_crps"
  },
  {
    "objectID": "examples/mlframeworksexample.html#load-hierarchical-dataset",
    "href": "examples/mlframeworksexample.html#load-hierarchical-dataset",
    "title": "Neural/MLForecast",
    "section": "2. Load hierarchical dataset",
    "text": "2. Load hierarchical dataset\nThis detailed Australian Tourism Dataset comes from the National Visitor Survey, managed by the Tourism Research Australia, it is composed of 555 monthly series from 1998 to 2016, it is organized geographically, and purpose of travel. The natural geographical hierarchy comprises seven states, divided further in 27 zones and 76 regions. The purpose of travel categories are holiday, visiting friends and relatives (VFR), business and other. The MinT (Wickramasuriya et al., 2019), among other hierarchical forecasting studies has used the dataset it in the past. The dataset can be accessed in the MinT reconciliation webpage, although other sources are available.\n\n\n\n\n\n\n\n\n\nGeographical Division\nNumber of series per division\nNumber of series per purpose\nTotal\n\n\n\n\nAustralia\n1\n4\n5\n\n\nStates\n7\n28\n35\n\n\nZones\n27\n108\n135\n\n\nRegions\n76\n304\n380\n\n\nTotal\n111\n444\n555\n\n\n\n\nY_df, S_df, tags = HierarchicalData.load('./data', 'TourismLarge')\nY_df['ds'] = pd.to_datetime(Y_df['ds'])\n\n\nY_df.head()\n\n\n  \n    \n      \n\n\n\n\n\n\nunique_id\nds\ny\n\n\n\n\n0\nTotalAll\n1998-01-01\n45151.071280\n\n\n1\nTotalAll\n1998-02-01\n17294.699551\n\n\n2\nTotalAll\n1998-03-01\n20725.114184\n\n\n3\nTotalAll\n1998-04-01\n25388.612353\n\n\n4\nTotalAll\n1998-05-01\n20330.035211\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nVisualize the aggregation matrix.\n\nhplot = HierarchicalPlot(S=S_df, tags=tags)\nhplot.plot_summing_matrix()\n\n\n\n\nSplit the dataframe in train/test splits.\n\ndef sort_hier_df(Y_df, S_df):\n    # sorts unique_id lexicographically\n    Y_df.unique_id = Y_df.unique_id.astype('category')\n    Y_df.unique_id = Y_df.unique_id.cat.set_categories(S_df.index)\n    Y_df = Y_df.sort_values(by=['unique_id', 'ds'])\n    return Y_df\n\nY_df = sort_hier_df(Y_df, S_df)\n\n\nhorizon = 12\nY_test_df = Y_df.groupby('unique_id').tail(horizon)\nY_train_df = Y_df.drop(Y_test_df.index)"
  },
  {
    "objectID": "examples/mlframeworksexample.html#fit-and-predict-models",
    "href": "examples/mlframeworksexample.html#fit-and-predict-models",
    "title": "Neural/MLForecast",
    "section": "3. Fit and Predict Models",
    "text": "3. Fit and Predict Models\nHierarchicalForecast is compatible with many different ML models. Here, we show two examples: 1. NBEATS, a MLP-based deep neural architecture. 2. XGBRegressor, a tree-based architecture.\n\nlevel = np.arange(0, 100, 2)\nqs = [[50-lv/2, 50+lv/2] for lv in level]\nquantiles = np.sort(np.concatenate(qs)/100)\n\n#fit/predict NBEATS from NeuralForecast\nnbeats = NBEATS(h=horizon,\n              input_size=2*horizon,\n              loss=GMM(n_components=10, quantiles=quantiles),\n              scaler_type='robust',\n              max_steps=2000)\nnf = NeuralForecast(models=[nbeats], freq='MS')\nnf.fit(df=Y_train_df)\nY_hat_nf = nf.predict()\n\n#fit/predict XGBRegressor from MLForecast\nmf = MLForecast(models=[xgb.XGBRegressor()], \n                freq='MS',\n                lags=[1,2,12,24],\n                date_features=['month'],\n                )\nmf.fit(Y_train_df, prediction_intervals=PredictionIntervals(n_windows=10, window_size=horizon)) \nY_hat_mf = mf.predict(horizon, level=level).set_index('unique_id')\n\nINFO:lightning_fabric.utilities.seed:Global seed set to 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nY_hat_nf\n\n\n  \n    \n      \n\n\n\n\n\n\nds\nNBEATS\nNBEATS-lo-98.0\nNBEATS-lo-96.0\nNBEATS-lo-94.0\nNBEATS-lo-92.0\nNBEATS-lo-90.0\nNBEATS-lo-88.0\nNBEATS-lo-86.0\nNBEATS-lo-84.0\n...\nNBEATS-hi-80.0\nNBEATS-hi-82.0\nNBEATS-hi-84.0\nNBEATS-hi-86.0\nNBEATS-hi-88.0\nNBEATS-hi-90.0\nNBEATS-hi-92.0\nNBEATS-hi-94.0\nNBEATS-hi-96.0\nNBEATS-hi-98.0\n\n\nunique_id\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTotalAll\n2016-01-01\n44304.039062\n24825.771484\n26974.607422\n27405.914062\n27881.269531\n28640.238281\n29469.513672\n30213.277344\n31009.929688\n...\n51838.828125\n52150.523438\n52404.886719\n52564.652344\n52951.238281\n53216.839844\n53689.351562\n54015.074219\n54545.882812\n55752.621094\n\n\nTotalAll\n2016-02-01\n20877.984375\n17909.365234\n18334.902344\n18577.355469\n18653.085938\n18755.072266\n18839.824219\n18965.947266\n19074.134766\n...\n22756.220703\n22892.509766\n23029.402344\n23133.941406\n23221.666016\n23385.628906\n23587.021484\n23862.343750\n24243.560547\n24526.462891\n\n\nTotalAll\n2016-03-01\n23444.972656\n18971.355469\n19329.705078\n19472.619141\n19756.503906\n19843.703125\n20075.363281\n20126.689453\n20259.271484\n...\n26024.242188\n26116.677734\n26196.498047\n26342.339844\n26535.798828\n26758.476562\n26934.582031\n27097.130859\n27441.996094\n27704.375000\n\n\nTotalAll\n2016-04-01\n28927.132812\n24030.257812\n24540.779297\n24732.566406\n24988.001953\n25160.744141\n25304.658203\n25456.001953\n25567.078125\n...\n31568.966797\n31698.855469\n31856.851562\n32097.916016\n32211.320312\n32345.988281\n32510.902344\n32724.638672\n33078.031250\n33525.035156\n\n\nTotalAll\n2016-05-01\n22716.433594\n19728.511719\n19910.925781\n20089.443359\n20214.955078\n20269.906250\n20355.708984\n20441.349609\n20491.029297\n...\n24937.335938\n25114.396484\n25270.279297\n25446.765625\n25676.287109\n26028.427734\n26440.011719\n27477.541016\n28452.419922\n29793.591797\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\nGBDOth\n2016-08-01\n4.731373\n-30.691290\n-8.694043\n-2.576124\n-2.196553\n-2.069076\n-1.913422\n-1.854156\n-1.767804\n...\n9.252028\n10.948211\n12.031944\n14.396760\n18.523523\n43.287716\n58.207531\n69.754929\n81.399673\n116.701561\n\n\nGBDOth\n2016-09-01\n5.685491\n-32.813366\n-11.985416\n-2.978264\n-2.413029\n-2.120405\n-1.788605\n-1.673310\n-1.550562\n...\n12.787840\n14.330542\n15.563581\n16.996040\n29.901039\n45.086597\n60.724380\n75.462578\n92.432518\n125.217796\n\n\nGBDOth\n2016-10-01\n4.760162\n-51.105358\n-27.034277\n-8.493114\n-2.859874\n-2.140030\n-1.905673\n-1.764797\n-1.621011\n...\n10.930604\n11.960605\n13.876516\n14.839364\n18.540100\n32.251144\n48.573261\n65.301460\n83.327026\n113.249001\n\n\nGBDOth\n2016-11-01\n6.491304\n-31.302568\n-6.776994\n-2.816422\n-2.196187\n-2.002094\n-1.806302\n-1.613474\n-1.538146\n...\n14.449442\n15.161877\n17.715519\n22.247185\n39.648643\n52.634579\n67.812111\n75.647865\n85.764038\n116.143196\n\n\nGBDOth\n2016-12-01\n6.683663\n-37.663929\n-13.461041\n-2.384047\n-2.037058\n-1.877487\n-1.620457\n-1.436237\n-1.304141\n...\n15.086438\n16.038090\n18.206852\n24.431122\n35.078407\n44.138805\n62.435913\n77.259911\n104.585594\n123.915787\n\n\n\n\n\n6660 rows × 102 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nY_hat_mf\n\n\n  \n    \n      \n\n\n\n\n\n\nds\nXGBRegressor\nXGBRegressor-lo-98\nXGBRegressor-lo-96\nXGBRegressor-lo-94\nXGBRegressor-lo-92\nXGBRegressor-lo-90\nXGBRegressor-lo-88\nXGBRegressor-lo-86\nXGBRegressor-lo-84\n...\nXGBRegressor-hi-80\nXGBRegressor-hi-82\nXGBRegressor-hi-84\nXGBRegressor-hi-86\nXGBRegressor-hi-88\nXGBRegressor-hi-90\nXGBRegressor-hi-92\nXGBRegressor-hi-94\nXGBRegressor-hi-96\nXGBRegressor-hi-98\n\n\nunique_id\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTotalAll\n2016-01-01\n43891.531250\n39048.053886\n39398.911368\n39749.768850\n40100.626332\n40451.483815\n40561.804681\n40586.219613\n40610.634545\n...\n47123.598090\n47148.013022\n47172.427955\n47196.842887\n47221.257819\n47331.578685\n47682.436168\n48033.293650\n48384.151132\n48735.008614\n\n\nTotalAll\n2016-02-01\n20715.656250\n18476.756884\n18482.492537\n18488.228190\n18493.963842\n18499.699495\n18539.212692\n18590.789297\n18642.365902\n...\n22685.793388\n22737.369993\n22788.946598\n22840.523203\n22892.099808\n22931.613005\n22937.348658\n22943.084310\n22948.819963\n22954.555616\n\n\nTotalAll\n2016-03-01\n23008.896484\n17292.312227\n17323.859641\n17355.407055\n17386.954469\n17418.501883\n17582.396869\n17793.558844\n18004.720819\n...\n27590.748200\n27801.910175\n28013.072150\n28224.234125\n28435.396100\n28599.291085\n28630.838500\n28662.385914\n28693.933328\n28725.480742\n\n\nTotalAll\n2016-04-01\n27731.050781\n22333.047144\n22537.510145\n22741.973145\n22946.436145\n23150.899146\n23233.881164\n23273.477118\n23313.073071\n...\n32069.836584\n32109.432538\n32149.028491\n32188.624445\n32228.220398\n32311.202417\n32515.665417\n32720.128417\n32924.591418\n33129.054418\n\n\nTotalAll\n2016-05-01\n24898.529297\n21768.004677\n21859.564615\n21951.124552\n22042.684490\n22134.244428\n22222.835407\n22310.366042\n22397.896678\n...\n27224.100644\n27311.631280\n27399.161916\n27486.692551\n27574.223187\n27662.814166\n27754.374104\n27845.934041\n27937.493979\n28029.053917\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\nGBDOth\n2016-08-01\n8.842277\n-2.504086\n-1.296329\n-0.088571\n1.119187\n2.326944\n2.794274\n2.997165\n3.200056\n...\n14.078715\n14.281606\n14.484497\n14.687388\n14.890279\n15.357609\n16.565366\n17.773124\n18.980882\n20.188639\n\n\nGBDOth\n2016-09-01\n4.991811\n-1.879816\n-1.879816\n-1.879816\n-1.879816\n-1.879816\n-1.635940\n-1.304965\n-0.973990\n...\n10.295662\n10.626637\n10.957612\n11.288587\n11.619561\n11.863438\n11.863438\n11.863438\n11.863438\n11.863438\n\n\nGBDOth\n2016-10-01\n8.647715\n2.339581\n2.339581\n2.339581\n2.339581\n2.339581\n2.339581\n2.339581\n2.339581\n...\n14.955848\n14.955848\n14.955848\n14.955848\n14.955848\n14.955848\n14.955848\n14.955848\n14.955848\n14.955848\n\n\nGBDOth\n2016-11-01\n5.180346\n0.451095\n0.451095\n0.451095\n0.451095\n0.451095\n0.451095\n0.451095\n0.451095\n...\n9.909597\n9.909597\n9.909597\n9.909597\n9.909597\n9.909597\n9.909597\n9.909597\n9.909597\n9.909597\n\n\nGBDOth\n2016-12-01\n6.052622\n1.791351\n1.791351\n1.791351\n1.791351\n1.791351\n1.791351\n1.791351\n1.791351\n...\n10.313892\n10.313892\n10.313892\n10.313892\n10.313892\n10.313892\n10.313892\n10.313892\n10.313892\n10.313892\n\n\n\n\n\n6660 rows × 102 columns"
  },
  {
    "objectID": "examples/mlframeworksexample.html#reconcile-predictions",
    "href": "examples/mlframeworksexample.html#reconcile-predictions",
    "title": "Neural/MLForecast",
    "section": "4. Reconcile Predictions",
    "text": "4. Reconcile Predictions\nWith minimal parsing, we can reconcile the raw output predictions with different HierarchicalForecast reconciliation methods.\n\n\n\n\n\n\nReconciliation Methods Availability\n\n\n\n\n\nThe following reconciliation methods require access to insample predictions: - ERM(method='closed'), ERM(method='reg_bu') - TopDown(method='average_proportions'), TopDown(method='proportion_averages') - MiddleOut(top_down_method='average_proportions'), MiddleOut(top_down_method='proportion_averages') - MinTrace(method='wls_var'), MinTrace(method='mint_cov'), MinTrace(method='mint_shrink')\nYou can obtain NeuralForecast’s insample predictions via the NeuralForecast.predict_insample method.\nWe are working on making MLForecast’s insample predictions available.\n\n\n\n\nreconcilers = [\n    BottomUp(),\n    MinTrace('ols')\n]\nhrec = HierarchicalReconciliation(reconcilers=reconcilers)\n\nY_rec_nf = hrec.reconcile(Y_hat_df=Y_hat_nf, Y_df = Y_train_df, S=S_df, tags=tags, level=level)\nY_rec_mf = hrec.reconcile(Y_hat_df=Y_hat_mf, Y_df = Y_train_df, S=S_df, tags=tags, level=level)"
  },
  {
    "objectID": "examples/mlframeworksexample.html#evaluation",
    "href": "examples/mlframeworksexample.html#evaluation",
    "title": "Neural/MLForecast",
    "section": "5. Evaluation",
    "text": "5. Evaluation\nTo evaluate we use a scaled variation of the CRPS, as proposed by Rangapuram (2021), to measure the accuracy of predicted quantiles y_hat compared to the observation y.\n\\[ \\mathrm{sCRPS}(\\hat{F}_{\\tau}, \\mathbf{y}_{\\tau}) = \\frac{2}{N} \\sum_{i}\n\\int^{1}_{0}\n\\frac{\\mathrm{QL}(\\hat{F}_{i,\\tau}, y_{i,\\tau})_{q}}{\\sum_{i} | y_{i,\\tau} |} dq \\]\n\nrec_model_names_nf = ['NBEATS/BottomUp', 'NBEATS/MinTrace_method-ols']\nrec_model_names_mf = ['XGBRegressor/BottomUp', 'XGBRegressor/MinTrace_method-ols']\n\nn_quantiles = len(quantiles)\nn_series = len(S_df)\n\nfor name in rec_model_names_nf:\n    quantile_columns = [col for col in Y_rec_nf.columns if (name+'-lo') in col or (name+'-hi') in col]\n    y_rec  = Y_rec_nf[quantile_columns].values \n    y_test = Y_test_df['y'].values\n\n    y_rec  = y_rec.reshape(n_series, horizon, n_quantiles)\n    y_test = y_test.reshape(n_series, horizon)\n    scrps  = scaled_crps(y=y_test, y_hat=y_rec, quantiles=quantiles)\n    print(\"{:&lt;40} {:.5f}\".format(name+\":\", scrps))\n\nfor name in rec_model_names_mf:\n    quantile_columns = [col for col in Y_rec_mf.columns if (name+'-lo') in col or (name+'-hi') in col]\n    y_rec  = Y_rec_mf[quantile_columns].values \n    y_test = Y_test_df['y'].values\n\n    y_rec  = y_rec.reshape(n_series, horizon, n_quantiles)\n    y_test = y_test.reshape(n_series, horizon)\n    scrps  = scaled_crps(y=y_test, y_hat=y_rec, quantiles=quantiles)\n    print(\"{:&lt;40} {:.5f}\".format(name+\":\", scrps))\n\nNBEATS/BottomUp:                         0.12853\nNBEATS/MinTrace_method-ols:              0.12945\nXGBRegressor/BottomUp:                   0.13202\nXGBRegressor/MinTrace_method-ols:        0.13417"
  },
  {
    "objectID": "examples/mlframeworksexample.html#visualizations",
    "href": "examples/mlframeworksexample.html#visualizations",
    "title": "Neural/MLForecast",
    "section": "6. Visualizations",
    "text": "6. Visualizations\n\nplot_nf = pd.concat([Y_df.set_index(['unique_id', 'ds']), \n                     Y_rec_nf.set_index('ds', append=True)], axis=1)\nplot_nf = plot_nf.reset_index('ds')\n\nplot_mf = pd.concat([Y_df.set_index(['unique_id', 'ds']), \n                     Y_rec_mf.set_index('ds', append=True)], axis=1)\nplot_mf = plot_mf.reset_index('ds')\n\n\nhplot.plot_series(\n    series='TotalVis',\n    Y_df=plot_nf, \n    models=['y', 'NBEATS', 'NBEATS/BottomUp', 'NBEATS/MinTrace_method-ols'],\n    level=[80]\n)\n\n\n\n\n\nhplot.plot_series(\n    series='TotalVis',\n    Y_df=plot_mf, \n    models=['y', 'XGBRegressor', 'XGBRegressor/BottomUp', 'XGBRegressor/MinTrace_method-ols'],\n    level=[80]\n)"
  },
  {
    "objectID": "examples/index.html",
    "href": "examples/index.html",
    "title": "Tutorials",
    "section": "",
    "text": "Click through to any of these tutorials to get started with HierarchicalForecast’s features.\n\n\n\n\n\n\n\n\n\n\nTitle\n\n\n\n\n\n\nBootstrap\n\n\n\n\nGeographical Aggregation (Prison Population)\n\n\n\n\nGeographical Aggregation (Tourism)\n\n\n\n\nGluonTS\n\n\n\n\nInstall\n\n\n\n\nIntroduction\n\n\n\n\nNeural/MLForecast\n\n\n\n\nNon-Negative MinTrace\n\n\n\n\nNormality\n\n\n\n\nPERMBU\n\n\n\n\nProbabilistic Forecast Evaluation\n\n\n\n\nReconciliation Quick Start\n\n\n\n\n\n\nNo matching items\n\nIf you find the code useful, please ⭐ us on Github"
  },
  {
    "objectID": "examples/tourismlarge-evaluation.html",
    "href": "examples/tourismlarge-evaluation.html",
    "title": "Probabilistic Forecast Evaluation",
    "section": "",
    "text": "This notebook offers a step to step guide to create a hierarchical forecasting pipeline.\nIn the pipeline we will use HierarchicalForecast and StatsForecast core class, to create base predictions, reconcile and evaluate them.\nWe will use the TourismL dataset that summarizes large Australian national visitor survey.\nOutline 1. Installing Packages 2. Prepare TourismL dataset - Read and aggregate - StatsForecast’s Base Predictions 3. Reconciliar 4. Evaluar\nIf you find the code useful, please ⭐ us on Github"
  },
  {
    "objectID": "examples/tourismlarge-evaluation.html#installing-hierarchicalforecast",
    "href": "examples/tourismlarge-evaluation.html#installing-hierarchicalforecast",
    "title": "Probabilistic Forecast Evaluation",
    "section": "1. Installing HierarchicalForecast",
    "text": "1. Installing HierarchicalForecast\nWe assume you have StatsForecast and HierarchicalForecast already installed, if not check this guide for instructions on how to install HierarchicalForecast.\n\n# %%capture\n# !pip install hierarchicalforecast\n# !pip install -U numba statsforecast datasetsforecast\n\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom statsforecast.core import StatsForecast\nfrom statsforecast.models import AutoARIMA, Naive\n\nfrom hierarchicalforecast.core import HierarchicalReconciliation\nfrom hierarchicalforecast.evaluation import HierarchicalEvaluation\nfrom hierarchicalforecast.methods import BottomUp, TopDown, MinTrace, ERM\n\nfrom hierarchicalforecast.utils import is_strictly_hierarchical\nfrom hierarchicalforecast.utils import HierarchicalPlot, CodeTimer\nfrom hierarchicalforecast.evaluation import scaled_crps, msse, energy_score\n\nfrom datasetsforecast.hierarchical import HierarchicalData, HierarchicalInfo\n\n/Users/cchallu/opt/anaconda3/envs/hierarchicalforecast/lib/python3.10/site-packages/statsforecast/core.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from tqdm.autonotebook import tqdm"
  },
  {
    "objectID": "examples/tourismlarge-evaluation.html#preparing-tourisml-dataset",
    "href": "examples/tourismlarge-evaluation.html#preparing-tourisml-dataset",
    "title": "Probabilistic Forecast Evaluation",
    "section": "2. Preparing TourismL Dataset",
    "text": "2. Preparing TourismL Dataset\n\n2.1 Read Hierarchical Dataset\n\n# ['Labour', 'Traffic', 'TourismSmall', 'TourismLarge', 'Wiki2']\ndataset = 'TourismSmall' # 'TourismLarge'\nverbose = True\nintervals_method = 'bootstrap'\nLEVEL = np.arange(0, 100, 2)\nqs = [[50-lv/2, 50+lv/2] for lv in LEVEL]\nQUANTILES = np.sort(np.concatenate(qs)/100)\n\n\nwith CodeTimer('Read and Parse data   ', verbose):\n    print(f'{dataset}')\n    if not os.path.exists('./data'):\n        os.makedirs('./data')\n    \n    dataset_info = HierarchicalInfo[dataset]\n    Y_df, S_df, tags = HierarchicalData.load(directory=f'./data/{dataset}', group=dataset)\n    Y_df['ds'] = pd.to_datetime(Y_df['ds'])\n\n    # Train/Test Splits\n    horizon = dataset_info.horizon\n    seasonality = dataset_info.seasonality\n    Y_test_df = Y_df.groupby('unique_id').tail(horizon)\n    Y_train_df = Y_df.drop(Y_test_df.index)\n    Y_test_df = Y_test_df.set_index('unique_id')\n    Y_train_df = Y_train_df.set_index('unique_id')\n\nTourismSmall\nCode block 'Read and Parse data   ' took:   0.99873 seconds\n\n\n100%|██████████| 1.30M/1.30M [00:00&lt;00:00, 2.74MiB/s]\nINFO:datasetsforecast.utils:Successfully downloaded datasets.zip, 1297279, bytes.\nINFO:datasetsforecast.utils:Decompressing zip file...\nINFO:datasetsforecast.utils:Successfully decompressed data/TourismSmall/hierarchical/datasets.zip\n\n\n\ndataset_info.seasonality\n\n4\n\n\n\nhplot = HierarchicalPlot(S=S_df, tags=tags)\nhplot.plot_summing_matrix()\n\n\n\n\n\nY_train_df\n\n\n\n\n\n\n\n\nds\ny\n\n\nunique_id\n\n\n\n\n\n\ntotal\n1998-03-31\n84503\n\n\ntotal\n1998-06-30\n65312\n\n\ntotal\n1998-09-30\n72753\n\n\ntotal\n1998-12-31\n70880\n\n\ntotal\n1999-03-31\n86893\n\n\n...\n...\n...\n\n\nnt-oth-noncity\n2003-12-31\n132\n\n\nnt-oth-noncity\n2004-03-31\n12\n\n\nnt-oth-noncity\n2004-06-30\n40\n\n\nnt-oth-noncity\n2004-09-30\n186\n\n\nnt-oth-noncity\n2004-12-31\n144\n\n\n\n\n2492 rows × 2 columns\n\n\n\n\n\n2.2 StatsForecast’s Base Predictions\nThis cell computes the base predictions Y_hat_df for all the series in Y_df using StatsForecast’s AutoARIMA. Additionally we obtain insample predictions Y_fitted_df for the methods that require them.\n\nwith CodeTimer('Fit/Predict Model     ', verbose):\n    # Read to avoid unnecesary AutoARIMA computation\n    yhat_file = f'./data/{dataset}/Y_hat.csv'\n    yfitted_file = f'./data/{dataset}/Y_fitted.csv'\n\n    if os.path.exists(yhat_file):\n        Y_hat_df = pd.read_csv(yhat_file)\n        Y_fitted_df = pd.read_csv(yfitted_file)\n\n        Y_hat_df = Y_hat_df.set_index('unique_id')\n        Y_fitted_df = Y_fitted_df.set_index('unique_id')\n\n    else:\n        fcst = StatsForecast(\n            df=Y_train_df, \n            models=[AutoARIMA(season_length=seasonality)],\n            fallback_model=[Naive()],\n            freq='M', \n            n_jobs=-1\n        )\n        Y_hat_df = fcst.forecast(h=horizon, fitted=True, level=LEVEL)\n        Y_fitted_df = fcst.forecast_fitted_values()\n        Y_hat_df.to_csv(yhat_file)\n        Y_fitted_df.to_csv(yfitted_file)\n\n\nY_hat_df\n\n\n\n\n\n\n\n\nds\nAutoARIMA\nAutoARIMA-lo-98\nAutoARIMA-lo-96\nAutoARIMA-lo-94\nAutoARIMA-lo-92\nAutoARIMA-lo-90\nAutoARIMA-lo-88\nAutoARIMA-lo-86\nAutoARIMA-lo-84\n...\nAutoARIMA-hi-80\nAutoARIMA-hi-82\nAutoARIMA-hi-84\nAutoARIMA-hi-86\nAutoARIMA-hi-88\nAutoARIMA-hi-90\nAutoARIMA-hi-92\nAutoARIMA-hi-94\nAutoARIMA-hi-96\nAutoARIMA-hi-98\n\n\nunique_id\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbus\n2005-01-31\n9673.424805\n7436.356445\n7698.493652\n7864.811523\n7989.925781\n8091.696289\n8178.319336\n8254.270508\n8322.276367\n...\n10905.793945\n10962.725586\n11024.573242\n11092.579102\n11168.530273\n11255.153320\n11356.923828\n11482.038086\n11648.356445\n11910.493164\n\n\nbus\n2005-02-28\n10393.900391\n8156.831543\n8418.968750\n8585.287109\n8710.401367\n8812.171875\n8898.794922\n8974.746094\n9042.751953\n...\n11626.269531\n11683.200195\n11745.048828\n11813.054688\n11889.005859\n11975.628906\n12077.399414\n12202.513672\n12368.832031\n12630.968750\n\n\nbus\n2005-03-31\n12028.134766\n9791.066406\n10053.204102\n10219.521484\n10344.635742\n10446.406250\n10533.029297\n10608.981445\n10676.986328\n...\n13260.503906\n13317.435547\n13379.283203\n13447.289062\n13523.240234\n13609.863281\n13711.633789\n13836.748047\n14003.066406\n14265.203125\n\n\nbus\n2005-04-30\n10995.679688\n8758.610352\n9020.748047\n9187.065430\n9312.179688\n9413.951172\n9500.574219\n9576.525391\n9644.531250\n...\n12228.047852\n12284.979492\n12346.828125\n12414.833008\n12490.785156\n12577.407227\n12679.178711\n12804.292969\n12970.610352\n13232.748047\n\n\nbus\n2005-05-31\n9673.424805\n7262.085449\n7544.643555\n7723.917480\n7858.778320\n7968.477539\n8061.848633\n8143.716797\n8217.019531\n...\n11001.796875\n11063.164062\n11129.830078\n11203.132812\n11285.000977\n11378.372070\n11488.071289\n11622.932617\n11802.206055\n12084.764648\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\nwa-vfr-noncity\n2005-04-30\n904.125549\n463.371521\n515.018616\n547.787048\n572.437439\n592.488647\n609.555359\n624.519531\n637.918213\n...\n1146.930542\n1158.147339\n1170.332886\n1183.731567\n1198.695679\n1215.762451\n1235.813721\n1260.464111\n1293.232544\n1344.879517\n\n\nwa-vfr-noncity\n2005-05-31\n904.125549\n457.607361\n509.929901\n543.126831\n568.099670\n588.413086\n605.703003\n620.862854\n634.436707\n...\n1150.105957\n1161.469482\n1173.814331\n1187.388184\n1202.548096\n1219.838013\n1240.151489\n1265.124268\n1298.321167\n1350.643677\n\n\nwa-vfr-noncity\n2005-06-30\n904.125549\n451.916687\n504.906036\n538.526062\n563.817139\n584.389465\n601.899719\n617.252808\n630.999634\n...\n1153.240967\n1164.749268\n1177.251465\n1190.998291\n1206.351440\n1223.861694\n1244.433960\n1269.724976\n1303.345093\n1356.334473\n\n\nwa-vfr-noncity\n2005-07-31\n904.125549\n446.296722\n499.944611\n533.982483\n559.587830\n580.415833\n598.143738\n613.687622\n627.605286\n...\n1156.336914\n1167.988159\n1180.645752\n1194.563477\n1210.107422\n1227.835327\n1248.663208\n1274.268677\n1308.306519\n1361.954346\n\n\nwa-vfr-noncity\n2005-08-31\n904.125549\n440.744904\n495.043365\n529.493958\n555.409851\n576.490417\n594.433289\n610.165649\n624.252136\n...\n1159.395264\n1171.187866\n1183.999023\n1198.085449\n1213.817871\n1231.760742\n1252.841309\n1278.757080\n1313.207764\n1367.506226\n\n\n\n\n712 rows × 102 columns\n\n\n\n\nY_fitted_df\n\n\n\n\n\n\n\n\nds\ny\nAutoARIMA\nAutoARIMA-lo-98\nAutoARIMA-lo-96\nAutoARIMA-lo-94\nAutoARIMA-lo-92\nAutoARIMA-lo-90\nAutoARIMA-lo-88\nAutoARIMA-lo-86\n...\nAutoARIMA-hi-80\nAutoARIMA-hi-82\nAutoARIMA-hi-84\nAutoARIMA-hi-86\nAutoARIMA-hi-88\nAutoARIMA-hi-90\nAutoARIMA-hi-92\nAutoARIMA-hi-94\nAutoARIMA-hi-96\nAutoARIMA-hi-98\n\n\nunique_id\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbus\n1998-03-31\n9815.0\n9805.184570\n7568.648926\n7830.724121\n7997.001953\n8122.086426\n8223.833008\n8310.435547\n8386.369141\n...\n11037.260742\n11094.178711\n11156.011719\n11224.000977\n11299.934570\n11386.537109\n11488.283203\n11613.368164\n11779.646484\n12041.720703\n\n\nbus\n1998-06-30\n11823.0\n11811.176758\n9574.640625\n9836.715820\n10002.994141\n10128.078125\n10229.825195\n10316.427734\n10392.361328\n...\n13043.252930\n13100.169922\n13162.003906\n13229.993164\n13305.926758\n13392.528320\n13494.275391\n13619.360352\n13785.637695\n14047.712891\n\n\nbus\n1998-09-30\n13565.0\n13551.434570\n11314.899414\n11576.973633\n11743.251953\n11868.336914\n11970.083008\n12056.685547\n12132.619141\n...\n14783.510742\n14840.428711\n14902.261719\n14970.250977\n15046.184570\n15132.787109\n15234.533203\n15359.618164\n15525.896484\n15787.970703\n\n\nbus\n1998-12-31\n11478.0\n11466.522461\n9229.986328\n9492.060547\n9658.338867\n9783.423828\n9885.169922\n9971.772461\n10047.706055\n...\n12698.597656\n12755.515625\n12817.348633\n12885.337891\n12961.271484\n13047.874023\n13149.620117\n13274.705078\n13440.983398\n13703.057617\n\n\nbus\n1999-03-31\n10027.0\n9845.011719\n7608.475586\n7870.550781\n8036.828613\n8161.913086\n8263.660156\n8350.262695\n8426.195312\n...\n11077.086914\n11134.004883\n11195.838867\n11263.828125\n11339.760742\n11426.363281\n11528.110352\n11653.194336\n11819.472656\n12081.547852\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\nwa-vfr-noncity\n2003-12-31\n1177.0\n927.351196\n504.362732\n553.928040\n585.375671\n609.032471\n628.275513\n644.654297\n659.015320\n...\n1160.369507\n1171.134155\n1182.828491\n1195.687012\n1210.048096\n1226.426880\n1245.669922\n1269.326660\n1300.774292\n1350.339600\n\n\nwa-vfr-noncity\n2004-03-31\n956.0\n969.565552\n546.577087\n596.142456\n627.590027\n651.246887\n670.489868\n686.868652\n701.229675\n...\n1202.583862\n1213.348511\n1225.042847\n1237.901489\n1252.262451\n1268.641235\n1287.884277\n1311.541016\n1342.988647\n1392.554077\n\n\nwa-vfr-noncity\n2004-06-30\n772.0\n967.268921\n544.280457\n593.845764\n625.293396\n648.950195\n668.193237\n684.572021\n698.933044\n...\n1200.287109\n1211.051880\n1222.746216\n1235.604736\n1249.965820\n1266.344604\n1285.587646\n1309.244385\n1340.692017\n1390.257324\n\n\nwa-vfr-noncity\n2004-09-30\n885.0\n934.251831\n511.263336\n560.828674\n592.276306\n615.933105\n635.176086\n651.554932\n665.915955\n...\n1167.270020\n1178.034790\n1189.729126\n1202.587646\n1216.948730\n1233.327515\n1252.570557\n1276.227295\n1307.674927\n1357.240234\n\n\nwa-vfr-noncity\n2004-12-31\n797.0\n925.923462\n502.934998\n552.500305\n583.947937\n607.604736\n626.847778\n643.226562\n657.587585\n...\n1158.941772\n1169.706421\n1181.400757\n1194.259277\n1208.620361\n1224.999146\n1244.242188\n1267.898926\n1299.346558\n1348.911865\n\n\n\n\n2492 rows × 103 columns"
  },
  {
    "objectID": "examples/tourismlarge-evaluation.html#reconciliate-predictions",
    "href": "examples/tourismlarge-evaluation.html#reconciliate-predictions",
    "title": "Probabilistic Forecast Evaluation",
    "section": "3. Reconciliate Predictions",
    "text": "3. Reconciliate Predictions\n\nwith CodeTimer('Reconcile Predictions ', verbose):\n    if is_strictly_hierarchical(S=S_df.values.astype(np.float32), \n        tags={key: S_df.index.get_indexer(val) for key, val in tags.items()}):\n        reconcilers = [\n            BottomUp(),\n            TopDown(method='average_proportions'),\n            TopDown(method='proportion_averages'),\n            MinTrace(method='ols'),\n            MinTrace(method='wls_var'),\n            MinTrace(method='mint_shrink'),\n            #ERM(method='reg_bu', lambda_reg=100) # Extremely inneficient\n            ERM(method='closed')\n        ]\n    else:\n        reconcilers = [\n            BottomUp(),\n            MinTrace(method='ols'),\n            MinTrace(method='wls_var'),\n            MinTrace(method='mint_shrink'),\n            #ERM(method='reg_bu', lambda_reg=100) # Extremely inneficient\n            ERM(method='closed')\n        ]\n    \n    hrec = HierarchicalReconciliation(reconcilers=reconcilers)\n    Y_rec_df = hrec.bootstrap_reconcile(Y_hat_df=Y_hat_df,\n                                        Y_df=Y_fitted_df,\n                                        S_df=S_df, tags=tags,\n                                        level=LEVEL,\n                                        intervals_method=intervals_method,\n                                        num_samples=10, num_seeds=10)\n\n    # Matching Y_test/Y_rec/S index ordering\n    Y_test_df = Y_test_df.reset_index()\n    Y_test_df.unique_id = Y_test_df.unique_id.astype('category')\n    Y_test_df.unique_id = Y_test_df.unique_id.cat.set_categories(S_df.index)\n    Y_test_df = Y_test_df.sort_values(by=['unique_id', 'ds'])\n\n    Y_rec_df = Y_rec_df.reset_index()\n    Y_rec_df.unique_id = Y_rec_df.unique_id.astype('category')\n    Y_rec_df.unique_id = Y_rec_df.unique_id.cat.set_categories(S_df.index)\n    Y_rec_df = Y_rec_df.sort_values(by=['seed', 'unique_id', 'ds'])\n\n    # Parsing model level columns\n    flat_cols = list(hrec.level_names.keys())\n    for model in hrec.level_names:\n        flat_cols += hrec.level_names[model]\n    for model in hrec.sample_names:\n        flat_cols += hrec.sample_names[model]\n    y_rec  = Y_rec_df[flat_cols]\n    model_columns = y_rec.columns\n\n    n_series = len(S_df)\n    n_seeds = len(Y_rec_df.seed.unique())\n    y_rec  = y_rec.values.reshape(n_seeds, n_series, horizon, len(model_columns))\n    y_test = Y_test_df['y'].values.reshape(n_series, horizon)\n    y_train = Y_train_df['y'].values.reshape(n_series, -1)\n\nCode block 'Reconcile Predictions ' took:   11.73492 seconds\n\n\n\n# Qualitative evaluation, of parsed quantiles\nrow_idx = 0\nseed_idx = 0\ncol_idxs = model_columns.get_indexer(hrec.level_names['AutoARIMA/BottomUp'])\nfor i, col in enumerate(col_idxs):\n    plt.plot(y_rec[seed_idx, row_idx,:,col], color='orange', alpha=i/100)\nfor i, col in enumerate(col_idxs):\n    plt.plot(y_rec[seed_idx+1, row_idx,:,col], color='green', alpha=i/100)\nplt.plot(y_test[row_idx,:], label='True')\nplt.title(f'{S_df.index[row_idx]} Visits \\n' + \\\n          f'AutoARIMA/BottomUp-{intervals_method}')\n\nplt.legend()\nplt.grid()\nplt.show()\nplt.close()\n\n\n\n\n\n#Y_rec_df\ntd_levels = hrec.level_names['AutoARIMA/TopDown_method-average_proportions']\nY_rec_df[td_levels]\n\n\n\n\n\n\n\n\nAutoARIMA/TopDown_method-average_proportions-lo-98\nAutoARIMA/TopDown_method-average_proportions-lo-96\nAutoARIMA/TopDown_method-average_proportions-lo-94\nAutoARIMA/TopDown_method-average_proportions-lo-92\nAutoARIMA/TopDown_method-average_proportions-lo-90\nAutoARIMA/TopDown_method-average_proportions-lo-88\nAutoARIMA/TopDown_method-average_proportions-lo-86\nAutoARIMA/TopDown_method-average_proportions-lo-84\nAutoARIMA/TopDown_method-average_proportions-lo-82\nAutoARIMA/TopDown_method-average_proportions-lo-80\n...\nAutoARIMA/TopDown_method-average_proportions-hi-80\nAutoARIMA/TopDown_method-average_proportions-hi-82\nAutoARIMA/TopDown_method-average_proportions-hi-84\nAutoARIMA/TopDown_method-average_proportions-hi-86\nAutoARIMA/TopDown_method-average_proportions-hi-88\nAutoARIMA/TopDown_method-average_proportions-hi-90\nAutoARIMA/TopDown_method-average_proportions-hi-92\nAutoARIMA/TopDown_method-average_proportions-hi-94\nAutoARIMA/TopDown_method-average_proportions-hi-96\nAutoARIMA/TopDown_method-average_proportions-hi-98\n\n\n\n\n0\n80750.389920\n80750.389920\n80750.389920\n82299.061781\n82299.061781\n82299.061781\n82600.022716\n82600.022716\n82600.022716\n82763.007090\n...\n88248.624229\n88248.624229\n88248.624229\n88248.624229\n88384.153447\n90507.444522\n90507.444522\n90507.444522\n90507.444522\n90507.444522\n\n\n1\n61825.843210\n61825.843210\n61825.843210\n61825.843210\n61825.843210\n61825.843210\n63374.515072\n63374.515072\n63374.515072\n63374.515072\n...\n68196.499405\n68196.499405\n68286.705654\n69324.077520\n69324.077520\n69437.018534\n71582.897812\n71582.897812\n71582.897812\n71582.897812\n\n\n2\n68249.624404\n68249.624404\n68249.624404\n68249.624404\n69798.296266\n69798.296266\n69798.296266\n69798.296266\n69798.296266\n69798.296266\n...\n74620.280598\n74620.280598\n74620.280598\n74699.211067\n75747.858714\n75747.858714\n75747.858714\n75815.623322\n78006.679006\n78006.679006\n\n\n3\n67456.030661\n67456.030661\n67456.030661\n67456.030661\n69004.702523\n69004.702523\n69004.702523\n69004.702523\n69004.702523\n69305.663457\n...\n73939.444667\n74954.264971\n74954.264971\n74954.264971\n74954.264971\n74954.264971\n75044.617782\n77213.085263\n77213.085263\n77213.085263\n\n\n4\n80371.819611\n80371.819611\n80371.819611\n80371.819611\n80371.819611\n81827.571161\n81920.491472\n81920.491472\n81920.491472\n81920.491472\n...\n87870.053920\n87870.053920\n87870.053920\n87870.053920\n88005.583138\n90128.874213\n90128.874213\n90128.874213\n90128.874213\n90128.874213\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n7115\n132.959504\n132.959504\n132.959504\n132.959504\n132.959504\n135.828870\n136.012021\n136.012021\n136.012021\n136.545910\n...\n147.738932\n147.738932\n147.738932\n152.191189\n152.191189\n152.191189\n152.191189\n152.191189\n152.191189\n152.191189\n\n\n7116\n158.417227\n158.417227\n158.417227\n158.417227\n158.417227\n161.469743\n161.469743\n161.469743\n161.469743\n162.062953\n...\n170.974136\n171.174163\n173.196654\n173.196654\n173.196654\n173.419267\n177.648912\n177.648912\n177.648912\n177.648912\n\n\n7117\n122.066765\n122.066765\n125.119281\n125.119281\n125.119281\n125.119281\n125.712492\n125.712492\n125.712492\n125.712492\n...\n134.623675\n134.623675\n134.801476\n136.846192\n136.846192\n136.846192\n137.024283\n141.298450\n141.298450\n141.298450\n\n\n7118\n134.467576\n134.467576\n134.467576\n134.467576\n137.520093\n137.520093\n137.520093\n137.520093\n138.113303\n138.113303\n...\n145.762241\n145.875843\n147.202288\n149.247004\n149.247004\n149.247004\n149.425094\n153.699262\n153.699262\n153.699262\n\n\n7119\n135.996894\n136.027420\n136.027420\n136.027420\n136.027420\n136.027420\n136.027420\n136.573173\n136.620630\n136.620630\n...\n145.531813\n145.531813\n145.709614\n147.754331\n147.754331\n147.754331\n147.932421\n152.206588\n152.206588\n152.206588\n\n\n\n\n7120 rows × 100 columns"
  },
  {
    "objectID": "examples/tourismlarge-evaluation.html#evaluation",
    "href": "examples/tourismlarge-evaluation.html#evaluation",
    "title": "Probabilistic Forecast Evaluation",
    "section": "4. Evaluation",
    "text": "4. Evaluation\n\nwith CodeTimer('Evaluate Models CRPS  ', verbose):\n    crps_results = {'Dataset': [dataset] * len(['Overall'] + list(tags.keys())),\n                    'Level': ['Overall'] + list(tags.keys()),}\n\n    for model in hrec.level_names.keys():\n        crps_results[model] = []\n        for level in crps_results['Level']:\n            if level=='Overall':\n                row_idxs = np.arange(len(S_df))\n            else:\n                row_idxs = S_df.index.get_indexer(tags[level])\n            col_idxs = model_columns.get_indexer(hrec.level_names[model])\n            _y = y_test[row_idxs,:]\n            _y_rec_seeds = y_rec[:,row_idxs,:,:][:,:,:,col_idxs]\n\n            level_model_crps = []\n            for seed_idx in range(y_rec.shape[0]):\n                _y_rec = _y_rec_seeds[seed_idx,:,:,:]\n                level_model_crps.append(scaled_crps(y=_y, y_hat=_y_rec,\n                                                    quantiles=QUANTILES))\n            level_model_crps = f'{np.mean(level_model_crps):.4f}±{(1.96 * np.std(level_model_crps)):.4f}'\n            crps_results[model].append(level_model_crps)\n\n    crps_results = pd.DataFrame(crps_results)\n\ncrps_results\n\nCode block 'Evaluate Models CRPS  ' took:   1.13514 seconds\n\n\n\n\n\n\n\n\n\nDataset\nLevel\nAutoARIMA/BottomUp\nAutoARIMA/TopDown_method-average_proportions\nAutoARIMA/TopDown_method-proportion_averages\nAutoARIMA/MinTrace_method-ols\nAutoARIMA/MinTrace_method-wls_var\nAutoARIMA/MinTrace_method-mint_shrink\nAutoARIMA/ERM_method-closed_lambda_reg-0.01\n\n\n\n\n0\nTourismSmall\nOverall\n0.0895±0.0012\n0.1195±0.0008\n0.1197±0.0008\n0.0927±0.0010\n0.0890±0.0010\n0.0898±0.0009\n0.1116±0.0015\n\n\n1\nTourismSmall\nCountry\n0.0481±0.0016\n0.0479±0.0011\n0.0479±0.0011\n0.0504±0.0010\n0.0510±0.0011\n0.0512±0.0011\n0.0525±0.0015\n\n\n2\nTourismSmall\nCountry/Purpose\n0.0699±0.0016\n0.0928±0.0009\n0.0931±0.0009\n0.0804±0.0012\n0.0724±0.0012\n0.0741±0.0012\n0.0927±0.0015\n\n\n3\nTourismSmall\nCountry/Purpose/State\n0.1085±0.0011\n0.1575±0.0009\n0.1579±0.0009\n0.1082±0.0011\n0.1043±0.0009\n0.1049±0.0008\n0.1325±0.0018\n\n\n4\nTourismSmall\nCountry/Purpose/State/CityNonCity\n0.1316±0.0012\n0.1799±0.0008\n0.1800±0.0008\n0.1319±0.0013\n0.1282±0.0011\n0.1290±0.0010\n0.1685±0.0029\n\n\n\n\n\n\n\n\nwith CodeTimer('Evaluate Models MSSE  ', verbose):\n    msse_results = {'Dataset': [dataset] * len(['Overall'] + list(tags.keys())),\n                    'Level': ['Overall'] + list(tags.keys()),}\n    for model in hrec.level_names.keys():\n        msse_results[model] = []\n        for level in msse_results['Level']:\n            if level=='Overall':\n                row_idxs = np.arange(len(S_df))\n            else:\n                row_idxs = S_df.index.get_indexer(tags[level])\n            col_idx = model_columns.get_loc(model)\n            _y = y_test[row_idxs,:]\n            _y_train = y_train[row_idxs,:]\n            _y_hat_seeds = y_rec[:,row_idxs,:,:][:,:,:,col_idx]\n\n            level_model_msse = []\n            for seed_idx in range(y_rec.shape[0]):\n                _y_hat = _y_hat_seeds[seed_idx,:,:]\n                level_model_msse.append(msse(y=_y, y_hat=_y_hat, y_train=_y_train))\n            #level_model_msse = f'{np.mean(level_model_msse):.4f}±{(1.96 * np.std(level_model_msse)):.4f}'\n            level_model_msse = f'{np.mean(level_model_msse):.4f}'\n            msse_results[model].append(level_model_msse)\n\n    msse_results = pd.DataFrame(msse_results)\n\nmsse_results\n\nCode block 'Evaluate Models MSSE  ' took:   0.73303 seconds\n\n\n\n\n\n\n\n\n\nDataset\nLevel\nAutoARIMA/BottomUp\nAutoARIMA/TopDown_method-average_proportions\nAutoARIMA/TopDown_method-proportion_averages\nAutoARIMA/MinTrace_method-ols\nAutoARIMA/MinTrace_method-wls_var\nAutoARIMA/MinTrace_method-mint_shrink\nAutoARIMA/ERM_method-closed_lambda_reg-0.01\n\n\n\n\n0\nTourismSmall\nOverall\n0.2530\n0.3628\n0.3649\n0.3039\n0.2789\n0.2822\n0.3942\n\n\n1\nTourismSmall\nCountry\n0.2564\n0.3180\n0.3180\n0.3522\n0.3381\n0.3394\n0.4117\n\n\n2\nTourismSmall\nCountry/Purpose\n0.2018\n0.3178\n0.3203\n0.2557\n0.2122\n0.2175\n0.3346\n\n\n3\nTourismSmall\nCountry/Purpose/State\n0.3231\n0.5077\n0.5114\n0.2943\n0.2858\n0.2890\n0.4534\n\n\n4\nTourismSmall\nCountry/Purpose/State/CityNonCity\n0.3423\n0.5047\n0.5099\n0.3238\n0.3083\n0.3115\n0.4791\n\n\n\n\n\n\n\n\nwith CodeTimer('Evaluate Models EScore', verbose):\n    energy_results = {'Dataset': [dataset] * len(['Overall'] + list(tags.keys())),\n                        'Level': ['Overall'] + list(tags.keys()),}\n    for model in hrec.sample_names.keys():\n        energy_results[model] = []\n        for level in energy_results['Level']:\n            if level=='Overall':\n                row_idxs = np.arange(len(S_df))\n            else:\n                row_idxs = S_df.index.get_indexer(tags[level])\n            col_idxs = model_columns.get_indexer(hrec.sample_names[model])\n            _y = y_test[row_idxs,:]\n            _y_sample1 = y_rec[0,row_idxs,:,:][:,:,col_idxs[:len(col_idxs)//2]]\n            _y_sample2 = y_rec[0,row_idxs,:,:][:,:,col_idxs[len(col_idxs)//2:]]\n            level_model_energy = energy_score(y=_y, \n                                              y_sample1=_y_sample1,\n                                              y_sample2=_y_sample2,\n                                              beta=2)\n            energy_results[model].append(level_model_energy)\n    energy_results = pd.DataFrame(energy_results)\n\nenergy_results\n\nCode block 'Evaluate Models EScore' took:   0.19443 seconds\n\n\n\n\n\n\n\n\n\nDataset\nLevel\nAutoARIMA/BottomUp\nAutoARIMA/TopDown_method-average_proportions\nAutoARIMA/TopDown_method-proportion_averages\nAutoARIMA/MinTrace_method-ols\nAutoARIMA/MinTrace_method-wls_var\nAutoARIMA/MinTrace_method-mint_shrink\nAutoARIMA/ERM_method-closed_lambda_reg-0.01\n\n\n\n\n0\nTourismSmall\nOverall\n6.874103e+07\n7.917294e+07\n7.962361e+07\n6.930268e+07\n6.914837e+07\n6.955018e+07\n8.235776e+07\n\n\n1\nTourismSmall\nCountry\n3.292999e+07\n2.757131e+07\n2.757129e+07\n3.081254e+07\n3.392861e+07\n3.353851e+07\n3.350023e+07\n\n\n2\nTourismSmall\nCountry/Purpose\n1.894485e+07\n2.661024e+07\n2.683828e+07\n2.218952e+07\n1.932895e+07\n1.984161e+07\n2.681792e+07\n\n\n3\nTourismSmall\nCountry/Purpose/State\n9.393103e+06\n1.408613e+07\n1.419471e+07\n9.016056e+06\n8.778983e+06\n8.928542e+06\n1.211747e+07\n\n\n4\nTourismSmall\nCountry/Purpose/State/CityNonCity\n7.473085e+06\n1.090527e+07\n1.101934e+07\n7.284562e+06\n7.111832e+06\n7.241519e+06\n9.922145e+06"
  },
  {
    "objectID": "examples/tourismlarge-evaluation.html#references",
    "href": "examples/tourismlarge-evaluation.html#references",
    "title": "Probabilistic Forecast Evaluation",
    "section": "References",
    "text": "References\n\nSyama Sundar Rangapuram, Lucien D Werner, Konstantinos Benidis, Pedro Mercado, Jan Gasthaus, Tim Januschowski. (2021). \"End-to-End Learning of Coherent Probabilistic Forecasts for Hierarchical Time Series\". Proceedings of the 38th International Conference on Machine Learning (ICML).\nKin G. Olivares, O. Nganba Meetei, Ruijun Ma, Rohan Reddy, Mengfei Cao, Lee Dicker (2022). “Probabilistic Hierarchical Forecasting with Deep Poisson Mixtures”. Submitted to the International Journal Forecasting, Working paper available at arxiv."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hierarchical Forecast 👑",
    "section": "",
    "text": "Large collections of time series organized into structures at different aggregation levels often require their forecasts to follow their aggregation constraints, which poses the challenge of creating novel algorithms capable of coherent forecasts.\nHierarchicalForecast offers a collection of reconciliation methods, including BottomUp, TopDown, MiddleOut, MinTrace and ERM. And Probabilistic coherent predictions including Normality, Bootstrap, and PERMBU.\nIf you find the code useful, please ⭐ us on Github"
  },
  {
    "objectID": "index.html#features",
    "href": "index.html#features",
    "title": "Hierarchical Forecast 👑",
    "section": "🎊 Features",
    "text": "🎊 Features\n\nClassic reconciliation methods:\n\nBottomUp: Simple addition to the upper levels.\nTopDown: Distributes the top levels forecasts trough the hierarchies.\n\nAlternative reconciliation methods:\n\nMiddleOut: It anchors the base predictions in a middle level. The levels above the base predictions use the bottom-up approach, while the levels below use a top-down.\nMinTrace: Minimizes the total forecast variance of the space of coherent forecasts, with the Minimum Trace reconciliation.\nERM: Optimizes the reconciliation matrix minimizing an L1 regularized objective.\n\nProbabilistic coherent methods:\n\nNormality: Uses MinTrace variance-covariance closed form matrix under a normality assumption.\nBootstrap: Generates distribution of hierarchically reconciled predictions using Gamakumara’s bootstrap approach.\nPERMBU: Reconciles independent sample predictions by reinjecting multivariate dependence with estimated rank permutation copulas, and performing a Bottom-Up aggregation.\n\n\nMissing something? Please open an issue here or write us in"
  },
  {
    "objectID": "index.html#why",
    "href": "index.html#why",
    "title": "Hierarchical Forecast 👑",
    "section": "📖 Why?",
    "text": "📖 Why?\nShort: We want to contribute to the ML field by providing reliable baselines and benchmarks for hierarchical forecasting task in industry and academia. Here’s the complete paper.\nVerbose: HierarchicalForecast integrates publicly available processed datasets, evaluation metrics, and a curated set of statistical baselines. In this library we provide usage examples and references to extensive experiments where we showcase the baseline’s use and evaluate the accuracy of their predictions. With this work, we hope to contribute to Machine Learning forecasting by bridging the gap to statistical and econometric modeling, as well as providing tools for the development of novel hierarchical forecasting algorithms rooted in a thorough comparison of these well-established models. We intend to continue maintaining and increasing the repository, promoting collaboration across the forecasting community."
  },
  {
    "objectID": "index.html#installation",
    "href": "index.html#installation",
    "title": "Hierarchical Forecast 👑",
    "section": "💻 Installation",
    "text": "💻 Installation\n\nPyPI\nYou can install the released version of HierarchicalForecast from the Python package index with:\npip install hierarchicalforecast\n(Installing inside a python virtualenvironment or a conda environment is recommended.)\n\n\nConda\nAlso you can install the released version of HierarchicalForecast from conda with:\nconda install -c conda-forge hierarchicalforecast\n(Installing inside a python virtualenvironment or a conda environment is recommended.)\n\n\nDev Mode\nIf you want to make some modifications to the code and see the effects in real time (without reinstalling), follow the steps below:\ngit clone https://github.com/Nixtla/hierarchicalforecast.git\ncd hierarchicalforecast\npip install -e ."
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "Hierarchical Forecast 👑",
    "section": "🧬 How to use",
    "text": "🧬 How to use\nThe following example needs statsforecast and datasetsforecast as additional packages. If not installed, install it via your preferred method, e.g. pip install statsforecast datasetsforecast. The datasetsforecast library allows us to download hierarhical datasets and we will use statsforecast to compute base forecasts to be reconciled.\nYou can open this example in Colab \nimport numpy as np\nimport pandas as pd\n\n#obtain hierarchical dataset\nfrom datasetsforecast.hierarchical import HierarchicalData\n\n# compute base forecast no coherent\nfrom statsforecast.core import StatsForecast\nfrom statsforecast.models import AutoARIMA, Naive\n\n#obtain hierarchical reconciliation methods and evaluation\nfrom hierarchicalforecast.core import HierarchicalReconciliation\nfrom hierarchicalforecast.evaluation import HierarchicalEvaluation\nfrom hierarchicalforecast.methods import BottomUp, TopDown, MiddleOut\n\n\n# Load TourismSmall dataset\nY_df, S, tags = HierarchicalData.load('./data', 'TourismSmall')\nY_df['ds'] = pd.to_datetime(Y_df['ds'])\n\n#split train/test sets\nY_test_df  = Y_df.groupby('unique_id').tail(4)\nY_train_df = Y_df.drop(Y_test_df.index)\n\n# Compute base auto-ARIMA predictions\nfcst = StatsForecast(df=Y_train_df,\n                     models=[AutoARIMA(season_length=4), Naive()],\n                     freq='Q', n_jobs=-1)\nY_hat_df = fcst.forecast(h=4)\n\n# Reconcile the base predictions\nreconcilers = [\n    BottomUp(),\n    TopDown(method='forecast_proportions'),\n    MiddleOut(middle_level='Country/Purpose/State',\n              top_down_method='forecast_proportions')\n]\nhrec = HierarchicalReconciliation(reconcilers=reconcilers)\nY_rec_df = hrec.reconcile(Y_hat_df=Y_hat_df, Y_df=Y_train_df,\n                          S=S, tags=tags)\n\nEvaluation\ndef mse(y, y_hat):\n    return np.mean((y-y_hat)**2)\n\nevaluator = HierarchicalEvaluation(evaluators=[mse])\nevaluator.evaluate(Y_hat_df=Y_rec_df, Y_test=Y_test_df.set_index('unique_id'),\n                   tags=tags, benchmark='Naive')"
  },
  {
    "objectID": "index.html#how-to-cite",
    "href": "index.html#how-to-cite",
    "title": "Hierarchical Forecast 👑",
    "section": "How to cite",
    "text": "How to cite\nHere’s the complete paper.\n@article{olivares2022hierarchicalforecast,\n    author    = {Kin G. Olivares and\n                 Federico Garza and \n                 David Luo and \n                 Cristian Challú and\n                 Max Mergenthaler and\n                 Souhaib Ben Taieb and\n                 Shanika L. Wickramasuriya and\n                 Artur Dubrawski},\n    title     = {{HierarchicalForecast}: A Reference Framework for Hierarchical Forecasting in Python},\n    journal   = {Work in progress paper, submitted to Journal of Machine Learning Research.},\n    volume    = {abs/2207.03517},\n    year      = {2022},\n    url       = {https://arxiv.org/abs/2207.03517},\n    archivePrefix = {arXiv}\n}"
  }
]